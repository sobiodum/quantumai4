{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-28 10:33:58,989\tINFO worker.py:1612 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-09-28 10:34:05,632\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import ray, random, os \n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.air.config import ScalingConfig\n",
    "import pandas as pd\n",
    "ray.init(_temp_dir='/Volumes/SSD980/ray')\n",
    "from ray import air, tune\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "import argparse\n",
    "from ray.tune import CLIReporter\n",
    "from worker_standlone import WorkerStandAlone\n",
    "from manager import Manager\n",
    "from ray.train.rl import RLTrainer\n",
    "from ray.rllib.policy.policy import Policy, PolicySpec\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune import TuneConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "from ray.air.config import RunConfig, CheckpointConfig\n",
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-28 10:34:20,290\tINFO tune.py:657 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/tune/tune.py:258: UserWarning: Passing a `local_dir` is deprecated and will be removed in the future. Pass `storage_path` instead or set the `RAY_AIR_LOCAL_CACHE_DIR` environment variable instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-09-28 10:34:20 (running for 00:00:00.17)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 1800.000: None | Iter 600.000: None | Iter 200.000: None\n",
      "Logical resource usage: 0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /Volumes/SSD980/ray/workertune/A2C\n",
      "Number of trials: 2/10 (2 PENDING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=16177)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:26,675\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a2c/` has been deprecated. Use `rllib_contrib/a2c/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:26,675\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a3c/` has been deprecated. Use `rllib_contrib/a3c/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:26,675\tWARNING algorithm_config.py:656 -- Cannot create A2CConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=16199)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:26,675\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a3c/` has been deprecated. Use `rllib_contrib/a3c/` instead. This will raise an error in the future!\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:26,675\tWARNING algorithm_config.py:656 -- Cannot create A2CConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,152\tDEBUG rollout_worker.py:1761 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,152\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,152\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,152\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,152\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,152\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,152\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,152\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (14,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,152\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,153\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,153\tDEBUG catalog.py:793 -- Created preprocessor <ray.rllib.models.preprocessors.DictFlatteningPreprocessor object at 0x17e4d1870>: Dict('current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)) -> (22,)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16201)\u001b[0m 2023-09-28 10:34:35,159\tINFO eager_tf_policy_v2.py:80 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,238\tINFO policy.py:1294 -- Policy (worker=1) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,268\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,268\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_modelv2.TFModelV2` has been deprecated. Use `ray.rllib.core.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,276\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.misc.normc_initializer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,410\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,411\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,411\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,414\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,414\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,418\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,547\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.MultiActionDistribution` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfMultiDistribution` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,552\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.DiagGaussian` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfDiagGaussian` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,555\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.TFActionDistribution` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfDistribution` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,578\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.Categorical` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.Categorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,704\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,704\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,704\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,709\tINFO json_writer.py:50 -- You are using JSONWriter. It is recommended to use DatasetWriter instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:35,714\tDEBUG rollout_worker.py:645 -- Created rollout worker with env <ray.rllib.env.vector_env.VectorEnvWrapper object at 0x17e4d0a60> (<WorkerStandAlone<worker-single-stock>>), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:36,050\tINFO worker_set.py:297 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Dict('current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), '__env__': (Dict('current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)))}\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:36,094\tINFO policy.py:1294 -- Policy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:36,248\tINFO rollout_worker.py:1742 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:36,248\tINFO rollout_worker.py:1743 -- Built preprocessor map: {'default_policy': None}\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:36,248\tINFO rollout_worker.py:550 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:36,249\tDEBUG rollout_worker.py:645 -- Created rollout worker with env None (None), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:36,254\tINFO algorithm_config.py:3667 -- Executing eagerly (framework='tf2'), with eager_tracing=True. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:36,306\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16203)\u001b[0m 2023-09-28 10:34:36,603\tINFO json_writer.py:107 -- Writing to new output file <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-3_0.json' mode='w' encoding='UTF-8'>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16203)\u001b[0m 2023-09-28 10:34:36,603\tDEBUG json_writer.py:81 -- Wrote 6635 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-3_0.json' mode='w' encoding='UTF-8'> in 0.003108978271484375s\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:36,615\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m { 'count': 11,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((11, 5), dtype=float32, min=-0.001, max=0.022, mean=0.006),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'action_logp': np.ndarray((11,), dtype=float32, min=-4.498, max=-2.029, mean=-2.871),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'action_prob': np.ndarray((11,), dtype=float32, min=0.011, max=0.131, mean=0.074),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'actions': np.ndarray((11, 2), dtype=float32, min=-2.232, max=2.224, mean=0.393),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'advantages': np.ndarray((11,), dtype=float32, min=-143.676, max=132.079, mean=-21.832),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'agent_index': np.ndarray((11,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'eps_id': np.ndarray((11,), dtype=int64, min=6.106183673307178e+17, max=6.106183673307178e+17, mean=6.106183673307178e+17),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'new_obs': np.ndarray((11, 22), dtype=float32, min=-25616.719, max=7075415.0, mean=370088.656),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'obs': np.ndarray((11, 22), dtype=float32, min=-25616.719, max=7075415.0, mean=368072.344),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'prev_actions': np.ndarray((11, 2), dtype=float32, min=-2.232, max=2.224, mean=0.345),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'prev_rewards': np.ndarray((11,), dtype=float32, min=-208.553, max=57.282, mean=-9.682),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'rewards': np.ndarray((11,), dtype=float32, min=-208.553, max=57.282, mean=-11.325),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           't': np.ndarray((11,), dtype=int64, min=0.0, max=10.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'terminateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'truncateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'unroll_id': np.ndarray((11,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'value_targets': np.ndarray((11,), dtype=float32, min=-143.663, max=132.092, mean=-21.82),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'values_bootstrapped': np.ndarray((11,), dtype=float32, min=0.01, max=0.013, mean=0.012),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'vf_preds': np.ndarray((11,), dtype=float32, min=0.01, max=0.013, mean=0.012)}},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:36,621\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:36,623\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:36,699\tWARNING deprecation.py:50 -- DeprecationWarning: `compute_gradients` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:36,761\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable fc_1/kernel:0\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:36,761\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable fc_1/bias:0\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:36,761\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable fc_2/kernel:0\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:36,761\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable fc_2/bias:0\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:36,761\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable fc_out/kernel:0\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:36,761\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable fc_out/bias:0\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:36,761\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable value_out/kernel:0\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:36,761\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable value_out/bias:0\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:34:37,221\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'learner_stats': { 'cur_lr': 7.899221236584708e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'entropy_coeff': 0.013264811597764492,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'policy_entropy': 80.7008,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'policy_loss': -114.851776,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'vf_loss': 253850.81},\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'vf_explained_var': 7.1525574e-07}}\n",
      "\u001b[2m\u001b[36m(pid=16198)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,170\tDEBUG rollout_worker.py:1761 -- Creating policy for default_policy\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,381\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\u001b[32m [repeated 207x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,172\tDEBUG catalog.py:793 -- Created preprocessor <ray.rllib.models.preprocessors.DictFlatteningPreprocessor object at 0x159bb3be0>: Dict('current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)) -> (22,)\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,176\tINFO eager_tf_policy_v2.py:80 -- Creating TF-eager policy running on CPU.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:34:35,994\tINFO policy.py:1294 -- Policy (worker=1) running on CPU.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,221\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,222\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_modelv2.TFModelV2` has been deprecated. Use `ray.rllib.core.rl_module.RLModule` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,230\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.misc.normc_initializer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,274\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,275\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,275\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,276\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,276\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,277\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,314\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.MultiActionDistribution` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfMultiDistribution` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,315\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.DiagGaussian` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfDiagGaussian` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,317\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.TFActionDistribution` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfDistribution` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,329\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.Categorical` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.Categorical` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,382\tINFO util.py:118 -- Using connectors:\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,383\tINFO util.py:119 --     AgentConnectorPipeline\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m         ObsPreprocessorConnector\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m         StateBufferConnector\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m         ViewRequirementAgentConnector\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,383\tINFO util.py:120 --     ActionConnectorPipeline\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m         ConvertToNumpyConnector\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m         NormalizeActionsConnector\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m         ImmutableActionsConnector\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,384\tINFO json_writer.py:50 -- You are using JSONWriter. It is recommended to use DatasetWriter instead.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:34:36,127\tDEBUG rollout_worker.py:645 -- Created rollout worker with env <ray.rllib.env.vector_env.VectorEnvWrapper object at 0x17b3e0a90> (<WorkerStandAlone<worker-single-stock>>), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,146\tINFO worker_set.py:297 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Dict('current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), '__env__': (Dict('current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)))}\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,196\tINFO policy.py:1294 -- Policy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,383\tINFO rollout_worker.py:1742 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,383\tINFO rollout_worker.py:1743 -- Built preprocessor map: {'default_policy': None}\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,383\tINFO rollout_worker.py:550 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,384\tDEBUG rollout_worker.py:645 -- Created rollout worker with env None (None), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,397\tINFO algorithm_config.py:3667 -- Executing eagerly (framework='tf2'), with eager_tracing=True. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:34:36,431\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16202)\u001b[0m 2023-09-28 10:34:36,670\tINFO json_writer.py:107 -- Writing to new output file <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00001_1_entropy_coeff=0.0746,gamma=0.9586,lambda=0.9267,lr=0.0001,vf_loss_coeff=0.1717_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-3_0.json' mode='w' encoding='UTF-8'>\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:41,616\tDEBUG json_writer.py:81 -- Wrote 7209 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002429485321044922s\u001b[32m [repeated 646x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:34:36,661\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m { 'count': 11,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((11, 5), dtype=float32, min=-0.007, max=0.015, mean=0.003),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m                                           'action_logp': np.ndarray((11,), dtype=float32, min=-4.503, max=-2.027, mean=-2.772),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m                                           'action_prob': np.ndarray((11,), dtype=float32, min=0.011, max=0.132, mean=0.078),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m                                           'actions': np.ndarray((11, 2), dtype=float32, min=-2.22, max=2.0, mean=0.488),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m                                           'advantages': np.ndarray((11,), dtype=float32, min=-188.426, max=101.753, mean=-22.468),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m                                           'agent_index': np.ndarray((11,), dtype=int64, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m                                           'eps_id': np.ndarray((11,), dtype=int64, min=8.881519981818358e+17, max=8.881519981818358e+17, mean=8.881519981818359e+17),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m                                           'new_obs': np.ndarray((11, 22), dtype=float32, min=0.0, max=7075415.0, mean=375483.094),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m                                           'obs': np.ndarray((11, 22), dtype=float32, min=0.0, max=7075415.0, mean=370745.719),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m                                           'prev_actions': np.ndarray((11, 2), dtype=float32, min=-2.22, max=2.0, mean=0.434),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m                                           'prev_rewards': np.ndarray((11,), dtype=float32, min=-53.755, max=131.364, mean=15.71),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m                                           'rewards': np.ndarray((11,), dtype=float32, min=-151.601, max=131.364, mean=1.929),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m                                           't': np.ndarray((11,), dtype=int64, min=0.0, max=10.0, mean=5.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m                                           'terminateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m                                           'truncateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m                                           'unroll_id': np.ndarray((11,), dtype=int64, min=1.0, max=1.0, mean=1.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m                                           'value_targets': np.ndarray((11,), dtype=float32, min=-188.422, max=101.758, mean=-22.464),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m                                           'values_bootstrapped': np.ndarray((11,), dtype=float32, min=-0.001, max=0.006, mean=0.003),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m                                           'vf_preds': np.ndarray((11,), dtype=float32, min=-0.001, max=0.006, mean=0.004)}},\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m   'type': 'MultiAgentBatch'}\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,673\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,675\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,736\tWARNING deprecation.py:50 -- DeprecationWarning: `compute_gradients` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:36,795\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable value_out/bias:0\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:34:37,237\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'grad_gnorm': 39.999996,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'learner_stats': { 'cur_lr': 6.326437141979113e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'entropy_coeff': 0.07458633184432983,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'policy_entropy': 80.62784,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'policy_loss': -204.55737,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'vf_loss': 222229.27},\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'vf_explained_var': 9.417534e-06}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics                                                                                                                                              </th><th>counters                                                                                                                                </th><th>custom_metrics  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_sampled_throughput_per_sec</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained_throughput_per_sec</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                          </th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                    </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </th><th>timers                                                                                                                                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>A2C_worker_d1b9a_00000</td><td style=\"text-align: right;\">                 449120</td><td>{&#x27;ObsPreprocessorConnector_ms&#x27;: 0.025473322187151228, &#x27;StateBufferConnector_ms&#x27;: 0.004147802080426898, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.09557826178414482}</td><td>{&#x27;num_env_steps_sampled&#x27;: 449120, &#x27;num_env_steps_trained&#x27;: 449120, &#x27;num_agent_steps_sampled&#x27;: 449120, &#x27;num_agent_steps_trained&#x27;: 449120}</td><td>{}              </td><td style=\"text-align: right;\">              6305</td><td>{}             </td><td style=\"text-align: right;\">              199692</td><td style=\"text-align: right;\">              67930.1</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">                   1</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 7.899221236584708e-05, &#x27;entropy_coeff&#x27;: 0.013264811597764492, &#x27;policy_loss&#x27;: -25250966000000.0, &#x27;policy_entropy&#x27;: -547.2268, &#x27;var_gnorm&#x27;: 24.321335, &#x27;vf_loss&#x27;: 78.606}, &#x27;grad_gnorm&#x27;: 40.000004, &#x27;vf_explained_var&#x27;: 2.0623207e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 14035, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 14034}}, &#x27;num_env_steps_sampled&#x27;: 449120, &#x27;num_env_steps_trained&#x27;: 449120, &#x27;num_agent_steps_sampled&#x27;: 449120, &#x27;num_agent_steps_trained&#x27;: 449120}</td><td style=\"text-align: right;\">                   449120</td><td style=\"text-align: right;\">                   449120</td><td style=\"text-align: right;\">                 449120</td><td style=\"text-align: right;\">                             7040</td><td style=\"text-align: right;\">                                   703.566</td><td style=\"text-align: right;\">                 449120</td><td style=\"text-align: right;\">                             7040</td><td style=\"text-align: right;\">                                   703.566</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    3</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                         7040</td><td>{&#x27;cpu_util_percent&#x27;: 90.04285714285716, &#x27;ram_util_percent&#x27;: 84.07142857142857}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.4966412519011265, &#x27;mean_inference_ms&#x27;: 1.313350577425996, &#x27;mean_action_processing_ms&#x27;: 0.18979633291464332, &#x27;mean_env_wait_ms&#x27;: 0.30858048058786863, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 199691.5709291429, &#x27;episode_reward_min&#x27;: -2.0, &#x27;episode_reward_mean&#x27;: 67930.08215603894, &#x27;episode_len_mean&#x27;: 6305.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 1, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [28632.2039503423, 15821.731639282312, 28435.84409679193, 136440.63439996625, 43397.19055744865, 16837.671950103486, 95882.31007636047, 100696.85642381455, 104942.43049612321, 72280.29872748531, 85743.60329255226, 114369.59861984605, 144998.41999530263, 68588.85607545768, 127916.77841141628, 97041.3447894823, 83036.82732863598, 123541.65700333091, 80784.35932328727, 78039.66454649041, 199691.5709291429, 144208.99659605042, 132966.29332317575, 110423.7524998668, 110558.89360707719, 136515.46768895653, 119440.54595050236, 118951.51611160603, 97767.02143994904, 168319.73085720837, 169132.70989083825, 185843.3307816854, 63501.38055788231, 64518.550439755985, 70295.53417521623, 69853.88897908572, 69951.14948620647, 107033.62312728167, 144367.1137345843, 133891.82607493177, 116176.25533165783, 63849.324734687805, 64332.88672590256, 100868.15902352333, 56845.12265641987, 58929.13983607292, 2095.9199731200933, 6692.6064175367355, 5344.061789631844, 39598.08112300187, 33006.46030293405, 3218.220173448324, 5661.753787561087, 36489.44458684325, 37810.86443160474, 3422.540355358273, 7909.586325287819, 48205.14539361, 0.0, -2.0, -2.0, -1.0, -1.0, 0.0, -1.0, -2.0, 0.0, -1.0, -1.0, 0.0], &#x27;episode_lengths&#x27;: [6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.4966412519011265, &#x27;mean_inference_ms&#x27;: 1.313350577425996, &#x27;mean_action_processing_ms&#x27;: 0.18979633291464332, &#x27;mean_env_wait_ms&#x27;: 0.30858048058786863, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.025473322187151228, &#x27;StateBufferConnector_ms&#x27;: 0.004147802080426898, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.09557826178414482}}                                                                                                                                                               </td><td>{&#x27;training_iteration_time_ms&#x27;: 52.233, &#x27;sample_time_ms&#x27;: 43.315, &#x27;learn_time_ms&#x27;: 4.779, &#x27;learn_throughput&#x27;: 6696.589, &#x27;synch_weights_time_ms&#x27;: 4.059}</td></tr>\n",
       "<tr><td>A2C_worker_d1b9a_00001</td><td style=\"text-align: right;\">                 448768</td><td>{&#x27;ObsPreprocessorConnector_ms&#x27;: 0.025590487888881137, &#x27;StateBufferConnector_ms&#x27;: 0.00331878662109375, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.09877852031162807} </td><td>{&#x27;num_env_steps_sampled&#x27;: 448768, &#x27;num_env_steps_trained&#x27;: 448768, &#x27;num_agent_steps_sampled&#x27;: 448768, &#x27;num_agent_steps_trained&#x27;: 448768}</td><td>{}              </td><td style=\"text-align: right;\">              6305</td><td>{}             </td><td style=\"text-align: right;\">              208564</td><td style=\"text-align: right;\">              96499.3</td><td style=\"text-align: right;\">                -361</td><td style=\"text-align: right;\">                   1</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 6.326437141979113e-05, &#x27;entropy_coeff&#x27;: 0.07458633184432983, &#x27;policy_loss&#x27;: 42002.434, &#x27;policy_entropy&#x27;: -49.922005, &#x27;var_gnorm&#x27;: 23.145494, &#x27;vf_loss&#x27;: 16086733.0}, &#x27;grad_gnorm&#x27;: 40.000008, &#x27;vf_explained_var&#x27;: 2.3424625e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 14024, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 14023}}, &#x27;num_env_steps_sampled&#x27;: 448768, &#x27;num_env_steps_trained&#x27;: 448768, &#x27;num_agent_steps_sampled&#x27;: 448768, &#x27;num_agent_steps_trained&#x27;: 448768}    </td><td style=\"text-align: right;\">                   448768</td><td style=\"text-align: right;\">                   448768</td><td style=\"text-align: right;\">                 448768</td><td style=\"text-align: right;\">                             7104</td><td style=\"text-align: right;\">                                   706.287</td><td style=\"text-align: right;\">                 448768</td><td style=\"text-align: right;\">                             7104</td><td style=\"text-align: right;\">                                   706.287</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    3</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                         7104</td><td>{&#x27;cpu_util_percent&#x27;: 89.97857142857143, &#x27;ram_util_percent&#x27;: 84.17857142857143}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.501706375126174, &#x27;mean_inference_ms&#x27;: 1.3124682632194022, &#x27;mean_action_processing_ms&#x27;: 0.19277593608159796, &#x27;mean_env_wait_ms&#x27;: 0.30344745717355226, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 208563.9755680341, &#x27;episode_reward_min&#x27;: -361.0, &#x27;episode_reward_mean&#x27;: 96499.30744143476, &#x27;episode_len_mean&#x27;: 6305.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 1, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [13894.754547186196, 4199.477072668844, 14393.930170061067, 47750.76353007648, 34455.16713004559, 11294.715549815446, 64020.38180048764, 47448.79617749527, 34013.74749781191, 44193.92839111388, 56049.09343481064, 52452.93396132067, 62238.84426097572, 54982.211725052446, 102816.54272177815, 104677.38082537986, 81197.35418907553, 85761.72733579949, 109387.43719465286, 122809.86724709533, 66667.19176000357, 95382.07034933567, 107582.9696034044, 12849.907972633839, -361.0, 73699.8909291625, 49315.47781879967, 52400.09836954795, 25519.07574832863, 131671.29676222388, 176971.58487742615, 199101.0475974134, 173810.0867641868, 175459.0291893419, 167631.92433395647, 59578.25741172023, 61376.047616612166, 19084.808691591956, 124704.1310083067, 126495.43328633785, 93980.30481088161, 56416.83466190798, 76003.56510535146, 95021.21751891148, 162155.50520830805, 144003.58216030174, 104485.89403224149, 184196.33925854537, 135148.08033047244, 164120.17193879845, 126417.06046529539, 208563.9755680341, 122769.06880500796, 104786.78034197327, 95041.35396623415, 125193.69616951182, 134217.1017691311, 98576.57795761597, 142052.02619169615, 135721.43681181414, 134425.49923900908, 136005.04126956093, 133275.79428926494, 120038.823009735, 137250.97160612256, 133458.5763606504, 202997.45484470954, 72504.22799388302, 76879.27946959311, 52266.8948928345], &#x27;episode_lengths&#x27;: [6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305, 6305]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.501706375126174, &#x27;mean_inference_ms&#x27;: 1.3124682632194022, &#x27;mean_action_processing_ms&#x27;: 0.19277593608159796, &#x27;mean_env_wait_ms&#x27;: 0.30344745717355226, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.025590487888881137, &#x27;StateBufferConnector_ms&#x27;: 0.00331878662109375, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.09877852031162807}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 58.007, &#x27;sample_time_ms&#x27;: 47.235, &#x27;learn_time_ms&#x27;: 6.255, &#x27;learn_throughput&#x27;: 5115.746, &#x27;synch_weights_time_ms&#x27;: 4.451}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:46,747\tDEBUG json_writer.py:81 -- Wrote 7369 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.000247955322265625s\u001b[32m [repeated 826x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:34:51,798\tDEBUG json_writer.py:81 -- Wrote 7456 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002498626708984375s\u001b[32m [repeated 788x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16202)\u001b[0m 2023-09-28 10:34:56,808\tDEBUG json_writer.py:81 -- Wrote 6596 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00001_1_entropy_coeff=0.0746,gamma=0.9586,lambda=0.9267,lr=0.0001,vf_loss_coeff=0.1717_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-3_0.json' mode='w' encoding='UTF-8'> in 0.00026106834411621094s\u001b[32m [repeated 724x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:35:01,836\tDEBUG json_writer.py:81 -- Wrote 7216 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0003261566162109375s\u001b[32m [repeated 699x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:35:06,834\tDEBUG json_writer.py:81 -- Wrote 6994 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002727508544921875s\u001b[32m [repeated 756x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:35:11,895\tDEBUG json_writer.py:81 -- Wrote 7267 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0003299713134765625s\u001b[32m [repeated 780x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16203)\u001b[0m 2023-09-28 10:35:16,893\tDEBUG json_writer.py:81 -- Wrote 6582 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-3_0.json' mode='w' encoding='UTF-8'> in 0.00025177001953125s\u001b[32m [repeated 740x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:35:21,979\tDEBUG json_writer.py:81 -- Wrote 7176 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002779960632324219s\u001b[32m [repeated 765x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:35:27,023\tDEBUG json_writer.py:81 -- Wrote 7255 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002980232238769531s\u001b[32m [repeated 786x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:35:32,084\tDEBUG json_writer.py:81 -- Wrote 7144 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.00025916099548339844s\u001b[32m [repeated 646x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:35:36,716\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m { 'count': 11,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((11, 5), dtype=float32, min=-2.609, max=2.497, mean=-0.374),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'action_logp': np.ndarray((11,), dtype=float32, min=-0.805, max=0.328, mean=0.044),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'action_prob': np.ndarray((11,), dtype=float32, min=0.447, max=1.388, mean=1.106),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'actions': np.ndarray((11, 2), dtype=float32, min=-0.278, max=1.0, mean=0.539),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'advantages': np.ndarray((11,), dtype=float32, min=-677.457, max=1429.721, mean=450.274),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'agent_index': np.ndarray((11,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'eps_id': np.ndarray((11,), dtype=int64, min=2.647764285082842e+17, max=2.647764285082842e+17, mean=2.6477642850828422e+17),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'new_obs': np.ndarray((11, 22), dtype=float32, min=0.038, max=6253585.5, mean=690754.125),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'obs': np.ndarray((11, 22), dtype=float32, min=0.038, max=6302745.5, mean=692989.25),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'prev_actions': np.ndarray((11, 2), dtype=float32, min=-0.278, max=1.0, mean=0.54),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'prev_rewards': np.ndarray((11,), dtype=float32, min=-951.82, max=666.52, mean=82.225),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'rewards': np.ndarray((11,), dtype=float32, min=-475.91, max=666.52, mean=125.489),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           't': np.ndarray((11,), dtype=int64, min=2966.0, max=2976.0, mean=2971.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'terminateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'truncateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'unroll_id': np.ndarray((11,), dtype=int64, min=1421.0, max=1421.0, mean=1421.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'value_targets': np.ndarray((11,), dtype=float32, min=-674.723, max=1432.503, mean=453.032),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'values_bootstrapped': np.ndarray((11,), dtype=float32, min=2.734, max=2.782, mean=2.754),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'vf_preds': np.ndarray((11,), dtype=float32, min=2.734, max=2.782, mean=2.758)}},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:35:36,728\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16200)\u001b[0m 2023-09-28 10:35:37,105\tDEBUG json_writer.py:81 -- Wrote 7221 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-2_0.json' mode='w' encoding='UTF-8'> in 0.0002548694610595703s\u001b[32m [repeated 384x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:35:37,226\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:35:37,231\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 1424,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'grad_gnorm': 40.000004,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'learner_stats': { 'cur_lr': 7.899221236584708e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'entropy_coeff': 0.013264811597764492,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'policy_entropy': 12.285839,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'policy_loss': 32355.717,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'var_gnorm': 22.78695,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'vf_loss': 54427270.0},\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'num_grad_updates_lifetime': 1425,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'vf_explained_var': -0.00014150143}}\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:35:37,275\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:35:36,790\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m { 'count': 32,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-1.448, max=0.89, mean=-0.38),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'action_logp': np.ndarray((32,), dtype=float32, min=-3.306, max=-0.675, mean=-1.476),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'action_prob': np.ndarray((32,), dtype=float32, min=0.037, max=0.509, mean=0.278),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'actions': np.ndarray((32, 2), dtype=float32, min=-2.204, max=2.0, mean=-0.457),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'advantages': np.ndarray((32,), dtype=float32, min=-2093.614, max=600.155, mean=-590.547),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'agent_index': np.ndarray((32,), dtype=int64, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'eps_id': np.ndarray((32,), dtype=int64, min=3.853820379887697e+17, max=5.617254185473203e+17, mean=4.481526547371264e+17),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'new_obs': np.ndarray((32, 22), dtype=float32, min=-0.065, max=6772301.5, mean=590461.438),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'obs': np.ndarray((32, 22), dtype=float32, min=-0.049, max=6785275.5, mean=594704.875),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'prev_actions': np.ndarray((32, 2), dtype=float32, min=-2.204, max=2.0, mean=-0.48),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'prev_rewards': np.ndarray((32,), dtype=float32, min=-1239.085, max=1781.07, mean=-117.374),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'rewards': np.ndarray((32,), dtype=float32, min=-1239.085, max=1781.07, mean=-158.192),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           't': np.ndarray((32,), dtype=int64, min=1670.0, max=3108.0, mean=2656.594),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'unroll_id': np.ndarray((32,), dtype=int64, min=1432.0, max=1433.0, mean=1432.688),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'value_targets': np.ndarray((32,), dtype=float32, min=-2091.893, max=601.941, mean=-588.581),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'values_bootstrapped': np.ndarray((32,), dtype=float32, min=1.733, max=2.259, mean=1.967),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'vf_preds': np.ndarray((32,), dtype=float32, min=1.721, max=2.259, mean=1.965)}},\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m   'type': 'MultiAgentBatch'}\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:35:36,751\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:35:42,147\tDEBUG json_writer.py:81 -- Wrote 7245 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002391338348388672s\u001b[32m [repeated 712x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:35:37,281\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:35:37,287\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 1428,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'learner_stats': { 'cur_lr': 6.326437141979113e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'entropy_coeff': 0.07458633184432983,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'policy_entropy': 56.549446,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'policy_loss': -33384.375,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'var_gnorm': 22.723747,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'vf_loss': 11572453.0},\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'num_grad_updates_lifetime': 1429,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'vf_explained_var': 0.00042682886}}\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:35:37,334\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:35:47,245\tDEBUG json_writer.py:81 -- Wrote 7170 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002608299255371094s\u001b[32m [repeated 713x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:35:52,250\tDEBUG json_writer.py:81 -- Wrote 7069 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002880096435546875s\u001b[32m [repeated 794x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:35:57,271\tDEBUG json_writer.py:81 -- Wrote 7293 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002849102020263672s\u001b[32m [repeated 714x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:36:02,286\tDEBUG json_writer.py:81 -- Wrote 7280 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0003001689910888672s\u001b[32m [repeated 754x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:36:07,369\tDEBUG json_writer.py:81 -- Wrote 7276 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.00026917457580566406s\u001b[32m [repeated 860x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:36:12,480\tDEBUG json_writer.py:81 -- Wrote 7143 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002970695495605469s\u001b[32m [repeated 844x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:36:17,527\tDEBUG json_writer.py:81 -- Wrote 7174 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0028600692749023438s\u001b[32m [repeated 818x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-09-28 10:36:20 (running for 00:02:00.21)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 1800.000: None | Iter 600.000: None | Iter 200.000: None\n",
      "Logical resource usage: 8.0/8 CPUs, 0/0 GPUs\n",
      "Current best trial: d1b9a_00000 with episode_reward_mean=66282.79778275189 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'env': 'worker', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.9673166178130927, 'lr': 7.899221497415072e-05, 'grad_clip': 40.0, 'grad_clip_by': 'global_norm', 'train_batch_size': 32, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x3024912d0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': 'logdir', 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 10, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'DEBUG', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': True, 'monitor': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'use_critic': True, 'use_gae': True, 'lr_schedule': None, 'vf_loss_coeff': 0.26735980037814955, 'entropy_coeff': 0.013264811275713958, 'entropy_coeff_schedule': None, 'microbatch_size': None, '__stdout_file__': None, '__stderr_file__': None, 'lambda': 0.9543274165749964, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf2', 'num_cpus_for_driver': 1, 'num_workers': 3}\n",
      "Result logdir: /Volumes/SSD980/ray/workertune/A2C\n",
      "Number of trials: 2/10 (2 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:36:22,564\tDEBUG json_writer.py:81 -- Wrote 7266 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.00028586387634277344s\u001b[32m [repeated 778x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:36:27,641\tDEBUG json_writer.py:81 -- Wrote 7404 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.000247955322265625s\u001b[32m [repeated 792x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:36:32,717\tDEBUG json_writer.py:81 -- Wrote 7214 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.00028586387634277344s\u001b[32m [repeated 810x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:36:36,743\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:36:36,774\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m { 'count': 11,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((11, 5), dtype=float32, min=-2.948, max=2.786, mean=-0.568),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'action_logp': np.ndarray((11,), dtype=float32, min=-2.661, max=1.066, mean=0.354),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'action_prob': np.ndarray((11,), dtype=float32, min=0.07, max=2.904, mean=1.954),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'actions': np.ndarray((11, 2), dtype=float32, min=-0.109, max=1.0, mean=0.51),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'advantages': np.ndarray((11,), dtype=float32, min=-663.54, max=147.44, mean=-265.099),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'agent_index': np.ndarray((11,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'eps_id': np.ndarray((11,), dtype=int64, min=1.3008822568357373e+17, max=1.3008822568357373e+17, mean=1.3008822568357374e+17),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'new_obs': np.ndarray((11, 22), dtype=float32, min=-0.01, max=8406667.0, mean=412140.156),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'obs': np.ndarray((11, 22), dtype=float32, min=-0.008, max=8273434.5, mean=403971.031),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'prev_actions': np.ndarray((11, 2), dtype=float32, min=-0.109, max=1.0, mean=0.514),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'prev_rewards': np.ndarray((11,), dtype=float32, min=-351.94, max=165.619, mean=-60.168),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'rewards': np.ndarray((11,), dtype=float32, min=-351.94, max=165.619, mean=-73.342),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           't': np.ndarray((11,), dtype=int64, min=991.0, max=1001.0, mean=996.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'terminateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'truncateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'unroll_id': np.ndarray((11,), dtype=int64, min=2967.0, max=2967.0, mean=2967.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'value_targets': np.ndarray((11,), dtype=float32, min=-657.629, max=153.355, mean=-259.185),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'values_bootstrapped': np.ndarray((11,), dtype=float32, min=5.911, max=5.915, mean=5.914),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'vf_preds': np.ndarray((11,), dtype=float32, min=5.911, max=5.919, mean=5.914)}},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:36:37,293\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:36:37,298\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 2969,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'grad_gnorm': 39.999996,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'learner_stats': { 'cur_lr': 7.899221236584708e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'entropy_coeff': 0.013264811597764492,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'policy_entropy': -17.834625,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'policy_loss': -9896.088,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'var_gnorm': 22.886232,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'vf_loss': 14394039.0},\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'num_grad_updates_lifetime': 2970,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'vf_explained_var': 6.198883e-06}}\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:36:37,337\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:36:37,840\tDEBUG json_writer.py:81 -- Wrote 7009 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002853870391845703s\u001b[32m [repeated 780x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:36:36,814\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:36:36,842\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m { 'count': 32,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-1.698, max=0.417, mean=-0.465),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'action_logp': np.ndarray((32,), dtype=float32, min=-4.09, max=-1.046, mean=-1.864),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'action_prob': np.ndarray((32,), dtype=float32, min=0.017, max=0.351, mean=0.189),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'actions': np.ndarray((32, 2), dtype=float32, min=-2.923, max=2.0, mean=-0.437),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'advantages': np.ndarray((32,), dtype=float32, min=-2604.695, max=612.692, mean=-595.445),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'agent_index': np.ndarray((32,), dtype=int64, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'eps_id': np.ndarray((32,), dtype=int64, min=2.3198248538050643e+17, max=6.794741382438124e+17, mean=5.236964643038711e+17),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'new_obs': np.ndarray((32, 22), dtype=float32, min=0.002, max=8054130.0, mean=648242.812),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'obs': np.ndarray((32, 22), dtype=float32, min=0.002, max=8045589.0, mean=649058.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'prev_actions': np.ndarray((32, 2), dtype=float32, min=-2.923, max=2.0, mean=-0.44),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'prev_rewards': np.ndarray((32,), dtype=float32, min=-1700.165, max=716.87, mean=-98.874),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'rewards': np.ndarray((32,), dtype=float32, min=-1700.165, max=716.87, mean=-92.579),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           't': np.ndarray((32,), dtype=int64, min=1211.0, max=4549.0, mean=2256.156),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'unroll_id': np.ndarray((32,), dtype=int64, min=2983.0, max=2987.0, mean=2985.75),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'value_targets': np.ndarray((32,), dtype=float32, min=-2600.015, max=617.843, mean=-590.43),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'values_bootstrapped': np.ndarray((32,), dtype=float32, min=4.648, max=5.161, mean=5.023),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'vf_preds': np.ndarray((32,), dtype=float32, min=4.637, max=5.161, mean=5.015)}},\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m   'type': 'MultiAgentBatch'}\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:36:37,346\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:36:37,350\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 2976,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'learner_stats': { 'cur_lr': 6.326437141979113e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'entropy_coeff': 0.07458633184432983,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'policy_entropy': 55.41941,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'policy_loss': -34227.445,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'var_gnorm': 22.790855,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'vf_loss': 14674481.0},\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'num_grad_updates_lifetime': 2977,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'vf_explained_var': 0.0004117489}}\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:36:37,382\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:36:42,903\tDEBUG json_writer.py:81 -- Wrote 7198 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002849102020263672s\u001b[32m [repeated 843x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:36:48,007\tDEBUG json_writer.py:81 -- Wrote 7149 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002460479736328125s\u001b[32m [repeated 873x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:36:53,076\tDEBUG json_writer.py:81 -- Wrote 7165 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.00028014183044433594s\u001b[32m [repeated 834x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:36:58,144\tDEBUG json_writer.py:81 -- Wrote 7000 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002472400665283203s\u001b[32m [repeated 790x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:37:03,278\tDEBUG json_writer.py:81 -- Wrote 7043 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.00028204917907714844s\u001b[32m [repeated 693x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:37:08,335\tDEBUG json_writer.py:81 -- Wrote 7183 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.00035500526428222656s\u001b[32m [repeated 765x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:37:13,331\tDEBUG json_writer.py:81 -- Wrote 7136 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.00027871131896972656s\u001b[32m [repeated 751x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:37:18,394\tDEBUG json_writer.py:81 -- Wrote 6974 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.00028228759765625s\u001b[32m [repeated 726x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:37:23,414\tDEBUG json_writer.py:81 -- Wrote 7087 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002751350402832031s\u001b[32m [repeated 721x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:37:28,504\tDEBUG json_writer.py:81 -- Wrote 7071 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002827644348144531s\u001b[32m [repeated 786x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:37:33,594\tDEBUG json_writer.py:81 -- Wrote 7118 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.00024700164794921875s\u001b[32m [repeated 765x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:37:36,808\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:37:36,835\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m { 'count': 11,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((11, 5), dtype=float32, min=-3.145, max=3.443, mean=-1.019),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'action_logp': np.ndarray((11,), dtype=float32, min=0.878, max=1.945, mean=1.717),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'action_prob': np.ndarray((11,), dtype=float32, min=2.407, max=6.992, mean=5.784),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'actions': np.ndarray((11, 2), dtype=float32, min=-1.284, max=1.0, mean=-0.101),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'advantages': np.ndarray((11,), dtype=float32, min=-2762.669, max=146.611, mean=-1064.018),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'agent_index': np.ndarray((11,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'eps_id': np.ndarray((11,), dtype=int64, min=6.097310775946725e+17, max=6.097310775946725e+17, mean=6.097310775946724e+17),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'new_obs': np.ndarray((11, 22), dtype=float32, min=0.267, max=15511801.0, mean=1911515.75),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'obs': np.ndarray((11, 22), dtype=float32, min=0.235, max=15511801.0, mean=1912808.25),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'prev_actions': np.ndarray((11, 2), dtype=float32, min=-1.284, max=1.0, mean=-0.1),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'prev_rewards': np.ndarray((11,), dtype=float32, min=-2780.18, max=1535.6, mean=-268.587),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'rewards': np.ndarray((11,), dtype=float32, min=-2780.18, max=1535.6, mean=-226.449),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           't': np.ndarray((11,), dtype=int64, min=5310.0, max=5320.0, mean=5315.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'terminateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'truncateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'unroll_id': np.ndarray((11,), dtype=int64, min=4510.0, max=4510.0, mean=4510.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'value_targets': np.ndarray((11,), dtype=float32, min=-2755.706, max=153.575, mean=-1057.03),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'values_bootstrapped': np.ndarray((11,), dtype=float32, min=6.964, max=7.041, mean=6.995),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'vf_preds': np.ndarray((11,), dtype=float32, min=6.961, max=7.041, mean=6.988)}},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:37:37,341\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:37:37,349\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 4509,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'grad_gnorm': 39.999996,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'learner_stats': { 'cur_lr': 7.899221236584708e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'entropy_coeff': 0.013264811597764492,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'policy_entropy': -48.040344,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'policy_loss': -2694.0317,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'var_gnorm': 22.977173,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'vf_loss': 14762643.0},\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'num_grad_updates_lifetime': 4510,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'vf_explained_var': 1.3113022e-05}}\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:37:37,381\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16203)\u001b[0m 2023-09-28 10:37:38,587\tDEBUG json_writer.py:81 -- Wrote 6590 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-3_0.json' mode='w' encoding='UTF-8'> in 0.000244140625s\u001b[32m [repeated 801x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m =========HRL is done=============\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m Worker is done\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m day: 6304, episode: 10\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m Beginn_Portfolio_Value: 1000000.0\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m End Total Assets: 10521027.0\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m Total PnL: 9521027.0\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m =================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:37:36,868\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:37:36,893\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m { 'count': 32,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-1.629, max=0.612, mean=-0.611),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'action_logp': np.ndarray((32,), dtype=float32, min=-2.942, max=-0.023, mean=-0.413),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'action_prob': np.ndarray((32,), dtype=float32, min=0.053, max=0.977, mean=0.742),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'actions': np.ndarray((32, 2), dtype=float32, min=-1.628, max=2.0, mean=-0.383),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'advantages': np.ndarray((32,), dtype=float32, min=-2820.976, max=3910.626, mean=202.157),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'agent_index': np.ndarray((32,), dtype=int64, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'eps_id': np.ndarray((32,), dtype=int64, min=1.860608033685313e+17, max=7.660544147246166e+17, mean=5.6594661832744166e+17),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'new_obs': np.ndarray((32, 22), dtype=float32, min=-0.016, max=13084010.0, mean=1259619.5),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'obs': np.ndarray((32, 22), dtype=float32, min=-0.016, max=13217340.0, mean=1263507.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'prev_actions': np.ndarray((32, 2), dtype=float32, min=-1.628, max=2.0, mean=-0.359),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'prev_rewards': np.ndarray((32,), dtype=float32, min=-2297.48, max=1403.62, mean=-240.743),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'rewards': np.ndarray((32,), dtype=float32, min=-2297.48, max=3636.67, mean=29.694),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           't': np.ndarray((32,), dtype=int64, min=1025.0, max=5551.0, mean=4134.594),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'unroll_id': np.ndarray((32,), dtype=int64, min=4528.0, max=4531.0, mean=4530.062),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'value_targets': np.ndarray((32,), dtype=float32, min=-2813.424, max=3918.104, mean=209.815),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'values_bootstrapped': np.ndarray((32,), dtype=float32, min=7.478, max=7.804, mean=7.652),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'vf_preds': np.ndarray((32,), dtype=float32, min=7.478, max=7.804, mean=7.658)}},\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m   'type': 'MultiAgentBatch'}\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:37:37,397\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:37:37,401\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 4516,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'learner_stats': { 'cur_lr': 6.326437141979113e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'entropy_coeff': 0.07458633184432983,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'policy_entropy': 23.427505,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'policy_loss': 4666.577,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'var_gnorm': 22.874947,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'vf_loss': 49742548.0},\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'num_grad_updates_lifetime': 4517,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'vf_explained_var': -3.0636787e-05}}\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:37:37,433\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:37:43,660\tDEBUG json_writer.py:81 -- Wrote 6967 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.000392913818359375s\u001b[32m [repeated 612x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:37:48,742\tDEBUG json_writer.py:81 -- Wrote 7102 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.00035572052001953125s\u001b[32m [repeated 820x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:37:53,862\tDEBUG json_writer.py:81 -- Wrote 7328 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.00035500526428222656s\u001b[32m [repeated 815x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:37:58,929\tDEBUG json_writer.py:81 -- Wrote 7133 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0005800724029541016s\u001b[32m [repeated 668x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=16202)\u001b[0m =========HRL is done=============\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16202)\u001b[0m Worker is done\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16202)\u001b[0m day: 6304, episode: 10\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16202)\u001b[0m Beginn_Portfolio_Value: 1000000.0\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16202)\u001b[0m End Total Assets: 8346931.0\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16202)\u001b[0m Total PnL: 7346931.0\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16202)\u001b[0m =================================\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:38:03,926\tDEBUG json_writer.py:81 -- Wrote 7173 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002701282501220703s\u001b[32m [repeated 736x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:38:09,010\tDEBUG json_writer.py:81 -- Wrote 7109 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002620220184326172s\u001b[32m [repeated 825x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16203)\u001b[0m 2023-09-28 10:38:13,999\tDEBUG json_writer.py:81 -- Wrote 6460 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-3_0.json' mode='w' encoding='UTF-8'> in 0.00025391578674316406s\u001b[32m [repeated 759x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:38:19,046\tDEBUG json_writer.py:81 -- Wrote 7175 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.00023293495178222656s\u001b[32m [repeated 726x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-09-28 10:38:20 (running for 00:04:00.23)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 1800.000: None | Iter 600.000: None | Iter 200.000: None\n",
      "Logical resource usage: 8.0/8 CPUs, 0/0 GPUs\n",
      "Current best trial: d1b9a_00000 with episode_reward_mean=95453.58678260728 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'env': 'worker', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.9673166178130927, 'lr': 7.899221497415072e-05, 'grad_clip': 40.0, 'grad_clip_by': 'global_norm', 'train_batch_size': 32, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x3024912d0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': 'logdir', 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 10, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'DEBUG', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': True, 'monitor': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'use_critic': True, 'use_gae': True, 'lr_schedule': None, 'vf_loss_coeff': 0.26735980037814955, 'entropy_coeff': 0.013264811275713958, 'entropy_coeff_schedule': None, 'microbatch_size': None, '__stdout_file__': None, '__stderr_file__': None, 'lambda': 0.9543274165749964, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf2', 'num_cpus_for_driver': 1, 'num_workers': 3}\n",
      "Result logdir: /Volumes/SSD980/ray/workertune/A2C\n",
      "Number of trials: 2/10 (2 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:38:24,104\tDEBUG json_writer.py:81 -- Wrote 7070 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002739429473876953s\u001b[32m [repeated 810x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:38:29,151\tDEBUG json_writer.py:81 -- Wrote 7080 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.00025010108947753906s\u001b[32m [repeated 819x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:38:34,155\tDEBUG json_writer.py:81 -- Wrote 6849 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002472400665283203s\u001b[32m [repeated 816x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:38:36,867\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:38:36,895\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m { 'count': 11,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((11, 5), dtype=float32, min=-3.466, max=3.857, mean=-1.232),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'action_logp': np.ndarray((11,), dtype=float32, min=1.049, max=2.521, mean=1.819),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'action_prob': np.ndarray((11,), dtype=float32, min=2.856, max=12.443, mean=7.299),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'actions': np.ndarray((11, 2), dtype=float32, min=-1.637, max=1.0, mean=-0.287),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'advantages': np.ndarray((11,), dtype=float32, min=-1132.876, max=2354.748, mean=324.756),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'agent_index': np.ndarray((11,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'eps_id': np.ndarray((11,), dtype=int64, min=8.400591326050752e+17, max=8.400591326050752e+17, mean=8.400591326050753e+17),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'new_obs': np.ndarray((11, 22), dtype=float32, min=-0.133, max=9498235.0, mean=1110388.625),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'obs': np.ndarray((11, 22), dtype=float32, min=-0.152, max=9498235.0, mean=1107996.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'prev_actions': np.ndarray((11, 2), dtype=float32, min=-1.637, max=1.0, mean=-0.286),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'prev_rewards': np.ndarray((11,), dtype=float32, min=-1214.18, max=3235.77, mean=375.163),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'rewards': np.ndarray((11,), dtype=float32, min=-1214.18, max=3235.77, mean=220.667),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           't': np.ndarray((11,), dtype=int64, min=3115.0, max=3125.0, mean=3120.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'terminateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'truncateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'unroll_id': np.ndarray((11,), dtype=int64, min=6036.0, max=6036.0, mean=6036.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'value_targets': np.ndarray((11,), dtype=float32, min=-1123.845, max=2363.83, mean=333.815),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'values_bootstrapped': np.ndarray((11,), dtype=float32, min=9.031, max=9.082, mean=9.059),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'vf_preds': np.ndarray((11,), dtype=float32, min=9.031, max=9.082, mean=9.059)}},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:38:37,410\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:38:37,414\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 6029,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'learner_stats': { 'cur_lr': 7.899221236584708e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'entropy_coeff': 0.013264811597764492,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'policy_entropy': -60.405815,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'policy_loss': 209317.69,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'var_gnorm': 23.025465,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'vf_loss': 445782300.0},\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'num_grad_updates_lifetime': 6030,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'vf_explained_var': -0.00013935566}}\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:38:37,448\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:38:39,267\tDEBUG json_writer.py:81 -- Wrote 7133 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0004189014434814453s\u001b[32m [repeated 816x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:38:36,921\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:38:36,943\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m { 'count': 32,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-2.228, max=1.08, mean=-0.659),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'action_logp': np.ndarray((32,), dtype=float32, min=-2.36, max=0.755, mean=0.212),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'action_prob': np.ndarray((32,), dtype=float32, min=0.094, max=2.128, mean=1.361),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'actions': np.ndarray((32, 2), dtype=float32, min=-0.867, max=1.0, mean=-0.053),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'advantages': np.ndarray((32,), dtype=float32, min=-9035.689, max=6651.082, mean=-103.226),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'agent_index': np.ndarray((32,), dtype=int64, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'eps_id': np.ndarray((32,), dtype=int64, min=3.5645157284777306e+17, max=6.016848744829322e+17, mean=5.007516827941267e+17),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'new_obs': np.ndarray((32, 22), dtype=float32, min=-0.158, max=13278258.0, mean=1366150.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'obs': np.ndarray((32, 22), dtype=float32, min=-0.158, max=13278258.0, mean=1366324.75),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'prev_actions': np.ndarray((32, 2), dtype=float32, min=-0.867, max=1.0, mean=-0.062),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'prev_rewards': np.ndarray((32,), dtype=float32, min=-5435.146, max=5679.71, mean=-373.292),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'rewards': np.ndarray((32,), dtype=float32, min=-5345.571, max=5679.71, mean=-46.095),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           't': np.ndarray((32,), dtype=int64, min=3368.0, max=3644.0, mean=3456.281),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'unroll_id': np.ndarray((32,), dtype=int64, min=6053.0, max=6059.0, mean=6057.125),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'value_targets': np.ndarray((32,), dtype=float32, min=-9027.386, max=6658.775, mean=-95.202),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'values_bootstrapped': np.ndarray((32,), dtype=float32, min=7.393, max=8.388, mean=8.019),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'vf_preds': np.ndarray((32,), dtype=float32, min=7.393, max=8.388, mean=8.025)}},\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m   'type': 'MultiAgentBatch'}\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:38:37,469\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:38:37,472\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 6038,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'learner_stats': { 'cur_lr': 6.326437141979113e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'entropy_coeff': 0.07458633184432983,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'policy_entropy': 2.0447392,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'policy_loss': 9976.021,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'var_gnorm': 22.904205,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'vf_loss': 227549740.0},\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'num_grad_updates_lifetime': 6039,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'vf_explained_var': -6.4373016e-05}}\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:38:37,467\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:38:44,356\tDEBUG json_writer.py:81 -- Wrote 7137 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0005817413330078125s\u001b[32m [repeated 834x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:38:49,362\tDEBUG json_writer.py:81 -- Wrote 6524 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.00025391578674316406s\u001b[32m [repeated 820x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:38:54,390\tDEBUG json_writer.py:81 -- Wrote 6939 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.00029587745666503906s\u001b[32m [repeated 853x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16202)\u001b[0m 2023-09-28 10:38:59,383\tDEBUG json_writer.py:81 -- Wrote 6661 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00001_1_entropy_coeff=0.0746,gamma=0.9586,lambda=0.9267,lr=0.0001,vf_loss_coeff=0.1717_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-3_0.json' mode='w' encoding='UTF-8'> in 0.0002739429473876953s\u001b[32m [repeated 788x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:39:04,456\tDEBUG json_writer.py:81 -- Wrote 7175 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002582073211669922s\u001b[32m [repeated 839x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:39:09,547\tDEBUG json_writer.py:81 -- Wrote 6526 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0004558563232421875s\u001b[32m [repeated 791x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:39:14,607\tDEBUG json_writer.py:81 -- Wrote 6911 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002601146697998047s\u001b[32m [repeated 919x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:39:19,660\tDEBUG json_writer.py:81 -- Wrote 7141 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.00024509429931640625s\u001b[32m [repeated 839x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:39:24,711\tDEBUG json_writer.py:81 -- Wrote 7101 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002739429473876953s\u001b[32m [repeated 982x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:39:29,753\tDEBUG json_writer.py:81 -- Wrote 7117 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002720355987548828s\u001b[32m [repeated 984x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:39:34,790\tDEBUG json_writer.py:81 -- Wrote 7124 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002579689025878906s\u001b[32m [repeated 987x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:39:36,911\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:39:36,934\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m { 'count': 11,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((11, 5), dtype=float32, min=-3.013, max=4.113, mean=-0.99),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'action_logp': np.ndarray((11,), dtype=float32, min=-0.497, max=1.371, mean=0.963),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'action_prob': np.ndarray((11,), dtype=float32, min=0.608, max=3.937, mean=2.956),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'actions': np.ndarray((11, 2), dtype=float32, min=-1.784, max=1.0, mean=-0.316),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'advantages': np.ndarray((11,), dtype=float32, min=-1562.866, max=1766.95, mean=-94.137),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'agent_index': np.ndarray((11,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'eps_id': np.ndarray((11,), dtype=int64, min=8.104405134806808e+16, max=8.104405134806808e+16, mean=8.104405134806808e+16),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'new_obs': np.ndarray((11, 22), dtype=float32, min=-0.319, max=10264149.0, mean=1168333.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'obs': np.ndarray((11, 22), dtype=float32, min=-0.319, max=10264149.0, mean=1168308.125),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'prev_actions': np.ndarray((11, 2), dtype=float32, min=-1.784, max=1.0, mean=-0.309),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'prev_rewards': np.ndarray((11,), dtype=float32, min=-4665.68, max=2686.57, mean=-462.742),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'rewards': np.ndarray((11,), dtype=float32, min=-3110.16, max=2686.57, mean=-77.097),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           't': np.ndarray((11,), dtype=int64, min=3384.0, max=3394.0, mean=3389.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'terminateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'truncateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'unroll_id': np.ndarray((11,), dtype=int64, min=7785.0, max=7785.0, mean=7785.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'value_targets': np.ndarray((11,), dtype=float32, min=-1552.693, max=1777.123, mean=-83.966),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'values_bootstrapped': np.ndarray((11,), dtype=float32, min=10.173, max=10.173, mean=10.173),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'vf_preds': np.ndarray((11,), dtype=float32, min=10.151, max=10.173, mean=10.171)}},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:39:37,480\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:39:37,482\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:39:37,485\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 7777,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'grad_gnorm': 39.999996,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'learner_stats': { 'cur_lr': 7.899221236584708e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'entropy_coeff': 0.013264811597764492,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'policy_entropy': -28.659145,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'policy_loss': -51848.406,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'var_gnorm': 23.02075,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'vf_loss': 203780060.0},\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'num_grad_updates_lifetime': 7778,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'vf_explained_var': 1.847744e-05}}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:39:39,836\tDEBUG json_writer.py:81 -- Wrote 7112 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002720355987548828s\u001b[32m [repeated 984x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:39:36,959\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:39:36,979\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m { 'count': 32,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-2.58, max=1.447, mean=-0.858),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'action_logp': np.ndarray((32,), dtype=float32, min=-2.747, max=1.343, mean=0.696),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'action_prob': np.ndarray((32,), dtype=float32, min=0.064, max=3.832, mean=2.582),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'actions': np.ndarray((32, 2), dtype=float32, min=-1.349, max=2.0, mean=-0.225),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'advantages': np.ndarray((32,), dtype=float32, min=-1655.103, max=5731.122, mean=1984.295),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'agent_index': np.ndarray((32,), dtype=int64, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'eps_id': np.ndarray((32,), dtype=int64, min=4.869669235464725e+17, max=9.788779144235786e+17, mean=7.965039697822542e+17),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'new_obs': np.ndarray((32, 22), dtype=float32, min=-0.201, max=11323542.0, mean=887926.375),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'obs': np.ndarray((32, 22), dtype=float32, min=-0.201, max=11323542.0, mean=884291.188),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'prev_actions': np.ndarray((32, 2), dtype=float32, min=-1.349, max=2.0, mean=-0.241),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'prev_rewards': np.ndarray((32,), dtype=float32, min=-3298.23, max=4059.0, mean=206.155),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'rewards': np.ndarray((32,), dtype=float32, min=-3298.23, max=4059.0, mean=342.077),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           't': np.ndarray((32,), dtype=int64, min=2110.0, max=3592.0, mean=3126.844),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'unroll_id': np.ndarray((32,), dtype=int64, min=7796.0, max=7803.0, mean=7800.812),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'value_targets': np.ndarray((32,), dtype=float32, min=-1643.959, max=5742.267, mean=1995.438),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'values_bootstrapped': np.ndarray((32,), dtype=float32, min=11.137, max=11.146, mean=11.142),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'vf_preds': np.ndarray((32,), dtype=float32, min=11.137, max=11.148, mean=11.143)}},\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m   'type': 'MultiAgentBatch'}\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:39:37,520\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:39:37,487\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:39:37,491\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 7777,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'learner_stats': { 'cur_lr': 6.326437141979113e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'entropy_coeff': 0.07458633184432983,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'policy_entropy': -16.25764,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'policy_loss': -39235.688,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'var_gnorm': 22.980684,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'vf_loss': 142216900.0},\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'num_grad_updates_lifetime': 7778,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'vf_explained_var': 2.1457672e-06}}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:39:44,911\tDEBUG json_writer.py:81 -- Wrote 7083 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.00024890899658203125s\u001b[32m [repeated 973x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:39:49,935\tDEBUG json_writer.py:81 -- Wrote 6887 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002529621124267578s\u001b[32m [repeated 971x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:39:55,007\tDEBUG json_writer.py:81 -- Wrote 7212 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.00024390220642089844s\u001b[32m [repeated 984x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:40:00,070\tDEBUG json_writer.py:81 -- Wrote 7133 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.00026798248291015625s\u001b[32m [repeated 964x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:40:05,121\tDEBUG json_writer.py:81 -- Wrote 6835 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002701282501220703s\u001b[32m [repeated 974x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:40:10,168\tDEBUG json_writer.py:81 -- Wrote 6994 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002720355987548828s\u001b[32m [repeated 965x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:40:15,174\tDEBUG json_writer.py:81 -- Wrote 7116 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002818107604980469s\u001b[32m [repeated 892x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:40:20,283\tDEBUG json_writer.py:81 -- Wrote 7175 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.000247955322265625s\u001b[32m [repeated 942x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-09-28 10:40:20 (running for 00:06:00.32)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 1800.000: None | Iter 600.000: None | Iter 200.000: None\n",
      "Logical resource usage: 8.0/8 CPUs, 0/0 GPUs\n",
      "Current best trial: d1b9a_00000 with episode_reward_mean=100224.72281296123 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'env': 'worker', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.9673166178130927, 'lr': 7.899221497415072e-05, 'grad_clip': 40.0, 'grad_clip_by': 'global_norm', 'train_batch_size': 32, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x3024912d0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': 'logdir', 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 10, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'DEBUG', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': True, 'monitor': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'use_critic': True, 'use_gae': True, 'lr_schedule': None, 'vf_loss_coeff': 0.26735980037814955, 'entropy_coeff': 0.013264811275713958, 'entropy_coeff_schedule': None, 'microbatch_size': None, '__stdout_file__': None, '__stderr_file__': None, 'lambda': 0.9543274165749964, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf2', 'num_cpus_for_driver': 1, 'num_workers': 3}\n",
      "Result logdir: /Volumes/SSD980/ray/workertune/A2C\n",
      "Number of trials: 2/10 (2 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:40:25,378\tDEBUG json_writer.py:81 -- Wrote 6513 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0002720355987548828s\u001b[32m [repeated 792x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:40:30,431\tDEBUG json_writer.py:81 -- Wrote 7027 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-34-36_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.0003190040588378906s\u001b[32m [repeated 570x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:40:33,070\tINFO json_writer.py:107 -- Writing to new output file <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00001_1_entropy_coeff=0.0746,gamma=0.9586,lambda=0.9267,lr=0.0001,vf_loss_coeff=0.1717_2023-09-28_10-34-20/output-2023-09-28_10-40-33_worker-1_1.json' mode='w' encoding='UTF-8'>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:40:35,532\tDEBUG json_writer.py:81 -- Wrote 7180 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.0002741813659667969s\u001b[32m [repeated 555x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:40:36,961\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:40:36,989\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m { 'count': 11,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((11, 5), dtype=float32, min=-3.29, max=4.244, mean=-1.162),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'action_logp': np.ndarray((11,), dtype=float32, min=0.745, max=1.727, mean=1.399),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'action_prob': np.ndarray((11,), dtype=float32, min=2.106, max=5.622, mean=4.217),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'actions': np.ndarray((11, 2), dtype=float32, min=-2.168, max=1.0, mean=-0.504),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'advantages': np.ndarray((11,), dtype=float32, min=-200.484, max=222.171, mean=35.315),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'agent_index': np.ndarray((11,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'eps_id': np.ndarray((11,), dtype=int64, min=9.114711636978641e+17, max=9.114711636978641e+17, mean=9.11471163697864e+17),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'new_obs': np.ndarray((11, 22), dtype=float32, min=-0.097, max=8794797.0, mean=474769.844),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'obs': np.ndarray((11, 22), dtype=float32, min=-0.112, max=8794797.0, mean=475262.125),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'prev_actions': np.ndarray((11, 2), dtype=float32, min=-2.168, max=1.0, mean=-0.504),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'prev_rewards': np.ndarray((11,), dtype=float32, min=-156.866, max=156.866, mean=19.692),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'rewards': np.ndarray((11,), dtype=float32, min=-156.866, max=156.866, mean=21.727),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           't': np.ndarray((11,), dtype=int64, min=3444.0, max=3454.0, mean=3449.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'terminateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'truncateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'unroll_id': np.ndarray((11,), dtype=int64, min=9516.0, max=9516.0, mean=9516.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'value_targets': np.ndarray((11,), dtype=float32, min=-188.358, max=233.977, mean=47.115),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'values_bootstrapped': np.ndarray((11,), dtype=float32, min=11.709, max=12.126, mean=11.818),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'vf_preds': np.ndarray((11,), dtype=float32, min=11.709, max=12.126, mean=11.8)}},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:40:37,519\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:40:37,523\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 9498,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'grad_gnorm': 39.999992,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'learner_stats': { 'cur_lr': 7.899221236584708e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'entropy_coeff': 0.013264811597764492,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'policy_entropy': -32.668495,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'policy_loss': -6252.67,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'var_gnorm': 23.10935,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'vf_loss': 959672.44},\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'num_grad_updates_lifetime': 9499,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'vf_explained_var': 0.0024297237}}\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:40:37,555\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16201)\u001b[0m 2023-09-28 10:40:35,066\tINFO json_writer.py:107 -- Writing to new output file <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00001_1_entropy_coeff=0.0746,gamma=0.9586,lambda=0.9267,lr=0.0001,vf_loss_coeff=0.1717_2023-09-28_10-34-20/output-2023-09-28_10-40-35_worker-2_1.json' mode='w' encoding='UTF-8'>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:40:40,612\tDEBUG json_writer.py:81 -- Wrote 7220 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.0002529621124267578s\u001b[32m [repeated 680x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:40:37,019\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:40:37,047\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m { 'count': 32,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-3.317, max=1.548, mean=-0.877),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'action_logp': np.ndarray((32,), dtype=float32, min=-1.132, max=1.394, mean=0.672),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'action_prob': np.ndarray((32,), dtype=float32, min=0.322, max=4.029, mean=2.322),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'actions': np.ndarray((32, 2), dtype=float32, min=-0.707, max=1.0, mean=0.071),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'advantages': np.ndarray((32,), dtype=float32, min=-7385.96, max=8987.558, mean=-102.842),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'agent_index': np.ndarray((32,), dtype=int64, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'eps_id': np.ndarray((32,), dtype=int64, min=3.8603741305795603e+17, max=7.142979345318278e+17, mean=5.172523917766917e+17),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'new_obs': np.ndarray((32, 22), dtype=float32, min=-0.03, max=13301597.0, mean=1124753.625),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'obs': np.ndarray((32, 22), dtype=float32, min=-0.03, max=13301597.0, mean=1122334.5),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'prev_actions': np.ndarray((32, 2), dtype=float32, min=-0.707, max=1.0, mean=0.068),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'prev_rewards': np.ndarray((32,), dtype=float32, min=-3250.28, max=6119.04, mean=716.228),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'rewards': np.ndarray((32,), dtype=float32, min=-3250.28, max=6119.04, mean=268.794),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           't': np.ndarray((32,), dtype=int64, min=435.0, max=3641.0, mean=2637.094),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'unroll_id': np.ndarray((32,), dtype=int64, min=9525.0, max=9533.0, mean=9530.5),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'value_targets': np.ndarray((32,), dtype=float32, min=-7373.25, max=9000.408, mean=-89.922),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'values_bootstrapped': np.ndarray((32,), dtype=float32, min=12.71, max=13.049, mean=12.914),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'vf_preds': np.ndarray((32,), dtype=float32, min=12.71, max=13.049, mean=12.92)}},\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m   'type': 'MultiAgentBatch'}\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:40:37,542\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:40:37,545\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 9501,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'learner_stats': { 'cur_lr': 6.326437141979113e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'entropy_coeff': 0.07458633184432983,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'policy_entropy': -18.71217,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'policy_loss': -12352.762,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'var_gnorm': 23.020035,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'vf_loss': 249793400.0},\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'num_grad_updates_lifetime': 9502,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'vf_explained_var': 1.424551e-05}}\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:40:37,578\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:40:45,602\tDEBUG json_writer.py:81 -- Wrote 7212 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.0003829002380371094s\u001b[32m [repeated 807x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:40:50,646\tDEBUG json_writer.py:81 -- Wrote 7113 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.00026702880859375s\u001b[32m [repeated 780x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:40:55,652\tDEBUG json_writer.py:81 -- Wrote 7011 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.00025010108947753906s\u001b[32m [repeated 789x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:41:00,737\tDEBUG json_writer.py:81 -- Wrote 7186 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.0003418922424316406s\u001b[32m [repeated 823x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:41:05,841\tDEBUG json_writer.py:81 -- Wrote 7132 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.00027298927307128906s\u001b[32m [repeated 816x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16202)\u001b[0m 2023-09-28 10:41:06,132\tINFO json_writer.py:107 -- Writing to new output file <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00001_1_entropy_coeff=0.0746,gamma=0.9586,lambda=0.9267,lr=0.0001,vf_loss_coeff=0.1717_2023-09-28_10-34-20/output-2023-09-28_10-41-06_worker-3_1.json' mode='w' encoding='UTF-8'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m =========HRL is done=============\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m Worker is done\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m day: 6304, episode: 20\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m Beginn_Portfolio_Value: 1000000.0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m End Total Assets: 4946806.5\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m Total PnL: 3946806.5\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m =================================\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:41:10,872\tDEBUG json_writer.py:81 -- Wrote 6562 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.0002617835998535156s\u001b[32m [repeated 786x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16203)\u001b[0m 2023-09-28 10:41:07,503\tINFO json_writer.py:107 -- Writing to new output file <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-41-07_worker-3_1.json' mode='w' encoding='UTF-8'>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:41:15,850\tDEBUG json_writer.py:81 -- Wrote 7065 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.000675201416015625s\u001b[32m [repeated 746x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:41:20,954\tDEBUG json_writer.py:81 -- Wrote 7186 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.0002760887145996094s\u001b[32m [repeated 586x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:41:25,975\tDEBUG json_writer.py:81 -- Wrote 7165 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.00025534629821777344s\u001b[32m [repeated 774x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:41:31,096\tDEBUG json_writer.py:81 -- Wrote 7204 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.0002830028533935547s\u001b[32m [repeated 822x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:41:36,105\tDEBUG json_writer.py:81 -- Wrote 6986 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.00024509429931640625s\u001b[32m [repeated 762x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:41:37,001\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:41:37,029\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m { 'count': 11,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((11, 5), dtype=float32, min=-3.402, max=3.675, mean=-1.091),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'action_logp': np.ndarray((11,), dtype=float32, min=0.773, max=2.477, mean=1.946),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'action_prob': np.ndarray((11,), dtype=float32, min=2.165, max=11.909, mean=8.251),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'actions': np.ndarray((11, 2), dtype=float32, min=-1.137, max=1.0, mean=-0.045),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'advantages': np.ndarray((11,), dtype=float32, min=7.621, max=33.399, mean=18.583),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'agent_index': np.ndarray((11,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'eps_id': np.ndarray((11,), dtype=int64, min=4.857668604954635e+17, max=4.857668604954635e+17, mean=4.857668604954635e+17),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'new_obs': np.ndarray((11, 22), dtype=float32, min=0.004, max=7015820.5, mean=365702.281),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'obs': np.ndarray((11, 22), dtype=float32, min=0.004, max=7015820.5, mean=365389.094),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'prev_actions': np.ndarray((11, 2), dtype=float32, min=-1.133, max=1.0, mean=-0.044),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'prev_rewards': np.ndarray((11,), dtype=float32, min=-6.979, max=22.795, mean=4.155),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'rewards': np.ndarray((11,), dtype=float32, min=-6.979, max=15.933, mean=2.806),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           't': np.ndarray((11,), dtype=int64, min=1326.0, max=1336.0, mean=1331.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'terminateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'truncateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'unroll_id': np.ndarray((11,), dtype=int64, min=11049.0, max=11049.0, mean=11049.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'value_targets': np.ndarray((11,), dtype=float32, min=17.948, max=43.725, mean=28.91),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'values_bootstrapped': np.ndarray((11,), dtype=float32, min=10.326, max=10.326, mean=10.326),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'vf_preds': np.ndarray((11,), dtype=float32, min=10.326, max=10.328, mean=10.326)}},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:41:37,567\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:41:37,573\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 11025,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'grad_gnorm': 39.999996,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'learner_stats': { 'cur_lr': 7.899221236584708e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'entropy_coeff': 0.013264811597764492,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'policy_entropy': -72.936134,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'policy_loss': -2070.7683,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'var_gnorm': 23.124077,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'vf_loss': 184151.19},\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'num_grad_updates_lifetime': 11026,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'vf_explained_var': 0.012936115}}\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:41:37,604\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:41:41,138\tDEBUG json_writer.py:81 -- Wrote 7187 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.00028324127197265625s\u001b[32m [repeated 801x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:41:37,070\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:41:37,099\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m { 'count': 32,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-3.687, max=1.932, mean=-0.947),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'action_logp': np.ndarray((32,), dtype=float32, min=-1.318, max=1.876, mean=1.267),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'action_prob': np.ndarray((32,), dtype=float32, min=0.268, max=6.525, mean=4.443),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'actions': np.ndarray((32, 2), dtype=float32, min=-0.374, max=1.0, mean=0.281),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'advantages': np.ndarray((32,), dtype=float32, min=-2415.419, max=1513.673, mean=219.498),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'agent_index': np.ndarray((32,), dtype=int64, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'eps_id': np.ndarray((32,), dtype=int64, min=3.261710502865494e+17, max=9.770404099837709e+17, mean=7.732474642766246e+17),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'new_obs': np.ndarray((32, 22), dtype=float32, min=-0.05, max=5703281.5, mean=497834.594),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'obs': np.ndarray((32, 22), dtype=float32, min=-0.05, max=5703281.5, mean=498117.906),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'prev_actions': np.ndarray((32, 2), dtype=float32, min=-0.366, max=1.0, mean=0.298),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'prev_rewards': np.ndarray((32,), dtype=float32, min=-1265.535, max=1752.175, mean=50.878),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'rewards': np.ndarray((32,), dtype=float32, min=-1265.535, max=808.63, mean=-0.419),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           't': np.ndarray((32,), dtype=int64, min=1271.0, max=2884.0, mean=1777.094),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'unroll_id': np.ndarray((32,), dtype=int64, min=11033.0, max=11044.0, mean=11040.562),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'value_targets': np.ndarray((32,), dtype=float32, min=-2400.653, max=1528.513, mean=234.321),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'values_bootstrapped': np.ndarray((32,), dtype=float32, min=14.78, max=14.842, mean=14.824),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'vf_preds': np.ndarray((32,), dtype=float32, min=14.767, max=14.842, mean=14.823)}},\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m   'type': 'MultiAgentBatch'}\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:41:37,597\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:41:37,600\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 11006,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'learner_stats': { 'cur_lr': 6.326437141979113e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'entropy_coeff': 0.07458633184432983,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'policy_entropy': -34.94245,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'policy_loss': -10127.679,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'var_gnorm': 23.062908,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'vf_loss': 16753712.0},\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'num_grad_updates_lifetime': 11007,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'vf_explained_var': 4.184246e-05}}\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:41:37,635\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:41:46,210\tDEBUG json_writer.py:81 -- Wrote 7218 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.0002460479736328125s\u001b[32m [repeated 843x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=16203)\u001b[0m =========HRL is done=============\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16203)\u001b[0m Worker is done\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16203)\u001b[0m day: 6304, episode: 20\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16203)\u001b[0m Beginn_Portfolio_Value: 1000000.0\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16203)\u001b[0m End Total Assets: 5804685.5\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16203)\u001b[0m Total PnL: 4804686.0\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16203)\u001b[0m =================================\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:41:51,271\tDEBUG json_writer.py:81 -- Wrote 7218 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.00028705596923828125s\u001b[32m [repeated 762x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:41:56,295\tDEBUG json_writer.py:81 -- Wrote 6506 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.00033402442932128906s\u001b[32m [repeated 680x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:42:01,349\tDEBUG json_writer.py:81 -- Wrote 6588 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.00026106834411621094s\u001b[32m [repeated 637x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:42:06,440\tDEBUG json_writer.py:81 -- Wrote 6626 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.000247955322265625s\u001b[32m [repeated 689x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16202)\u001b[0m 2023-09-28 10:42:11,428\tDEBUG json_writer.py:81 -- Wrote 6590 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00001_1_entropy_coeff=0.0746,gamma=0.9586,lambda=0.9267,lr=0.0001,vf_loss_coeff=0.1717_2023-09-28_10-34-20/output-2023-09-28_10-41-06_worker-3_1.json' mode='w' encoding='UTF-8'> in 0.0002999305725097656s\u001b[32m [repeated 755x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:42:16,536\tDEBUG json_writer.py:81 -- Wrote 6676 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.0002579689025878906s\u001b[32m [repeated 803x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-09-28 10:42:20 (running for 00:08:00.32)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 1800.000: None | Iter 600.000: None | Iter 200.000: None\n",
      "Logical resource usage: 8.0/8 CPUs, 0/0 GPUs\n",
      "Current best trial: d1b9a_00001 with episode_reward_mean=93283.18782236197 and parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'env': 'worker', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.9585533658248044, 'lr': 6.326437422966773e-05, 'grad_clip': 40.0, 'grad_clip_by': 'global_norm', 'train_batch_size': 32, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x3024912d0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': 'logdir', 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 10, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'DEBUG', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': True, 'monitor': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'use_critic': True, 'use_gae': True, 'lr_schedule': None, 'vf_loss_coeff': 0.1716786108163829, 'entropy_coeff': 0.07458633087192683, 'entropy_coeff_schedule': None, 'microbatch_size': None, '__stdout_file__': None, '__stderr_file__': None, 'lambda': 0.9267053398702879, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf2', 'num_cpus_for_driver': 1, 'num_workers': 3}\n",
      "Result logdir: /Volumes/SSD980/ray/workertune/A2C\n",
      "Number of trials: 2/10 (2 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:42:21,649\tDEBUG json_writer.py:81 -- Wrote 6631 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.0002932548522949219s\u001b[32m [repeated 797x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:42:26,749\tDEBUG json_writer.py:81 -- Wrote 6523 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.0002608299255371094s\u001b[32m [repeated 775x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:42:31,789\tDEBUG json_writer.py:81 -- Wrote 6648 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.00028061866760253906s\u001b[32m [repeated 768x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:42:36,888\tDEBUG json_writer.py:81 -- Wrote 6643 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.00025010108947753906s\u001b[32m [repeated 837x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:42:37,044\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:42:37,072\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m { 'count': 11,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((11, 5), dtype=float32, min=-24.245, max=4.723, mean=-6.828),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'action_logp': np.ndarray((11,), dtype=float32, min=7.165, max=7.572, mean=7.429),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'action_prob': np.ndarray((11,), dtype=float32, min=1293.602, max=1943.446, mean=1703.302),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'actions': np.ndarray((11, 2), dtype=float32, min=-24.245, max=1.0, mean=-11.622),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'advantages': np.ndarray((11,), dtype=float32, min=-3.344, max=-0.439, mean=-2.064),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'agent_index': np.ndarray((11,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'eps_id': np.ndarray((11,), dtype=int64, min=5.160383814013566e+17, max=5.160383814013566e+17, mean=5.1603838140135654e+17),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'new_obs': np.ndarray((11, 22), dtype=float32, min=0.0, max=11063568.0, mean=541680.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'obs': np.ndarray((11, 22), dtype=float32, min=-0.001, max=11063568.0, mean=541899.75),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'prev_actions': np.ndarray((11, 2), dtype=float32, min=-24.245, max=1.0, mean=-11.621),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'prev_rewards': np.ndarray((11,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'rewards': np.ndarray((11,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           't': np.ndarray((11,), dtype=int64, min=5249.0, max=5259.0, mean=5254.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'terminateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'truncateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'unroll_id': np.ndarray((11,), dtype=int64, min=12556.0, max=12556.0, mean=12556.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'value_targets': np.ndarray((11,), dtype=float32, min=10.096, max=13.001, mean=11.376),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'values_bootstrapped': np.ndarray((11,), dtype=float32, min=13.44, max=13.44, mean=13.44),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'vf_preds': np.ndarray((11,), dtype=float32, min=13.44, max=13.44, mean=13.44)}},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:42:37,613\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:42:37,617\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 12528,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'learner_stats': { 'cur_lr': 7.899221236584708e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'entropy_coeff': 0.013264811597764492,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'policy_entropy': -228.13242,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'policy_loss': -55488.758,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'var_gnorm': 23.74971,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'vf_loss': 78.576744},\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'num_grad_updates_lifetime': 12529,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'vf_explained_var': 2.9325485e-05}}\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:42:37,651\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:42:41,968\tDEBUG json_writer.py:81 -- Wrote 6611 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.0002510547637939453s\u001b[32m [repeated 786x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:42:37,111\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:42:37,148\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m { 'count': 32,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-4.069, max=2.145, mean=-1.08),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'action_logp': np.ndarray((32,), dtype=float32, min=-1.255, max=2.262, mean=1.521),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'action_prob': np.ndarray((32,), dtype=float32, min=0.285, max=9.605, mean=6.033),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'actions': np.ndarray((32, 2), dtype=float32, min=-0.412, max=1.0, mean=0.315),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'advantages': np.ndarray((32,), dtype=float32, min=-2587.262, max=4620.549, mean=1122.098),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'agent_index': np.ndarray((32,), dtype=int64, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'eps_id': np.ndarray((32,), dtype=int64, min=9.980618978481746e+16, max=6.516349223389651e+17, mean=4.3067961085866214e+17),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'new_obs': np.ndarray((32, 22), dtype=float32, min=-0.121, max=15612855.0, mean=1755051.125),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'obs': np.ndarray((32, 22), dtype=float32, min=-0.121, max=15612855.0, mean=1754806.875),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'prev_actions': np.ndarray((32, 2), dtype=float32, min=-0.412, max=1.0, mean=0.315),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'prev_rewards': np.ndarray((32,), dtype=float32, min=-3970.43, max=2233.37, mean=6.553),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'rewards': np.ndarray((32,), dtype=float32, min=-3970.43, max=2233.37, mean=59.134),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           't': np.ndarray((32,), dtype=int64, min=5260.0, max=5364.0, mean=5294.531),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'unroll_id': np.ndarray((32,), dtype=int64, min=12545.0, max=12557.0, mean=12553.25),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'value_targets': np.ndarray((32,), dtype=float32, min=-2570.951, max=4636.997, mean=1138.542),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'values_bootstrapped': np.ndarray((32,), dtype=float32, min=16.311, max=16.534, mean=16.448),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'vf_preds': np.ndarray((32,), dtype=float32, min=16.307, max=16.534, mean=16.444)}},\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m   'type': 'MultiAgentBatch'}\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:42:37,661\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:42:37,674\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 12515,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'grad_gnorm': 40.000004,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'learner_stats': { 'cur_lr': 6.326437141979113e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'entropy_coeff': 0.07458633184432983,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'policy_entropy': -48.083626,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'policy_loss': -62187.867,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'var_gnorm': 23.100475,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'vf_loss': 84121544.0},\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'num_grad_updates_lifetime': 12516,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'vf_explained_var': 4.1425228e-05}}\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:42:37,706\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:42:46,990\tDEBUG json_writer.py:81 -- Wrote 6647 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.0002770423889160156s\u001b[32m [repeated 784x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:42:52,070\tDEBUG json_writer.py:81 -- Wrote 6632 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.0002541542053222656s\u001b[32m [repeated 794x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:42:57,173\tDEBUG json_writer.py:81 -- Wrote 6638 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.0002951622009277344s\u001b[32m [repeated 618x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:43:02,237\tDEBUG json_writer.py:81 -- Wrote 6601 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.00023794174194335938s\u001b[32m [repeated 714x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:43:07,251\tDEBUG json_writer.py:81 -- Wrote 6451 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.00024819374084472656s\u001b[32m [repeated 759x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:43:12,279\tDEBUG json_writer.py:81 -- Wrote 6517 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.0002300739288330078s\u001b[32m [repeated 720x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:43:17,330\tDEBUG json_writer.py:81 -- Wrote 6597 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.00024318695068359375s\u001b[32m [repeated 765x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:43:22,366\tDEBUG json_writer.py:81 -- Wrote 6569 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.00027632713317871094s\u001b[32m [repeated 746x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:43:27,435\tDEBUG json_writer.py:81 -- Wrote 6521 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.0002598762512207031s\u001b[32m [repeated 784x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16200)\u001b[0m 2023-09-28 10:43:32,424\tDEBUG json_writer.py:81 -- Wrote 6560 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-35_worker-2_1.json' mode='w' encoding='UTF-8'> in 0.0002601146697998047s\u001b[32m [repeated 699x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:43:37,097\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m { 'count': 11,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((11, 5), dtype=float32, min=-53.792, max=4.724, mean=-14.616),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'action_logp': np.ndarray((11,), dtype=float32, min=16.955, max=16.963, mean=16.963),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'action_prob': np.ndarray((11,), dtype=float32, min=23086612.0, max=23288816.0, mean=23270288.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'actions': np.ndarray((11, 2), dtype=float32, min=-53.792, max=1.0, mean=-26.395),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'advantages': np.ndarray((11,), dtype=float32, min=-3.344, max=-0.439, mean=-2.064),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'agent_index': np.ndarray((11,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'eps_id': np.ndarray((11,), dtype=int64, min=9.197204309425508e+17, max=9.197204309425508e+17, mean=9.197204309425508e+17),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'new_obs': np.ndarray((11, 22), dtype=float32, min=0.0, max=4564125.5, mean=249393.125),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'obs': np.ndarray((11, 22), dtype=float32, min=0.0, max=4621599.5, mean=250279.828),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'prev_actions': np.ndarray((11, 2), dtype=float32, min=-53.792, max=1.0, mean=-26.394),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'prev_rewards': np.ndarray((11,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'rewards': np.ndarray((11,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           't': np.ndarray((11,), dtype=int64, min=2251.0, max=2261.0, mean=2256.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'terminateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'truncateds': np.ndarray((11,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'unroll_id': np.ndarray((11,), dtype=int64, min=14008.0, max=14008.0, mean=14008.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'value_targets': np.ndarray((11,), dtype=float32, min=10.098, max=13.003, mean=11.378),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'values_bootstrapped': np.ndarray((11,), dtype=float32, min=13.442, max=13.442, mean=13.442),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m                                           'vf_preds': np.ndarray((11,), dtype=float32, min=13.442, max=13.442, mean=13.442)}},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:43:37,113\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:43:37,540\tDEBUG json_writer.py:81 -- Wrote 6567 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.0002608299255371094s\u001b[32m [repeated 612x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:43:37,659\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:43:37,663\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 13975,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'learner_stats': { 'cur_lr': 7.899221236584708e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'entropy_coeff': 0.013264811597764492,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'policy_entropy': -530.49536,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'policy_loss': -8874927000000.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'var_gnorm': 24.295053,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                                          'vf_loss': 78.60588},\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'num_grad_updates_lifetime': 13976,\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m                       'vf_explained_var': 4.053116e-05}}\n",
      "\u001b[2m\u001b[36m(A2C pid=16177)\u001b[0m 2023-09-28 10:43:37,698\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:43:37,183\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m { 'count': 32,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-3.959, max=1.689, mean=-0.991),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'action_logp': np.ndarray((32,), dtype=float32, min=-0.294, max=2.278, mean=1.464),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'action_prob': np.ndarray((32,), dtype=float32, min=0.746, max=9.76, mean=5.506),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'actions': np.ndarray((32, 2), dtype=float32, min=0.0, max=1.0, mean=0.472),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'advantages': np.ndarray((32,), dtype=float32, min=-604.824, max=242.931, mean=-202.239),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'agent_index': np.ndarray((32,), dtype=int64, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'eps_id': np.ndarray((32,), dtype=int64, min=3.385071235202506e+17, max=8.869982543743789e+17, mean=5.782249564299775e+17),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'new_obs': np.ndarray((32, 22), dtype=float32, min=-273670.5, max=5933746.5, mean=352649.094),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'obs': np.ndarray((32, 22), dtype=float32, min=-272203.594, max=5933746.5, mean=353441.688),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'prev_actions': np.ndarray((32, 2), dtype=float32, min=0.0, max=1.0, mean=0.472),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'prev_rewards': np.ndarray((32,), dtype=float32, min=-338.2, max=169.385, mean=-57.413),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'rewards': np.ndarray((32,), dtype=float32, min=-338.2, max=169.385, mean=-49.699),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           't': np.ndarray((32,), dtype=int64, min=920.0, max=2283.0, mean=1855.031),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'unroll_id': np.ndarray((32,), dtype=int64, min=13997.0, max=14010.0, mean=14005.938),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'value_targets': np.ndarray((32,), dtype=float32, min=-586.515, max=261.24, mean=-183.933),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'values_bootstrapped': np.ndarray((32,), dtype=float32, min=18.301, max=18.309, mean=18.306),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                           'vf_preds': np.ndarray((32,), dtype=float32, min=18.298, max=18.309, mean=18.306)}},\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m   'type': 'MultiAgentBatch'}\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16198)\u001b[0m 2023-09-28 10:43:37,157\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:43:42,603\tDEBUG json_writer.py:81 -- Wrote 6592 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.00028324127197265625s\u001b[32m [repeated 701x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:43:37,718\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:43:37,722\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m { 'default_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 13963,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'learner_stats': { 'cur_lr': 6.326437141979113e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'entropy_coeff': 0.07458633184432983,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'policy_entropy': -47.65142,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'policy_loss': 8733.984,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'var_gnorm': 23.144724,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                                          'vf_loss': 1850700.1},\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'num_grad_updates_lifetime': 13964,\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m                       'vf_explained_var': -8.46386e-06}}\n",
      "\u001b[2m\u001b[36m(A2C pid=16178)\u001b[0m 2023-09-28 10:43:37,751\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=16199)\u001b[0m 2023-09-28 10:43:47,622\tDEBUG json_writer.py:81 -- Wrote 6581 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/workertune/A2C/A2C_worker_d1b9a_00000_0_entropy_coeff=0.0133,gamma=0.9673,lambda=0.9543,lr=0.0001,vf_loss_coeff=0.2674_2023-09-28_10-34-20/output-2023-09-28_10-40-34_worker-1_1.json' mode='w' encoding='UTF-8'> in 0.0007741451263427734s\u001b[32m [repeated 507x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if os.name == 'nt':\n",
    "    path_to_save = \"C:\\\\GitHub\\\\ray\\\\workertune\"\n",
    "else:\n",
    "    path_to_save = \"/Volumes/SSD980/ray/workertune1\"\n",
    "\n",
    "\n",
    "env = WorkerStandAlone()\n",
    "def env_creator(env_config):\n",
    "    return WorkerStandAlone()  \n",
    "\n",
    "register_env(\"worker\", env_creator)\n",
    "\n",
    "asha_scheduler = ASHAScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    max_t=5000,\n",
    "    grace_period=200,\n",
    "    reduction_factor=3,\n",
    "    brackets=1,\n",
    ")\n",
    "\n",
    "param_space = {\n",
    "    \"env\": \"worker\",\n",
    "    \"rollout_fragment_length\": \"auto\",\n",
    "    \"framework\": \"tf2\",\n",
    "    \"lr\": tune.uniform(1e-5,1e-4),\n",
    "    \"gamma\": tune.uniform(0.95, 0.9999),\n",
    "    \"lambda\": tune.uniform(0.9,1.0),\n",
    "    \"entropy_coeff\": tune.uniform(0.01,0.1),\n",
    "    \"vf_loss_coeff\": tune.uniform(0.1,0.3),\n",
    "    \"num_workers\": 2, \n",
    "    \"log_level\": \"ERROR\",\n",
    "    \"output\": \"logdir\",\n",
    "    \"monitor\": False,\n",
    "}\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"A2C\", \n",
    "    metric=\"episode_reward_mean\", \n",
    "    num_samples=10,\n",
    "    resume=False,\n",
    "    mode=\"max\",\n",
    "    config=param_space, \n",
    "    local_dir=path_to_save,\n",
    "    search_alg=None,\n",
    "    scheduler=asha_scheduler,\n",
    "    progress_reporter=CLIReporter(max_progress_rows=10,max_report_frequency=120),\n",
    "    max_concurrent_trials=2,\n",
    "    #checkpoint_config not checked yet\n",
    "    checkpoint_config={\n",
    "        \"num_to_keep\": 3,\n",
    "        \"checkpoint_score_attribute\": \"episode_reward_mean\",\n",
    "        \"checkpoint_score_order\": \"max\",\n",
    "        \"checkpoint_frequency\": 10\n",
    "    }\n",
    "    )\n",
    "\n",
    "print(pretty_print(analysis.last_result))\n",
    "print(\"Best hyperparameters found were: \", pretty_print(analysis.best_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WorkerStandAlone()\n",
    "def env_creator(env_config):\n",
    "    return WorkerStandAlone()  \n",
    "\n",
    "register_env(\"worker\", env_creator)\n",
    "\n",
    "\n",
    "\n",
    "analysis = tune.run(\n",
    "        \"A2C\",\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        config={\n",
    "            \"env\": \"worker\",\n",
    "            \"env_config\": {\"initial_capital\": 1e6},\n",
    "            \"lr\": tune.uniform(1e-5, 1e-4),\n",
    "            \"train_batch_size\": tune.choice([10000, 20000, 40000]),\n",
    "}, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========HRL is done=============\n",
      "Worker is done\n",
      "day: 6304, episode: 1\n",
      "Beginn_Portfolio_Value: 1000000.0\n",
      "End Total Assets: 2301572.0\n",
      "Total PnL: 1301572.875\n",
      "=================================\n",
      "Episode finished!\n",
      "=========HRL is done=============\n",
      "Worker is done\n",
      "day: 6304, episode: 2\n",
      "Beginn_Portfolio_Value: 1000000.0\n",
      "End Total Assets: 5299982.5\n",
      "Total PnL: 4299987.0\n",
      "=================================\n",
      "Episode finished!\n",
      "=========HRL is done=============\n",
      "Worker is done\n",
      "day: 6304, episode: 3\n",
      "Beginn_Portfolio_Value: 1000000.0\n",
      "End Total Assets: 1822478.875\n",
      "Total PnL: 822476.375\n",
      "=================================\n",
      "Episode finished!\n",
      "=========HRL is done=============\n",
      "Worker is done\n",
      "day: 6304, episode: 4\n",
      "Beginn_Portfolio_Value: 1000000.0\n",
      "End Total Assets: 1857416.875\n",
      "Total PnL: 857417.625\n",
      "=================================\n",
      "Episode finished!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "n_iterations = 30000\n",
    "\n",
    "\n",
    "env = WorkerStandAlone(data=data)\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    action = env.action_space.sample()  # Generate random actions for the manager\n",
    "    worker_dones = {...}  # You need to provide the values for worker_dones\n",
    "    worker_truncateds = {...}  # You need to provide the values for worker_truncateds\n",
    "\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        print(\"Episode finished!\")\n",
    "        state = env.reset()\n",
    "    else:\n",
    "        state = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-28 08:39:25,055\tINFO tune.py:657 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/tune/tune.py:258: UserWarning: Passing a `local_dir` is deprecated and will be removed in the future. Pass `storage_path` instead or set the `RAY_AIR_LOCAL_CACHE_DIR` environment variable instead.\n",
      "  warnings.warn(\n",
      "2023-09-28 08:39:25,087\tWARNING deprecation.py:50 -- DeprecationWarning: `build_tf_policy` has been deprecated. This will raise an error in the future!\n",
      "2023-09-28 08:39:25,090\tWARNING deprecation.py:50 -- DeprecationWarning: `build_policy_class` has been deprecated. This will raise an error in the future!\n",
      "2023-09-28 08:39:25,120\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "2023-09-28 08:39:25,167\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(pid=5198)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:32,404\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a2c/` has been deprecated. Use `rllib_contrib/a2c/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:32,404\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a3c/` has been deprecated. Use `rllib_contrib/a3c/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:32,405\tWARNING algorithm_config.py:656 -- Cannot create A2CConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=5227)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:32,404\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a3c/` has been deprecated. Use `rllib_contrib/a3c/` instead. This will raise an error in the future!\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:32,405\tWARNING algorithm_config.py:656 -- Cannot create A2CConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,681\tDEBUG rollout_worker.py:1761 -- Creating policy for ABT.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,683\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(0.0, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,688\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,688\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,688\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,688\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,688\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,688\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,689\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (14,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,689\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,689\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,690\tDEBUG catalog.py:793 -- Created preprocessor <ray.rllib.models.preprocessors.DictFlatteningPreprocessor object at 0x172c88850>: Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)) -> (23,)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,697\tDEBUG rollout_worker.py:1761 -- Creating policy for AMGN.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,702\tDEBUG rollout_worker.py:1761 -- Creating policy for BDX.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,708\tDEBUG rollout_worker.py:1761 -- Creating policy for BMY.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,710\tDEBUG rollout_worker.py:1761 -- Creating policy for HUM.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,712\tDEBUG rollout_worker.py:1761 -- Creating policy for JNJ.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,713\tDEBUG rollout_worker.py:1761 -- Creating policy for LLY.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,714\tDEBUG rollout_worker.py:1761 -- Creating policy for MDT.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,715\tDEBUG rollout_worker.py:1761 -- Creating policy for MRK.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,716\tDEBUG rollout_worker.py:1761 -- Creating policy for PFE.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,719\tDEBUG rollout_worker.py:1761 -- Creating policy for SYK.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,722\tDEBUG rollout_worker.py:1761 -- Creating policy for TMO.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,723\tDEBUG rollout_worker.py:1761 -- Creating policy for UNH.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,725\tDEBUG rollout_worker.py:1761 -- Creating policy for manager_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,725\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,725\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,726\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,726\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,727\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,727\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,728\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,728\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,729\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,729\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,729\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,730\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,730\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,731\tDEBUG catalog.py:793 -- Created preprocessor <ray.rllib.models.preprocessors.DictFlatteningPreprocessor object at 0x173e25e70>: Dict('ABT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'AMGN.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BDX.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BMY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'HUM.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'JNJ.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'LLY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MDT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MRK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'PFE.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'SYK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'TMO.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'UNH.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))) -> (52,)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,756\tINFO eager_tf_policy_v2.py:80 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5225)\u001b[0m 2023-09-28 08:39:41,827\tINFO policy.py:1294 -- Policy (worker=2) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,865\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,865\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_modelv2.TFModelV2` has been deprecated. Use `ray.rllib.core.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,878\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.misc.normc_initializer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,971\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,971\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,971\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,975\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,975\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,978\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:42,065\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.MultiActionDistribution` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfMultiDistribution` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:42,067\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.DiagGaussian` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfDiagGaussian` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:42,070\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.TFActionDistribution` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfDistribution` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:42,097\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.Categorical` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.Categorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,978\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,978\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,978\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,980\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,980\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,980\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,981\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,981\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,981\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,983\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,983\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,983\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,984\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,984\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,984\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,986\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,986\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,986\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,987\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,987\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,987\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,989\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,989\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,989\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,990\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,990\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,990\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,991\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,991\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,991\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,993\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,993\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,993\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,994\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,994\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,994\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5225)\u001b[0m 2023-09-28 08:39:42,933\tINFO json_writer.py:50 -- You are using JSONWriter. It is recommended to use DatasetWriter instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5225)\u001b[0m 2023-09-28 08:39:42,933\tDEBUG rollout_worker.py:645 -- Created rollout worker with env <ray.rllib.env.multi_agent_env.MultiAgentEnvWrapper object at 0x2e785ffa0> (<HRL instance>), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['MDT.US', 'SYK.US', 'AMGN.US', 'JNJ.US', 'TMO.US', 'BDX.US', 'HUM.US', 'UNH.US', 'LLY.US', 'ABT.US', 'manager_policy', 'PFE.US', 'MRK.US', 'BMY.US']>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:42,955\tDEBUG rollout_worker.py:645 -- Created rollout worker with env <ray.rllib.env.multi_agent_env.MultiAgentEnvWrapper object at 0x172c88370> (<HRL instance>), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['SYK.US', 'ABT.US', 'UNH.US', 'TMO.US', 'manager_policy', 'BMY.US', 'JNJ.US', 'PFE.US', 'LLY.US', 'HUM.US', 'MRK.US', 'BDX.US', 'AMGN.US', 'MDT.US']>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5226)\u001b[0m 2023-09-28 08:39:42,968\tDEBUG rollout_worker.py:645 -- Created rollout worker with env <ray.rllib.env.multi_agent_env.MultiAgentEnvWrapper object at 0x17d778490> (<HRL instance>), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['LLY.US', 'BDX.US', 'PFE.US', 'BMY.US', 'TMO.US', 'ABT.US', 'HUM.US', 'manager_policy', 'SYK.US', 'AMGN.US', 'JNJ.US', 'UNH.US', 'MRK.US', 'MDT.US']>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:43,008\tDEBUG rollout_worker.py:645 -- Created rollout worker with env <ray.rllib.env.multi_agent_env.MultiAgentEnvWrapper object at 0x17fd27fa0> (<HRL instance>), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['ABT.US', 'BMY.US', 'MRK.US', 'BDX.US', 'HUM.US', 'UNH.US', 'manager_policy', 'PFE.US', 'SYK.US', 'JNJ.US', 'LLY.US', 'AMGN.US', 'TMO.US', 'MDT.US']>\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,080\tINFO worker_set.py:297 -- Inferred observation/action spaces from remote worker (local worker has no env): {'SYK.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'ABT.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'UNH.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'TMO.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'manager_policy': (Dict('ABT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'AMGN.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BDX.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BMY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'HUM.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'JNJ.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'LLY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MDT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MRK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'PFE.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'SYK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'TMO.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'UNH.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))), Dict('ABT.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'AMGN.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'BDX.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'BMY.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'HUM.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'JNJ.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'LLY.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'MDT.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'MRK.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'PFE.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'SYK.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'TMO.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'UNH.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)))), 'BMY.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'JNJ.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'PFE.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'LLY.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'HUM.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'MRK.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'BDX.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'AMGN.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'MDT.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), '__env__': (Dict('ABT.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'AMGN.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'BDX.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'BMY.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'HUM.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'JNJ.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'LLY.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'MDT.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'MRK.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'PFE.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'SYK.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'TMO.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'UNH.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'manager': Dict('ABT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'AMGN.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BDX.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BMY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'HUM.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'JNJ.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'LLY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MDT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MRK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'PFE.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'SYK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'TMO.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'UNH.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)))), Dict('ABT.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'AMGN.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'BDX.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'BMY.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'HUM.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'JNJ.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'LLY.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'MDT.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'MRK.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'PFE.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'SYK.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'TMO.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'UNH.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'manager': Dict('ABT.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'AMGN.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'BDX.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'BMY.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'HUM.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'JNJ.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'LLY.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'MDT.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'MRK.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'PFE.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'SYK.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'TMO.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'UNH.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)))))}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5227)\u001b[0m 2023-09-28 08:39:43,050\tDEBUG rollout_worker.py:645 -- Created rollout worker with env <ray.rllib.env.multi_agent_env.MultiAgentEnvWrapper object at 0x17af78490> (<HRL instance>), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['HUM.US', 'ABT.US', 'TMO.US', 'manager_policy', 'PFE.US', 'SYK.US', 'MDT.US', 'MRK.US', 'UNH.US', 'BDX.US', 'JNJ.US', 'AMGN.US', 'LLY.US', 'BMY.US']>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5222)\u001b[0m 2023-09-28 08:39:43,086\tDEBUG rollout_worker.py:645 -- Created rollout worker with env <ray.rllib.env.multi_agent_env.MultiAgentEnvWrapper object at 0x17db60460> (<HRL instance>), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['manager_policy', 'UNH.US', 'JNJ.US', 'HUM.US', 'LLY.US', 'PFE.US', 'SYK.US', 'MDT.US', 'BMY.US', 'TMO.US', 'BDX.US', 'MRK.US', 'AMGN.US', 'ABT.US']>\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,129\tINFO worker_set.py:297 -- Inferred observation/action spaces from remote worker (local worker has no env): {'manager_policy': (Dict('ABT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'AMGN.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BDX.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BMY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'HUM.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'JNJ.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'LLY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MDT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MRK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'PFE.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'SYK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'TMO.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'UNH.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))), Dict('ABT.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'AMGN.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'BDX.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'BMY.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'HUM.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'JNJ.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'LLY.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'MDT.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'MRK.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'PFE.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'SYK.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'TMO.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'UNH.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)))), 'UNH.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'JNJ.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'HUM.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'LLY.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'PFE.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'SYK.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'MDT.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'BMY.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'TMO.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'BDX.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'MRK.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'AMGN.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'ABT.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), '__env__': (Dict('ABT.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'AMGN.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'BDX.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'BMY.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'HUM.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'JNJ.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'LLY.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'MDT.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'MRK.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'PFE.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'SYK.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'TMO.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'UNH.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'manager': Dict('ABT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'AMGN.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BDX.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BMY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'HUM.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'JNJ.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'LLY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MDT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MRK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'PFE.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'SYK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'TMO.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'UNH.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)))), Dict('ABT.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'AMGN.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'BDX.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'BMY.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'HUM.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'JNJ.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'LLY.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'MDT.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'MRK.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'PFE.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'SYK.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'TMO.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'UNH.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'manager': Dict('ABT.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'AMGN.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'BDX.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'BMY.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'HUM.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'JNJ.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'LLY.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'MDT.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'MRK.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'PFE.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'SYK.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'TMO.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'UNH.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)))))}\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,226\tINFO policy.py:1294 -- Policy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,893\tINFO rollout_worker.py:1742 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['BDX.US', 'MDT.US', 'TMO.US', 'MRK.US', 'PFE.US', 'manager_policy', 'AMGN.US', 'HUM.US', 'UNH.US', 'LLY.US', 'JNJ.US', 'SYK.US', 'BMY.US', 'ABT.US']>\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,893\tINFO rollout_worker.py:1743 -- Built preprocessor map: {'ABT.US': None, 'AMGN.US': None, 'BDX.US': None, 'BMY.US': None, 'HUM.US': None, 'JNJ.US': None, 'LLY.US': None, 'MDT.US': None, 'MRK.US': None, 'PFE.US': None, 'SYK.US': None, 'TMO.US': None, 'UNH.US': None, 'manager_policy': None}\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,895\tINFO rollout_worker.py:550 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,895\tDEBUG rollout_worker.py:645 -- Created rollout worker with env None (None), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['BDX.US', 'MDT.US', 'TMO.US', 'MRK.US', 'PFE.US', 'manager_policy', 'AMGN.US', 'HUM.US', 'UNH.US', 'LLY.US', 'JNJ.US', 'SYK.US', 'BMY.US', 'ABT.US']>\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,911\tINFO algorithm_config.py:3667 -- Executing eagerly (framework='tf2'), with eager_tracing=True. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m Trainable.setup took 11.507 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,946\tINFO rollout_worker.py:1742 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['UNH.US', 'MRK.US', 'AMGN.US', 'PFE.US', 'TMO.US', 'JNJ.US', 'BMY.US', 'BDX.US', 'HUM.US', 'ABT.US', 'LLY.US', 'manager_policy', 'SYK.US', 'MDT.US']>\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,948\tDEBUG rollout_worker.py:645 -- Created rollout worker with env None (None), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['UNH.US', 'MRK.US', 'AMGN.US', 'PFE.US', 'TMO.US', 'JNJ.US', 'BMY.US', 'BDX.US', 'HUM.US', 'ABT.US', 'LLY.US', 'manager_policy', 'SYK.US', 'MDT.US']>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:44,107\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5227)\u001b[0m WARNING:tensorflow:5 out of the last 5 calls to <bound method with_lock.<locals>.wrapper of A3CTF2Policy_traced> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5227)\u001b[0m WARNING:tensorflow:5 out of the last 5 calls to <bound method with_lock.<locals>.wrapper of A3CTF2Policy_traced> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[2m\u001b[36m(pid=5222)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5227)\u001b[0m 2023-09-28 08:39:46,880\tINFO json_writer.py:107 -- Writing to new output file <_io.TextIOWrapper name='/Volumes/SSD980/ray/results/tunerun2/A2C/A2C_hrl_c3ec2_00001_1_entropy_coeff=0.0254,gamma=0.9723,lambda=0.9582,lr=0.0000,vf_loss_coeff=0.1742_2023-09-28_08-39-25/output-2023-09-28_08-39-46_worker-3_0.json' mode='w' encoding='UTF-8'>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5227)\u001b[0m 2023-09-28 08:39:46,881\tDEBUG json_writer.py:81 -- Wrote 100336 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/results/tunerun2/A2C/A2C_hrl_c3ec2_00001_1_entropy_coeff=0.0254,gamma=0.9723,lambda=0.9582,lr=0.0000,vf_loss_coeff=0.1742_2023-09-28_08-39-25/output-2023-09-28_08-39-46_worker-3_0.json' mode='w' encoding='UTF-8'> in 0.004518985748291016s\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,179\tDEBUG rollout_worker.py:1761 -- Creating policy for ABT.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,925\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(0.0, inf, (1,), float32)\u001b[32m [repeated 311x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,943\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\u001b[32m [repeated 4463x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,257\tDEBUG catalog.py:793 -- Created preprocessor <ray.rllib.models.preprocessors.DictFlatteningPreprocessor object at 0x15c187910>: Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)) -> (23,)\u001b[32m [repeated 103x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,235\tDEBUG rollout_worker.py:1761 -- Creating policy for AMGN.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,237\tDEBUG rollout_worker.py:1761 -- Creating policy for BDX.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,240\tDEBUG rollout_worker.py:1761 -- Creating policy for BMY.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,242\tDEBUG rollout_worker.py:1761 -- Creating policy for HUM.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,245\tDEBUG rollout_worker.py:1761 -- Creating policy for JNJ.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,247\tDEBUG rollout_worker.py:1761 -- Creating policy for LLY.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,249\tDEBUG rollout_worker.py:1761 -- Creating policy for MDT.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,252\tDEBUG rollout_worker.py:1761 -- Creating policy for MRK.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,253\tDEBUG rollout_worker.py:1761 -- Creating policy for PFE.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,254\tDEBUG rollout_worker.py:1761 -- Creating policy for SYK.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,255\tDEBUG rollout_worker.py:1761 -- Creating policy for TMO.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,256\tDEBUG rollout_worker.py:1761 -- Creating policy for UNH.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,257\tDEBUG rollout_worker.py:1761 -- Creating policy for manager_policy\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,943\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\u001b[32m [repeated 299x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,263\tDEBUG catalog.py:793 -- Created preprocessor <ray.rllib.models.preprocessors.DictFlatteningPreprocessor object at 0x15c187d30>: Dict('ABT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'AMGN.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BDX.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BMY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'HUM.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'JNJ.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'LLY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MDT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MRK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'PFE.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'SYK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'TMO.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'UNH.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))) -> (52,)\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,819\tINFO eager_tf_policy_v2.py:80 -- Creating TF-eager policy running on CPU.\u001b[32m [repeated 111x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5222)\u001b[0m 2023-09-28 08:39:42,974\tINFO policy.py:1294 -- Policy (worker=1) running on CPU.\u001b[32m [repeated 83x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,305\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,306\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_modelv2.TFModelV2` has been deprecated. Use `ray.rllib.core.rl_module.RLModule` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,312\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.misc.normc_initializer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:46,952\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:46,961\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m \n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m { 'count': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'policy_batches': { 'ABT.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.017, max=0.002, mean=-0.006),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-4.549, max=-2.006, mean=-2.427),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.011, max=0.135, mean=0.099),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-2.252, max=2.0, mean=0.374),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-119.046, max=0.744, mean=-25.326),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-12516.689, max=7075415.0, mean=304317.688),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-8074.749, max=7075415.0, mean=301944.406),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-2.252, max=2.0, mean=0.356),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-70.369, max=2.799, mean=-2.861),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-70.369, max=2.799, mean=-4.272),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=3.0, max=3.0, mean=3.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-119.055, max=0.734, mean=-25.336),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.011, max=-0.008, mean=-0.01),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=-0.011, max=-0.008, mean=-0.01)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'AMGN.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.016, max=0.008, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'action_logp': np.ndarray((32,), dtype=float32, min=-5.268, max=-2.016, mean=-2.621),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'action_prob': np.ndarray((32,), dtype=float32, min=0.005, max=0.133, mean=0.088),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'actions': np.ndarray((32, 2), dtype=float32, min=-1.805, max=2.535, mean=0.498),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'advantages': np.ndarray((32,), dtype=float32, min=-16.783, max=32.969, mean=0.541),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'agent_index': np.ndarray((32,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'new_obs': np.ndarray((32, 23), dtype=float32, min=-1699.332, max=2968512.0, mean=138661.016),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'obs': np.ndarray((32, 23), dtype=float32, min=-1699.332, max=2968512.0, mean=135816.062),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'prev_actions': np.ndarray((32, 2), dtype=float32, min=-1.759, max=2.535, mean=0.495),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'prev_rewards': np.ndarray((32,), dtype=float32, min=-32.967, max=32.967, mean=1.236),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'rewards': np.ndarray((32,), dtype=float32, min=-32.967, max=32.967, mean=0.362),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'unroll_id': np.ndarray((32,), dtype=int64, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'value_targets': np.ndarray((32,), dtype=float32, min=-16.788, max=32.964, mean=0.538),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.006, max=-0.0, mean=-0.003),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'vf_preds': np.ndarray((32,), dtype=float32, min=-0.006, max=-0.0, mean=-0.003)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'BDX.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.011, max=0.013, mean=0.001),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-5.208, max=-2.019, mean=-2.442),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.005, max=0.133, mean=0.098),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-2.503, max=2.0, mean=0.456),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-16.767, max=93.009, mean=29.683),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=3.0, max=3.0, mean=3.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-967.66, max=1524314.375, mean=69080.148),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-967.66, max=1524314.375, mean=69080.672),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-2.503, max=2.0, mean=0.409),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-13.873, max=60.171, mean=1.937),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-13.873, max=60.171, mean=3.326),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=7.0, max=7.0, mean=7.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-16.76, max=93.011, mean=29.688),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=0.002, max=0.008, mean=0.005),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=0.002, max=0.008, mean=0.005)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'BMY.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.013, max=-0.001, mean=-0.007),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-4.304, max=-2.014, mean=-2.675),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.014, max=0.133, mean=0.084),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-2.141, max=2.0, mean=0.487),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-119.992, max=12.26, mean=-18.385),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=4.0, max=4.0, mean=4.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-9953.668, max=2857173.5, mean=127917.609),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-9953.668, max=2857173.5, mean=127796.117),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-2.107, max=2.0, mean=0.479),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-66.432, max=29.68, mean=-3.457),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-66.432, max=29.68, mean=-3.688),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=9.0, max=9.0, mean=9.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-119.998, max=12.254, mean=-18.389),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.007, max=-0.002, mean=-0.004),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=-0.007, max=-0.002, mean=-0.004)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'HUM.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.013, max=0.01, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-3.802, max=-2.009, mean=-2.555),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.022, max=0.134, mean=0.087),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-1.882, max=2.0, mean=0.292),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-27.472, max=79.34, mean=15.815),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-9967.238, max=345678.0, mean=21035.688),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-9967.238, max=345678.0, mean=20218.762),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-1.882, max=2.0, mean=0.296),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-101.242, max=36.792, mean=1.552),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-101.242, max=36.792, mean=1.066),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=11.0, max=11.0, mean=11.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-27.474, max=79.338, mean=15.814),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.004, max=0.006, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=-0.004, max=0.006, mean=-0.001)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'JNJ.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.007, max=0.006, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-4.698, max=-2.018, mean=-2.508),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.009, max=0.133, mean=0.097),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-2.329, max=2.0, mean=0.467),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-19.599, max=9.658, mean=-5.962),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=6.0, max=6.0, mean=6.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-1870.901, max=9122048.0, mean=395924.469),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-1870.901, max=9122048.0, mean=395740.344),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-2.329, max=2.0, mean=0.435),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-16.161, max=11.609, mean=-0.734),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-16.161, max=11.609, mean=-0.987),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=13.0, max=13.0, mean=13.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-19.592, max=9.666, mean=-5.955),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=0.006, max=0.007, mean=0.007),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=0.006, max=0.007, mean=0.007)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'LLY.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.011, max=0.003, mean=-0.004),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-4.219, max=-2.026, mean=-2.444),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.015, max=0.132, mean=0.097),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-2.096, max=2.0, mean=0.504),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-65.822, max=-0.0, mean=-14.998),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=7.0, max=7.0, mean=7.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-2258.431, max=2188128.0, mean=98849.945),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-2219.708, max=2188128.0, mean=97754.805),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-2.096, max=2.0, mean=0.449),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-28.513, max=29.715, mean=-1.255),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-28.513, max=29.715, mean=-1.379),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=15.0, max=15.0, mean=15.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-65.819, max=0.004, mean=-14.995),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=0.002, max=0.006, mean=0.003),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=0.002, max=0.006, mean=0.003)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'MDT.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.012, max=0.012, mean=0.001),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-4.825, max=-2.002, mean=-2.636),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.008, max=0.135, mean=0.087),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-1.864, max=2.364, mean=0.53),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-51.101, max=-1.999, mean=-23.288),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=8.0, max=8.0, mean=8.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-3837.633, max=3927296.0, mean=180036.516),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-3423.231, max=3927296.0, mean=176480.812),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-1.864, max=2.364, mean=0.406),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-21.105, max=13.535, mean=-3.036),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-21.105, max=13.535, mean=-3.427),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=17.0, max=17.0, mean=17.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-51.109, max=-2.008, mean=-23.296),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.01, max=-0.007, mean=-0.009),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=-0.01, max=-0.007, mean=-0.009)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'MRK.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.003, max=0.006, mean=0.002),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-4.802, max=-2.019, mean=-2.55),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.008, max=0.133, mean=0.092),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-2.351, max=2.194, mean=0.626),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-45.727, max=20.148, mean=-8.865),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=9.0, max=9.0, mean=9.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-4366.6, max=7334398.5, mean=315331.219),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-4366.6, max=7334398.5, mean=312400.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-1.695, max=2.194, mean=0.653),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-17.953, max=18.033, mean=-2.251),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-17.953, max=18.033, mean=-2.173),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=19.0, max=19.0, mean=19.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-45.731, max=20.143, mean=-8.871),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.007, max=-0.004, mean=-0.006),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=-0.007, max=-0.004, mean=-0.006)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'PFE.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.006, max=0.008, mean=0.001),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-7.428, max=-2.022, mean=-2.548),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.001, max=0.132, mean=0.098),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-2.411, max=3.308, mean=0.595),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-111.953, max=72.144, mean=3.957),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=10.0, max=10.0, mean=10.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-3463.674, max=13160256.0, mean=568116.625),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-3463.674, max=13160256.0, mean=564016.188),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-2.411, max=3.308, mean=0.509),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-77.584, max=79.599, mean=-0.389),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-77.584, max=79.599, mean=0.202),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=21.0, max=21.0, mean=21.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-111.953, max=72.144, mean=3.957),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=-0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'SYK.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.014, max=0.004, mean=-0.005),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-4.48, max=-2.0, mean=-2.55),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.011, max=0.135, mean=0.091),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-1.872, max=2.208, mean=0.567),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-19.047, max=26.385, mean=1.955),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=11.0, max=11.0, mean=11.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-3859.983, max=526560.0, mean=33543.512),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-3859.983, max=526560.0, mean=32500.904),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-1.872, max=2.208, mean=0.485),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-24.451, max=29.752, mean=-0.425),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-24.451, max=29.752, mean=-0.163),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=23.0, max=23.0, mean=23.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-19.038, max=26.394, mean=1.967),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=0.006, max=0.018, mean=0.011),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=0.006, max=0.018, mean=0.011)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'TMO.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.02, max=0.017, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-5.373, max=-2.01, mean=-2.56),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.005, max=0.134, mean=0.092),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-1.956, max=2.581, mean=0.495),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-1.534, max=163.978, mean=31.584),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=12.0, max=12.0, mean=12.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-320.936, max=338528.562, mean=13489.719),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-320.936, max=338528.562, mean=12550.16),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-1.834, max=2.581, mean=0.49),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-45.314, max=93.007, mean=5.5),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-45.314, max=93.007, mean=6.482),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=25.0, max=25.0, mean=25.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-1.534, max=163.979, mean=31.587),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.001, max=0.009, mean=0.004),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=-0.001, max=0.009, mean=0.003)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'UNH.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.01, max=0.008, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-6.776, max=-2.009, mean=-2.632),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.001, max=0.134, mean=0.092),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-1.966, max=3.077, mean=0.729),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-93.637, max=180.843, mean=77.741),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=13.0, max=13.0, mean=13.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-2377.873, max=2697408.0, mean=91147.273),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-2377.873, max=2584128.0, mean=82118.898),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-1.966, max=3.077, mean=0.645),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-91.788, max=271.558, mean=13.246),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-91.788, max=271.558, mean=12.264),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=27.0, max=27.0, mean=27.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-93.642, max=180.835, mean=77.735),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.012, max=0.001, mean=-0.007),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=-0.012, max=0.001, mean=-0.007)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'manager_policy': { 'action_dist_inputs': np.ndarray((32, 26), dtype=float32, min=-0.015, max=0.023, mean=0.001),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'action_logp': np.ndarray((32,), dtype=float32, min=-24.776, max=-13.801, mean=-18.594),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'action_prob': np.ndarray((32,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'actions': np.ndarray((32, 13), dtype=float32, min=-3.005, max=3.226, mean=-0.03),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'advantages': np.ndarray((32,), dtype=float32, min=-1.516, max=5.283, mean=1.491),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'agent_index': np.ndarray((32,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'new_obs': np.ndarray((32, 52), dtype=float32, min=-12.87, max=506126.312, mean=38574.641),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'obs': np.ndarray((32, 52), dtype=float32, min=-12.87, max=480950.531, mean=38548.602),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'prev_actions': np.ndarray((32, 13), dtype=float32, min=-3.005, max=3.226, mean=-0.019),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'prev_rewards': np.ndarray((32,), dtype=float32, min=-0.46, max=2.465, mean=0.298),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'rewards': np.ndarray((32,), dtype=float32, min=-0.46, max=2.465, mean=0.325),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'unroll_id': np.ndarray((32,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'value_targets': np.ndarray((32,), dtype=float32, min=-1.508, max=5.287, mean=1.494),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.013, max=0.012, mean=0.004),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'vf_preds': np.ndarray((32,), dtype=float32, min=-0.013, max=0.012, mean=0.003)}},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m \n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,297\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,297\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,297\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,299\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,299\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,300\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:47,026\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,372\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.MultiActionDistribution` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfMultiDistribution` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,373\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.DiagGaussian` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfDiagGaussian` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,373\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.TFActionDistribution` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfDistribution` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,377\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.Categorical` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.Categorical` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:47,201\tWARNING deprecation.py:50 -- DeprecationWarning: `compute_gradients` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:47,303\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable fc_1/kernel:0\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:47,303\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable fc_1/bias:0\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:47,303\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable fc_2/kernel:0\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:47,303\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable fc_2/bias:0\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:47,303\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable fc_out/kernel:0\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:47,303\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable fc_out/bias:0\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:47,303\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable value_out/kernel:0\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:47,303\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable value_out/bias:0\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m WARNING:tensorflow:5 out of the last 5 calls to <bound method EagerTFPolicyV2._learn_on_batch_helper of A3CTF2Policy_traced> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m WARNING:tensorflow:5 out of the last 5 calls to <bound method EagerTFPolicyV2._learn_on_batch_helper of A3CTF2Policy_traced> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,945\tINFO util.py:118 -- Using connectors:\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,946\tINFO util.py:119 --     AgentConnectorPipeline\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m         ObsPreprocessorConnector\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m         StateBufferConnector\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m         ViewRequirementAgentConnector\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,946\tINFO util.py:120 --     ActionConnectorPipeline\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m         ConvertToNumpyConnector\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m         NormalizeActionsConnector\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m         ImmutableActionsConnector\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,948\tINFO json_writer.py:50 -- You are using JSONWriter. It is recommended to use DatasetWriter instead.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,820\tINFO policy.py:1294 -- Policy (worker=local) running on CPU.\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,946\tINFO rollout_worker.py:1743 -- Built preprocessor map: {'ABT.US': None, 'AMGN.US': None, 'BDX.US': None, 'BMY.US': None, 'HUM.US': None, 'JNJ.US': None, 'LLY.US': None, 'MDT.US': None, 'MRK.US': None, 'PFE.US': None, 'SYK.US': None, 'TMO.US': None, 'UNH.US': None, 'manager_policy': None}\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,948\tINFO rollout_worker.py:550 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,970\tINFO algorithm_config.py:3667 -- Executing eagerly (framework='tf2'), with eager_tracing=True. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m Trainable.setup took 11.566 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5222)\u001b[0m 2023-09-28 08:39:44,190\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5222)\u001b[0m WARNING:tensorflow:6 out of the last 6 calls to <bound method with_lock.<locals>.wrapper of A3CTF2Policy_traced> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:55,213\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m { 'ABT.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 40.000004,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.39264,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': -1907.0433,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 33895.95},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': -1.013279e-05},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'AMGN.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'policy_entropy': 80.576904,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'policy_loss': -3.3338966,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_loss': 1810.0549},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                'vf_explained_var': 0.00011783838},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'BDX.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 39.999996,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.29306,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': 2305.891,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 33707.188},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': -7.390976e-06},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'BMY.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.451,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': -1490.9479,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 20517.621},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': 1.7046928e-05},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'HUM.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.35127,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': 1357.3127,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 14293.232},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': -1.0728836e-06},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'JNJ.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 40.000004,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.731445,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': -486.1594,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 1287.006},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': -3.7789345e-05},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'LLY.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.53737,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': -1111.7954,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 8022.0},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': 3.8087368e-05},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'MDT.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.367096,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': -1903.5897,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 11451.156},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': -2.6106834e-05},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'MRK.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.5073,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': -684.8697,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 5459.323},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': -3.695488e-06},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'PFE.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.69364,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': 235.2103,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 21715.215},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': 7.1525574e-07},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'SYK.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.34216,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': 150.17735,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 1707.3002},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': -2.0623207e-05},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'TMO.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 39.999996,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.63844,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': 2360.1567,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 50625.99},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': -1.66893e-05},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'UNH.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.48277,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': 6724.289,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 157513.16},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': -5.722046e-06},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'manager_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                          'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                          'policy_entropy': 590.8353,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                          'policy_loss': 891.9219,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                          'var_gnorm': 22.627476,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                          'vf_loss': 96.15842},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'vf_explained_var': -0.001449585}}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5222)\u001b[0m 2023-09-28 08:39:46,924\tINFO json_writer.py:107 -- Writing to new output file <_io.TextIOWrapper name='/Volumes/SSD980/ray/results/tunerun2/A2C/A2C_hrl_c3ec2_00000_0_entropy_coeff=0.0982,gamma=0.9788,lambda=0.9898,lr=0.0001,vf_loss_coeff=0.1134_2023-09-28_08-39-25/output-2023-09-28_08-39-46_worker-1_0.json' mode='w' encoding='UTF-8'>\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5222)\u001b[0m 2023-09-28 08:39:46,925\tDEBUG json_writer.py:81 -- Wrote 110942 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/results/tunerun2/A2C/A2C_hrl_c3ec2_00000_0_entropy_coeff=0.0982,gamma=0.9788,lambda=0.9898,lr=0.0001,vf_loss_coeff=0.1134_2023-09-28_08-39-25/output-2023-09-28_08-39-46_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.007759809494018555s\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:47,037\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:47,045\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m \u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m { 'count': 32,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'policy_batches': { 'ABT.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.016, max=0.007, mean=-0.001),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'action_logp': np.ndarray((32,), dtype=float32, min=-24.319, max=-14.64, mean=-17.818),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'action_prob': np.ndarray((32,), dtype=float32, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'actions': np.ndarray((32, 13), dtype=float32, min=-3.404, max=2.721, mean=-0.082),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'advantages': np.ndarray((32,), dtype=float32, min=-2.656, max=0.775, mean=-0.607),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'agent_index': np.ndarray((32,), dtype=int64, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'eps_id': np.ndarray((32,), dtype=int64, min=1.3434254895257957e+17, max=6.912236311907135e+17, mean=3.3961916041261274e+17),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'new_obs': np.ndarray((32, 52), dtype=float32, min=-15.872, max=517745.906, mean=38344.918),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'obs': np.ndarray((32, 52), dtype=float32, min=-15.872, max=517745.906, mean=38354.082),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'prev_actions': np.ndarray((32, 13), dtype=float32, min=-3.404, max=2.721, mean=-0.079),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'prev_rewards': np.ndarray((32,), dtype=float32, min=-0.641, max=1.321, mean=-0.085),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'rewards': np.ndarray((32,), dtype=float32, min=-0.641, max=1.321, mean=-0.092),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'unroll_id': np.ndarray((32,), dtype=int64, min=1.0, max=1.0, mean=1.0),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'value_targets': np.ndarray((32,), dtype=float32, min=-2.654, max=0.773, mean=-0.606),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.009, max=0.011, mean=0.001),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'vf_preds': np.ndarray((32,), dtype=float32, min=-0.009, max=0.011, mean=0.001)}},\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'AMGN.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.009, max=0.004, mean=-0.002),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'BDX.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.012, max=0.005, mean=-0.002),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'BMY.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.007, max=0.013, mean=0.002),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'HUM.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.015, max=0.014, mean=-0.001),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'JNJ.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.004, max=0.005, mean=-0.001),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'LLY.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.007, max=0.009, mean=0.001),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'MDT.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.013, max=0.006, mean=-0.002),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'MRK.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.007, max=0.02, mean=0.002),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'PFE.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.001, max=0.006, mean=0.002),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'SYK.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.011, max=0.018, mean=0.004),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'TMO.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.018, max=0.013, mean=0.001),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'UNH.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.005, max=0.016, mean=0.005),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'manager_policy': { 'action_dist_inputs': np.ndarray((32, 26), dtype=float32, min=-0.022, max=0.017, mean=-0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'type': 'MultiAgentBatch'}\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5222)\u001b[0m 2023-09-28 08:39:46,934\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:47,274\tWARNING deprecation.py:50 -- DeprecationWarning: `compute_gradients` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:47,378\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable value_out/bias:0\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics  </th><th>counters                                                                                                                      </th><th>custom_metrics  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_sampled_throughput_per_sec</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained_throughput_per_sec</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                          </th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf  </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                </th><th>timers                                                                                                                                                   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>A2C_hrl_c3ec2_00000</td><td style=\"text-align: right;\">                   8064</td><td>{}                 </td><td>{&#x27;num_env_steps_sampled&#x27;: 576, &#x27;num_env_steps_trained&#x27;: 576, &#x27;num_agent_steps_sampled&#x27;: 8064, &#x27;num_agent_steps_trained&#x27;: 8064}</td><td>{}              </td><td style=\"text-align: right;\">               nan</td><td>{}             </td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                  nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                   0</td><td>{&#x27;learner&#x27;: {&#x27;manager_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: 901.1411, &#x27;policy_entropy&#x27;: 601.7747, &#x27;var_gnorm&#x27;: 22.628664, &#x27;vf_loss&#x27;: 50.134377}, &#x27;grad_gnorm&#x27;: 39.999992, &#x27;vf_explained_var&#x27;: 0.009977937, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;ABT.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: -1006.28406, &#x27;policy_entropy&#x27;: 80.95214, &#x27;var_gnorm&#x27;: 22.626245, &#x27;vf_loss&#x27;: 12896.663}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -3.671646e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;AMGN.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: -3292.4568, &#x27;policy_entropy&#x27;: 79.18648, &#x27;var_gnorm&#x27;: 22.626692, &#x27;vf_loss&#x27;: 145427.92}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -6.556511e-06, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;BDX.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: 664.559, &#x27;policy_entropy&#x27;: 76.49822, &#x27;var_gnorm&#x27;: 22.629839, &#x27;vf_loss&#x27;: 4443.395}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 3.516674e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;BMY.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: -648.1537, &#x27;policy_entropy&#x27;: 82.30185, &#x27;var_gnorm&#x27;: 22.626278, &#x27;vf_loss&#x27;: 25455.188}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -1.04904175e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;HUM.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: -915.4024, &#x27;policy_entropy&#x27;: 83.10939, &#x27;var_gnorm&#x27;: 22.627779, &#x27;vf_loss&#x27;: 12785.971}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -8.237362e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;JNJ.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: -187.85243, &#x27;policy_entropy&#x27;: 78.22313, &#x27;var_gnorm&#x27;: 22.6267, &#x27;vf_loss&#x27;: 8748.242}, &#x27;grad_gnorm&#x27;: 39.999996, &#x27;vf_explained_var&#x27;: 1.1920929e-07, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;LLY.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: 325.8291, &#x27;policy_entropy&#x27;: 81.954185, &#x27;var_gnorm&#x27;: 22.628183, &#x27;vf_loss&#x27;: 4307.9087}, &#x27;grad_gnorm&#x27;: 40.000004, &#x27;vf_explained_var&#x27;: -4.017353e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;MDT.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: -876.94666, &#x27;policy_entropy&#x27;: 79.1238, &#x27;var_gnorm&#x27;: 22.628193, &#x27;vf_loss&#x27;: 8408.438}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 3.8444996e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;MRK.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: 2009.5212, &#x27;policy_entropy&#x27;: 79.18473, &#x27;var_gnorm&#x27;: 22.62747, &#x27;vf_loss&#x27;: 70097.32}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 8.523464e-06, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;PFE.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: -778.70966, &#x27;policy_entropy&#x27;: 79.73829, &#x27;var_gnorm&#x27;: 22.626127, &#x27;vf_loss&#x27;: 4801.4683}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 4.118681e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;SYK.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: -1364.7584, &#x27;policy_entropy&#x27;: 79.540276, &#x27;var_gnorm&#x27;: 22.62874, &#x27;vf_loss&#x27;: 41140.688}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 0.00020438433, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;TMO.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: -1614.6804, &#x27;policy_entropy&#x27;: 79.96266, &#x27;var_gnorm&#x27;: 22.626652, &#x27;vf_loss&#x27;: 69313.375}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -2.6464462e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;UNH.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: -1679.7865, &#x27;policy_entropy&#x27;: 82.97427, &#x27;var_gnorm&#x27;: 22.626808, &#x27;vf_loss&#x27;: 48220.156}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 2.7000904e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}}, &#x27;num_env_steps_sampled&#x27;: 576, &#x27;num_env_steps_trained&#x27;: 576, &#x27;num_agent_steps_sampled&#x27;: 8064, &#x27;num_agent_steps_trained&#x27;: 8064}                                        </td><td style=\"text-align: right;\">                     8064</td><td style=\"text-align: right;\">                     8064</td><td style=\"text-align: right;\">                    576</td><td style=\"text-align: right;\">                              544</td><td style=\"text-align: right;\">                                   53.4054</td><td style=\"text-align: right;\">                    576</td><td style=\"text-align: right;\">                              544</td><td style=\"text-align: right;\">                                   53.4054</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    3</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          544</td><td>{&#x27;cpu_util_percent&#x27;: 83.28571428571426, &#x27;ram_util_percent&#x27;: 84.42857142857143}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{}            </td><td>{&#x27;episode_reward_max&#x27;: nan, &#x27;episode_reward_min&#x27;: nan, &#x27;episode_reward_mean&#x27;: nan, &#x27;episode_len_mean&#x27;: nan, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 0, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [], &#x27;episode_lengths&#x27;: []}, &#x27;sampler_perf&#x27;: {}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 648.727, &#x27;sample_time_ms&#x27;: 570.327, &#x27;learn_time_ms&#x27;: 60.712, &#x27;learn_throughput&#x27;: 527.075, &#x27;synch_weights_time_ms&#x27;: 17.043}</td></tr>\n",
       "<tr><td>A2C_hrl_c3ec2_00001</td><td style=\"text-align: right;\">                   8064</td><td>{}                 </td><td>{&#x27;num_env_steps_sampled&#x27;: 576, &#x27;num_env_steps_trained&#x27;: 576, &#x27;num_agent_steps_sampled&#x27;: 8064, &#x27;num_agent_steps_trained&#x27;: 8064}</td><td>{}              </td><td style=\"text-align: right;\">               nan</td><td>{}             </td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                  nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                   0</td><td>{&#x27;learner&#x27;: {&#x27;manager_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: 556.09717, &#x27;policy_entropy&#x27;: 590.591, &#x27;var_gnorm&#x27;: 22.627518, &#x27;vf_loss&#x27;: 20.31974}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -0.0065151453, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;ABT.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: -865.8674, &#x27;policy_entropy&#x27;: 80.621956, &#x27;var_gnorm&#x27;: 22.627419, &#x27;vf_loss&#x27;: 11959.842}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 2.7775764e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;AMGN.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: -137.4093, &#x27;policy_entropy&#x27;: 80.20775, &#x27;var_gnorm&#x27;: 22.627441, &#x27;vf_loss&#x27;: 13420.486}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -1.5497208e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;BDX.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: -1318.0247, &#x27;policy_entropy&#x27;: 80.59806, &#x27;var_gnorm&#x27;: 22.627346, &#x27;vf_loss&#x27;: 96704.58}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -1.4305115e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;BMY.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: 1918.8545, &#x27;policy_entropy&#x27;: 80.74762, &#x27;var_gnorm&#x27;: 22.627441, &#x27;vf_loss&#x27;: 38827.23}, &#x27;grad_gnorm&#x27;: 40.000004, &#x27;vf_explained_var&#x27;: -5.9604645e-06, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;HUM.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: -385.5206, &#x27;policy_entropy&#x27;: 80.43183, &#x27;var_gnorm&#x27;: 22.627441, &#x27;vf_loss&#x27;: 15117.015}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 3.6120415e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;JNJ.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: -2905.2727, &#x27;policy_entropy&#x27;: 80.2654, &#x27;var_gnorm&#x27;: 22.62743, &#x27;vf_loss&#x27;: 87446.47}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -2.026558e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;LLY.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: -395.91446, &#x27;policy_entropy&#x27;: 80.69043, &#x27;var_gnorm&#x27;: 22.627455, &#x27;vf_loss&#x27;: 10427.22}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -2.7537346e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;MDT.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: 1021.4447, &#x27;policy_entropy&#x27;: 80.41681, &#x27;var_gnorm&#x27;: 22.627434, &#x27;vf_loss&#x27;: 16002.359}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -2.5987625e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;MRK.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: 125.383644, &#x27;policy_entropy&#x27;: 80.13869, &#x27;var_gnorm&#x27;: 22.627495, &#x27;vf_loss&#x27;: 417.1977}, &#x27;grad_gnorm&#x27;: 40.000008, &#x27;vf_explained_var&#x27;: -5.7935715e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;PFE.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: -1232.9484, &#x27;policy_entropy&#x27;: 80.664474, &#x27;var_gnorm&#x27;: 22.627474, &#x27;vf_loss&#x27;: 17973.56}, &#x27;grad_gnorm&#x27;: 40.000004, &#x27;vf_explained_var&#x27;: 3.2782555e-06, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;SYK.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: 2686.023, &#x27;policy_entropy&#x27;: 80.46045, &#x27;var_gnorm&#x27;: 22.627417, &#x27;vf_loss&#x27;: 207924.72}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 1.7046928e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;TMO.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: -3117.0676, &#x27;policy_entropy&#x27;: 80.712105, &#x27;var_gnorm&#x27;: 22.627441, &#x27;vf_loss&#x27;: 112642.0}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 1.8775463e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;UNH.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: -3112.5872, &#x27;policy_entropy&#x27;: 80.46647, &#x27;var_gnorm&#x27;: 22.627409, &#x27;vf_loss&#x27;: 77825.05}, &#x27;grad_gnorm&#x27;: 39.999996, &#x27;vf_explained_var&#x27;: 3.0696392e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}}, &#x27;num_env_steps_sampled&#x27;: 576, &#x27;num_env_steps_trained&#x27;: 576, &#x27;num_agent_steps_sampled&#x27;: 8064, &#x27;num_agent_steps_trained&#x27;: 8064}</td><td style=\"text-align: right;\">                     8064</td><td style=\"text-align: right;\">                     8064</td><td style=\"text-align: right;\">                    576</td><td style=\"text-align: right;\">                              544</td><td style=\"text-align: right;\">                                   52.2368</td><td style=\"text-align: right;\">                    576</td><td style=\"text-align: right;\">                              544</td><td style=\"text-align: right;\">                                   52.2368</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    3</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          544</td><td>{&#x27;cpu_util_percent&#x27;: 76.5, &#x27;ram_util_percent&#x27;: 84.32666666666665}             </td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{}            </td><td>{&#x27;episode_reward_max&#x27;: nan, &#x27;episode_reward_min&#x27;: nan, &#x27;episode_reward_mean&#x27;: nan, &#x27;episode_len_mean&#x27;: nan, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 0, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [], &#x27;episode_lengths&#x27;: []}, &#x27;sampler_perf&#x27;: {}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 661.05, &#x27;sample_time_ms&#x27;: 567.554, &#x27;learn_time_ms&#x27;: 75.07, &#x27;learn_throughput&#x27;: 426.27, &#x27;synch_weights_time_ms&#x27;: 17.809}   </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m WARNING:tensorflow:6 out of the last 6 calls to <bound method EagerTFPolicyV2._learn_on_batch_helper of A3CTF2Policy_traced> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:55,220\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m { 'ABT.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 0,\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'grad_gnorm': 39.999996,\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'learner_stats': { 'cur_lr': 1.0548947102506645e-05,\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                          'entropy_coeff': 0.025421002879738808,\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                          'policy_entropy': 590.42316,\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                          'policy_loss': -340.913,\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                          'var_gnorm': 22.627476,\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                          'vf_loss': 20.893913},\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'num_agent_steps_trained': 32,\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'num_grad_updates_lifetime': 1,\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'vf_explained_var': -9.775162e-05}}\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'AMGN.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'BDX.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'BMY.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'HUM.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'JNJ.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'LLY.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'MDT.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'MRK.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'PFE.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'SYK.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'TMO.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'UNH.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'manager_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:40:00,300\tDEBUG json_writer.py:81 -- Wrote 115558 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/results/tunerun2/A2C/A2C_hrl_c3ec2_00000_0_entropy_coeff=0.0982,gamma=0.9788,lambda=0.9898,lr=0.0001,vf_loss_coeff=0.1134_2023-09-28_08-39-25/output-2023-09-28_08-39-46_worker-2_0.json' mode='w' encoding='UTF-8'> in 0.003008127212524414s\u001b[32m [repeated 49x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5227)\u001b[0m 2023-09-28 08:40:05,474\tDEBUG json_writer.py:81 -- Wrote 108353 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/results/tunerun2/A2C/A2C_hrl_c3ec2_00001_1_entropy_coeff=0.0254,gamma=0.9723,lambda=0.9582,lr=0.0000,vf_loss_coeff=0.1742_2023-09-28_08-39-25/output-2023-09-28_08-39-46_worker-3_0.json' mode='w' encoding='UTF-8'> in 0.0027458667755126953s\u001b[32m [repeated 50x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Mit vielen policies\n",
    "import sys\n",
    "# Redirect the standard error to the log file\n",
    "\n",
    "# Create a log file\n",
    "log_file = open(\"console_logs.txt\", \"w\")\n",
    "\n",
    "\n",
    "# Redirect the standard output to the log file\n",
    "sys.stdout = log_file\n",
    "\n",
    "\n",
    "env = HRL()\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  \n",
    "\n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "def create_policy_spec(worker_id):\n",
    "    # print(f\"Creating policy for {worker_id} with obs space {env.observation_space[worker_id]} and action space {env.action_space[worker_id]}\")\n",
    "    return PolicySpec(\n",
    "        observation_space=env.observation_space[worker_id],\n",
    "        action_space=env.action_space[worker_id],\n",
    "        config={}\n",
    "    )\n",
    "\n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    "\n",
    "policies = {\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    "\n",
    "for worker_id in env.workers:\n",
    "    policies[worker_id] = create_policy_spec(worker_id)\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id == 'manager':\n",
    "        # print(f\"!!!!!! policy mapping manager: {agent_id}\")\n",
    "        return \"manager_policy\"\n",
    "    elif agent_id in env.workers:\n",
    "        return agent_id\n",
    "    else:\n",
    "        print(\"defaul policy triggered\")\n",
    "        return \"default_policy\"\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "    \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "        \"rollout_fragment_length\": \"auto\",\n",
    "        \"framework\": \"tf2\",\n",
    "        \"lr\": tune.uniform(1e-5,1e-4),\n",
    "        \"gamma\": tune.uniform(0.95, 0.9999),\n",
    "        \"lambda\": tune.uniform(0.9,1.0),\n",
    "        \"entropy_coeff\": tune.uniform(0.01,0.1),\n",
    "        \"vf_loss_coeff\": tune.uniform(0.1,0.3),\n",
    "        \"num_workers\": 3, \n",
    "        #Change for Debugging\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"output\": \"logdir\",\n",
    "        \"monitor\": True,\n",
    "}\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"A2C\",\n",
    "    metric=\"episode_reward_mean\", \n",
    "    num_samples=5,\n",
    "    mode=\"max\",\n",
    "    config=param_space, \n",
    "    local_dir=\"/Volumes/SSD980/ray/results/tunerun2\",\n",
    "    search_alg=None,\n",
    "    scheduler=None,\n",
    "    progress_reporter=CLIReporter(max_progress_rows=5,max_report_frequency=120),\n",
    "    max_concurrent_trials=2,\n",
    "    #checkpoint_config not checked yet\n",
    "    checkpoint_config={\n",
    "        \"num_to_keep\": 1,\n",
    "        \"checkpoint_score_attribute\": \"episode_reward_mean\",\n",
    "        \"checkpoint_score_order\": \"max\",\n",
    "        \"checkpoint_frequency\": 10\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HRL()\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  \n",
    "\n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    "    \n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    " \n",
    "initial_params = [{\n",
    "    \"lr\": 0.001,\n",
    "    \"gamma\": 0.92,\n",
    "    \"lambda\": 0.95,\n",
    "    \"entropy_coeff\": 1e-3,\n",
    "    \"vf_loss_coeff\": 0.5,\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": 64,\n",
    "        \"fcnet_activation\":\"relu\",\n",
    "    },\n",
    "}]\n",
    "\n",
    "search_space = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"gamma\": tune.uniform(0.9, 0.99),\n",
    "    \"lambda\": tune.uniform(0.9, 1.0),\n",
    "    \"entropy_coeff\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"vf_loss_coeff\": tune.uniform(0.1, 1.0),\n",
    "    # \"model\": {\n",
    "    #     \"fcnet_hiddens\": tune.grid_search([[64, 64], [128, 128], [256, 256]]),\n",
    "    #     \"fcnet_activation\": tune.choice([\"relu\", \"tanh\"]),\n",
    "    # },\n",
    "}\n",
    "\n",
    "\n",
    "algo = HyperOptSearch(space=search_space,metric=\"episode_reward_mean\", mode=\"max\",)\n",
    "\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "\n",
    "        \"num_workers\": 1,  \n",
    "        \"num_cpus_per_trial\": 1,\n",
    "         \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"gamma\": tune.uniform(0.9, 0.99),\n",
    "    \"lambda\": tune.uniform(0.9, 1.0),\n",
    "    \"entropy_coeff\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"vf_loss_coeff\": tune.uniform(0.1, 1.0),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "          \"rollout_fragment_length\": \"auto\",\n",
    "          \"lr\": tune.uniform(1e-5,1e-4),\n",
    "          \"lambda\": tune.uniform(0.95, 1.0),\n",
    "          \"vf_loss_coeff\": tune.uniform(0.1, 1.0),\n",
    "          \"entropy_coeff\": tune.uniform(1e-4, 1e-1),\n",
    "        \"num_workers\": 1,  \n",
    "        \"num_envs_per_worker\": 1\n",
    "}\n",
    "\n",
    "analysis = tune.run(\"A2C\", \n",
    "                    metric=\"episode_reward_mean\", \n",
    "                    mode=\"max\",\n",
    "                    config=param_space,\n",
    "                    num_samples=10,\n",
    "                    stop={\"training_iteration\": 100},\n",
    "                    progress_reporter=CLIReporter(max_progress_rows=10,max_report_frequency=600),\n",
    "                    # local_dir=\"/Users/floriankockler/rayresults/overnight1\",\n",
    "                    storage_path=\"/Users/floriankockler/Documents/GitHub.nosync/raystorage\",\n",
    "                    checkpoint_config=CheckpointConfig(\n",
    "                        num_to_keep=2,\n",
    "                        checkpoint_score_attribute=\"episode_reward_mean\", \n",
    "                        checkpoint_score_order=\"max\"\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "\n",
    "best = analysis.best_trial\n",
    "print(pretty_print(best.last_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = HRL()\n",
    "\n",
    "n_iterations = 1\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    obs, reward, done, truncated, info= env.step(action)\n",
    "\n",
    "    # print(f\"Action: {action}, Reward: {reward}, Portfolio Value: {obs[0] + obs[1] * obs[2]}\")\n",
    "    \n",
    "    if done[\"__all__\"]:\n",
    "        print(\"Episode finished!\")\n",
    "        state = env.reset()\n",
    "    else:\n",
    "        state = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    "    \n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    "\n",
    "    \n",
    "algo = PPOConfig().environment(env=HRL).multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn = policy_mapping_fn,  \n",
    "    ).training(train_batch_size=4000).build()\n",
    "\n",
    "print(algo.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HRL()\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  \n",
    "\n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    "    \n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    " \n",
    "search_space = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),  # Learning rate\n",
    "    \"gamma\": tune.uniform(0.9, 0.99),  # Discount factor\n",
    "}\n",
    "\n",
    "tune_config = TuneConfig(\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    max_concurrent_trials=1,\n",
    "    num_samples=100,\n",
    "    search_alg=BayesOptSearch(\n",
    "          search_space,\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        random_search_steps=4\n",
    "    ),\n",
    "    scheduler=HyperBandScheduler()\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    name=\"MyExperiment\",\n",
    "    storage_path=\"/Users/floriankockler/rayresults/tuner_trial\",\n",
    "    verbose=2,\n",
    "    # checkpoint_config=air.CheckpointConfig(checkpoint_frequency=2),\n",
    "    callbacks=None,\n",
    "    stop=None,\n",
    "    failure_config=None,\n",
    "    sync_config=None,\n",
    "    checkpoint_config=None,\n",
    "    progress_reporter=CLIReporter(max_progress_rows=10),# Define the policy mapping function\n",
    "\n",
    ")\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "          \"rollout_fragment_length\": \"auto\",\n",
    "          \"lr\": tune.uniform(1e-5,1e-4),\n",
    "        \"num_workers\": 1,  \n",
    "        \"num_cpus_per_trial\": 1,\n",
    "}\n",
    "\n",
    "analysis = tune.run(\"A2C\", metric=\"episode_reward_mean\", mode=\"max\",config=param_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "algo = PPOConfig().environment(env=HRL).multi_agent(\n",
    "    policies={\n",
    "        \"policy_1\": ()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune import CLIReporter\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  # Assuming this is your environment\n",
    " \n",
    "env = HRL()\n",
    "\n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    "    \n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    " \n",
    "search_space = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),  # Learning rate\n",
    "    \"gamma\": tune.uniform(0.9, 0.99),  # Discount factor\n",
    "}\n",
    "\n",
    "tune_config = TuneConfig(\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    max_concurrent_trials=1,\n",
    "    num_samples=100,\n",
    "    search_alg=BayesOptSearch(\n",
    "          search_space,\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        random_search_steps=4\n",
    "    ),\n",
    "    scheduler=HyperBandScheduler()\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    name=\"MyExperiment\",\n",
    "    storage_path=\"/Users/floriankockler/rayresults/tuner_trial\",\n",
    "    verbose=2,\n",
    "    # checkpoint_config=air.CheckpointConfig(checkpoint_frequency=2),\n",
    "    callbacks=None,\n",
    "    stop=None,\n",
    "    failure_config=None,\n",
    "    sync_config=None,\n",
    "    checkpoint_config=None,\n",
    "    progress_reporter=CLIReporter(max_progress_rows=10),# Define the policy mapping function\n",
    "\n",
    ")\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "          \"rollout_fragment_length\": \"auto\",\n",
    "        \"num_workers\": 1,  \n",
    "        \"num_cpus_per_trial\": 1,\n",
    "}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"A2C\",\n",
    "    tune_config=tune_config,\n",
    "    param_space=param_space,\n",
    "\n",
    "    run_config=run_config,\n",
    ")\n",
    "results = tuner.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from ray import air, tune\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "import argparse\n",
    "from ray.tune import CLIReporter\n",
    "from env.multi_agent.hrl import HRL\n",
    "\n",
    "reporter = CLIReporter(max_progress_rows=10)\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  # Assuming this is your environment\n",
    " \n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "manager_config = {\n",
    "    \"df\": train_df,\n",
    "\n",
    "}\n",
    "hrl_config={\n",
    "        \"manager_config\": manager_config\n",
    "        }\n",
    "env = HRL(hrl_config)\n",
    " \n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    " \n",
    "\n",
    "\n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\"\n",
    ")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    "\n",
    "\n",
    "def explore(config):\n",
    "    # Ensure we collect enough timesteps to do sgd\n",
    "    if config[\"train_batch_size\"] < config[\"rollout_fragment_length\"] * 2:\n",
    "        config[\"train_batch_size\"] = config[\"rollout_fragment_length\"] * 2\n",
    "    return config\n",
    "\n",
    "hyperparam_mutations = {\n",
    "    \"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "    \"gamma\": lambda: random.uniform(0.9, 1.0),\n",
    "    \"entropy_coeff\": [0.01, 0.1, 1.0],\n",
    "    \"num_envs_per_worker\": [1, 2, 4, 8],\n",
    "    #\"rollout_fragment_length\": [50, 100, 200, 400],\n",
    "    \"train_batch_size\": lambda: random.randint(200, 1500),\n",
    "    \"sgd_minibatch_size\": tune.choice([50, 100, 200]),\n",
    "\n",
    "}\n",
    "\n",
    "pbt = PopulationBasedTraining(\n",
    "        time_attr=\"time_total_s\",\n",
    "        perturbation_interval=120,\n",
    "        resample_probability=0.25,\n",
    "        # Specifies the mutations of these hyperparams\n",
    "        hyperparam_mutations=hyperparam_mutations,\n",
    "        custom_explore_fn=explore,\n",
    "    )\n",
    "\n",
    "# Stop when we've reached 100 training iterations or reward=300\n",
    "stopping_criteria = {\"training_iteration\": 100}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"A2C\",\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        scheduler=pbt,\n",
    "        num_samples=1 if args.smoke_test else 10,\n",
    "    ),\n",
    "    param_space={\n",
    "        \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "        \"env_config\": hrl_config,\n",
    "        \"rollout_fragment_length\": \"auto\",\n",
    "        \"framework\": \"tf2\",\n",
    "        \"num_workers\": 1,  # 1 for training + 4 for sampling\n",
    "        \"num_cpus_per_trial\": 3,\n",
    "        # \"num_cpus\": 1,  # number of CPUs to use per trial --> 6 in total = max available\n",
    "        # \"num_gpus\": 0,  # number of GPUs to use per trial\n",
    "        # These params are tuned from a fixed starting value.\n",
    "        \"lr\": 1e-4,\n",
    "        # These params start off randomly drawn from a set.\n",
    "        \"sgd_minibatch_size\": tune.choice([50, 100, 200]),\n",
    "        \"train_batch_size\": tune.choice([200, 400, 600]),\n",
    "    },\n",
    "\n",
    "    run_config=air.RunConfig(stop=stopping_criteria, local_dir=\"/Users/floriankockler/rayresults/autobatch\", progress_reporter=reporter),\n",
    ")\n",
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
