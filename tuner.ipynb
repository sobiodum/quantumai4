{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-28 08:39:11,889\tINFO worker.py:1612 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-09-28 08:39:21,224\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import ray, random, os \n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.air.config import ScalingConfig\n",
    "import pandas as pd\n",
    "ray.init(_temp_dir='/Volumes/SSD980/ray')\n",
    "from ray import air, tune\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "import argparse\n",
    "from ray.tune import CLIReporter\n",
    "from hrl import HRL\n",
    "from ray.train.rl import RLTrainer\n",
    "from ray.rllib.policy.policy import Policy, PolicySpec\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune import TuneConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "from ray.air.config import RunConfig, CheckpointConfig\n",
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.search.hyperopt import HyperOptSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-28 08:39:25,055\tINFO tune.py:657 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/tune/tune.py:258: UserWarning: Passing a `local_dir` is deprecated and will be removed in the future. Pass `storage_path` instead or set the `RAY_AIR_LOCAL_CACHE_DIR` environment variable instead.\n",
      "  warnings.warn(\n",
      "2023-09-28 08:39:25,087\tWARNING deprecation.py:50 -- DeprecationWarning: `build_tf_policy` has been deprecated. This will raise an error in the future!\n",
      "2023-09-28 08:39:25,090\tWARNING deprecation.py:50 -- DeprecationWarning: `build_policy_class` has been deprecated. This will raise an error in the future!\n",
      "2023-09-28 08:39:25,120\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "2023-09-28 08:39:25,167\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(pid=5198)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:32,404\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a2c/` has been deprecated. Use `rllib_contrib/a2c/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:32,404\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a3c/` has been deprecated. Use `rllib_contrib/a3c/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:32,405\tWARNING algorithm_config.py:656 -- Cannot create A2CConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=5227)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:32,404\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a3c/` has been deprecated. Use `rllib_contrib/a3c/` instead. This will raise an error in the future!\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:32,405\tWARNING algorithm_config.py:656 -- Cannot create A2CConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,681\tDEBUG rollout_worker.py:1761 -- Creating policy for ABT.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,683\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(0.0, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,688\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,688\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,688\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,688\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,688\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,688\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,689\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (14,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,689\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,689\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,690\tDEBUG catalog.py:793 -- Created preprocessor <ray.rllib.models.preprocessors.DictFlatteningPreprocessor object at 0x172c88850>: Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)) -> (23,)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,697\tDEBUG rollout_worker.py:1761 -- Creating policy for AMGN.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,702\tDEBUG rollout_worker.py:1761 -- Creating policy for BDX.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,708\tDEBUG rollout_worker.py:1761 -- Creating policy for BMY.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,710\tDEBUG rollout_worker.py:1761 -- Creating policy for HUM.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,712\tDEBUG rollout_worker.py:1761 -- Creating policy for JNJ.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,713\tDEBUG rollout_worker.py:1761 -- Creating policy for LLY.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,714\tDEBUG rollout_worker.py:1761 -- Creating policy for MDT.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,715\tDEBUG rollout_worker.py:1761 -- Creating policy for MRK.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,716\tDEBUG rollout_worker.py:1761 -- Creating policy for PFE.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,719\tDEBUG rollout_worker.py:1761 -- Creating policy for SYK.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,722\tDEBUG rollout_worker.py:1761 -- Creating policy for TMO.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,723\tDEBUG rollout_worker.py:1761 -- Creating policy for UNH.US\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,725\tDEBUG rollout_worker.py:1761 -- Creating policy for manager_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,725\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,725\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,726\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,726\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,727\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,727\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,728\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,728\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,729\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,729\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,729\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,730\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,730\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,731\tDEBUG catalog.py:793 -- Created preprocessor <ray.rllib.models.preprocessors.DictFlatteningPreprocessor object at 0x173e25e70>: Dict('ABT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'AMGN.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BDX.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BMY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'HUM.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'JNJ.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'LLY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MDT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MRK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'PFE.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'SYK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'TMO.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'UNH.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))) -> (52,)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,756\tINFO eager_tf_policy_v2.py:80 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5225)\u001b[0m 2023-09-28 08:39:41,827\tINFO policy.py:1294 -- Policy (worker=2) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,865\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,865\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_modelv2.TFModelV2` has been deprecated. Use `ray.rllib.core.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,878\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.misc.normc_initializer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,971\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,971\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,971\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,975\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,975\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:41,978\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:42,065\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.MultiActionDistribution` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfMultiDistribution` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:42,067\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.DiagGaussian` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfDiagGaussian` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:42,070\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.TFActionDistribution` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfDistribution` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:42,097\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.Categorical` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.Categorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,978\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,978\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,978\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,980\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,980\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,980\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,981\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,981\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,981\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,983\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,983\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,983\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,984\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,984\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,984\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,986\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,986\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,986\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,987\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,987\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,987\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,989\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,989\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,989\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,990\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,990\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,990\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,991\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,991\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,991\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,993\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,993\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,993\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,994\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,994\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:42,994\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5225)\u001b[0m 2023-09-28 08:39:42,933\tINFO json_writer.py:50 -- You are using JSONWriter. It is recommended to use DatasetWriter instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5225)\u001b[0m 2023-09-28 08:39:42,933\tDEBUG rollout_worker.py:645 -- Created rollout worker with env <ray.rllib.env.multi_agent_env.MultiAgentEnvWrapper object at 0x2e785ffa0> (<HRL instance>), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['MDT.US', 'SYK.US', 'AMGN.US', 'JNJ.US', 'TMO.US', 'BDX.US', 'HUM.US', 'UNH.US', 'LLY.US', 'ABT.US', 'manager_policy', 'PFE.US', 'MRK.US', 'BMY.US']>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:42,955\tDEBUG rollout_worker.py:645 -- Created rollout worker with env <ray.rllib.env.multi_agent_env.MultiAgentEnvWrapper object at 0x172c88370> (<HRL instance>), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['SYK.US', 'ABT.US', 'UNH.US', 'TMO.US', 'manager_policy', 'BMY.US', 'JNJ.US', 'PFE.US', 'LLY.US', 'HUM.US', 'MRK.US', 'BDX.US', 'AMGN.US', 'MDT.US']>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5226)\u001b[0m 2023-09-28 08:39:42,968\tDEBUG rollout_worker.py:645 -- Created rollout worker with env <ray.rllib.env.multi_agent_env.MultiAgentEnvWrapper object at 0x17d778490> (<HRL instance>), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['LLY.US', 'BDX.US', 'PFE.US', 'BMY.US', 'TMO.US', 'ABT.US', 'HUM.US', 'manager_policy', 'SYK.US', 'AMGN.US', 'JNJ.US', 'UNH.US', 'MRK.US', 'MDT.US']>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:39:43,008\tDEBUG rollout_worker.py:645 -- Created rollout worker with env <ray.rllib.env.multi_agent_env.MultiAgentEnvWrapper object at 0x17fd27fa0> (<HRL instance>), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['ABT.US', 'BMY.US', 'MRK.US', 'BDX.US', 'HUM.US', 'UNH.US', 'manager_policy', 'PFE.US', 'SYK.US', 'JNJ.US', 'LLY.US', 'AMGN.US', 'TMO.US', 'MDT.US']>\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,080\tINFO worker_set.py:297 -- Inferred observation/action spaces from remote worker (local worker has no env): {'SYK.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'ABT.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'UNH.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'TMO.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'manager_policy': (Dict('ABT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'AMGN.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BDX.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BMY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'HUM.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'JNJ.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'LLY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MDT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MRK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'PFE.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'SYK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'TMO.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'UNH.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))), Dict('ABT.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'AMGN.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'BDX.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'BMY.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'HUM.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'JNJ.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'LLY.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'MDT.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'MRK.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'PFE.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'SYK.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'TMO.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'UNH.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)))), 'BMY.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'JNJ.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'PFE.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'LLY.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'HUM.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'MRK.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'BDX.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'AMGN.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'MDT.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), '__env__': (Dict('ABT.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'AMGN.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'BDX.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'BMY.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'HUM.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'JNJ.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'LLY.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'MDT.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'MRK.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'PFE.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'SYK.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'TMO.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'UNH.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'manager': Dict('ABT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'AMGN.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BDX.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BMY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'HUM.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'JNJ.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'LLY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MDT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MRK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'PFE.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'SYK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'TMO.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'UNH.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)))), Dict('ABT.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'AMGN.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'BDX.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'BMY.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'HUM.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'JNJ.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'LLY.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'MDT.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'MRK.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'PFE.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'SYK.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'TMO.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'UNH.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'manager': Dict('ABT.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'AMGN.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'BDX.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'BMY.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'HUM.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'JNJ.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'LLY.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'MDT.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'MRK.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'PFE.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'SYK.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'TMO.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'UNH.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)))))}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5227)\u001b[0m 2023-09-28 08:39:43,050\tDEBUG rollout_worker.py:645 -- Created rollout worker with env <ray.rllib.env.multi_agent_env.MultiAgentEnvWrapper object at 0x17af78490> (<HRL instance>), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['HUM.US', 'ABT.US', 'TMO.US', 'manager_policy', 'PFE.US', 'SYK.US', 'MDT.US', 'MRK.US', 'UNH.US', 'BDX.US', 'JNJ.US', 'AMGN.US', 'LLY.US', 'BMY.US']>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5222)\u001b[0m 2023-09-28 08:39:43,086\tDEBUG rollout_worker.py:645 -- Created rollout worker with env <ray.rllib.env.multi_agent_env.MultiAgentEnvWrapper object at 0x17db60460> (<HRL instance>), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['manager_policy', 'UNH.US', 'JNJ.US', 'HUM.US', 'LLY.US', 'PFE.US', 'SYK.US', 'MDT.US', 'BMY.US', 'TMO.US', 'BDX.US', 'MRK.US', 'AMGN.US', 'ABT.US']>\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,129\tINFO worker_set.py:297 -- Inferred observation/action spaces from remote worker (local worker has no env): {'manager_policy': (Dict('ABT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'AMGN.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BDX.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BMY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'HUM.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'JNJ.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'LLY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MDT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MRK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'PFE.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'SYK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'TMO.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'UNH.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))), Dict('ABT.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'AMGN.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'BDX.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'BMY.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'HUM.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'JNJ.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'LLY.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'MDT.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'MRK.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'PFE.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'SYK.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'TMO.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'UNH.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)))), 'UNH.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'JNJ.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'HUM.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'LLY.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'PFE.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'SYK.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'MDT.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'BMY.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'TMO.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'BDX.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'MRK.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'AMGN.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), 'ABT.US': (Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3))), '__env__': (Dict('ABT.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'AMGN.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'BDX.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'BMY.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'HUM.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'JNJ.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'LLY.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'MDT.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'MRK.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'PFE.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'SYK.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'TMO.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'UNH.US': Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)), 'manager': Dict('ABT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'AMGN.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BDX.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BMY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'HUM.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'JNJ.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'LLY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MDT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MRK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'PFE.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'SYK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'TMO.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'UNH.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)))), Dict('ABT.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'AMGN.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'BDX.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'BMY.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'HUM.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'JNJ.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'LLY.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'MDT.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'MRK.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'PFE.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'SYK.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'TMO.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'UNH.US': Dict('amount': Box(0.0, 1.0, (1,), float32), 'type': Discrete(3)), 'manager': Dict('ABT.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'AMGN.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'BDX.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'BMY.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'HUM.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'JNJ.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'LLY.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'MDT.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'MRK.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'PFE.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'SYK.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'TMO.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)), 'UNH.US': Dict('capital_allocation': Box(0.0, 1.0, (1,), float32)))))}\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,226\tINFO policy.py:1294 -- Policy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,893\tINFO rollout_worker.py:1742 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['BDX.US', 'MDT.US', 'TMO.US', 'MRK.US', 'PFE.US', 'manager_policy', 'AMGN.US', 'HUM.US', 'UNH.US', 'LLY.US', 'JNJ.US', 'SYK.US', 'BMY.US', 'ABT.US']>\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,893\tINFO rollout_worker.py:1743 -- Built preprocessor map: {'ABT.US': None, 'AMGN.US': None, 'BDX.US': None, 'BMY.US': None, 'HUM.US': None, 'JNJ.US': None, 'LLY.US': None, 'MDT.US': None, 'MRK.US': None, 'PFE.US': None, 'SYK.US': None, 'TMO.US': None, 'UNH.US': None, 'manager_policy': None}\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,895\tINFO rollout_worker.py:550 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,895\tDEBUG rollout_worker.py:645 -- Created rollout worker with env None (None), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['BDX.US', 'MDT.US', 'TMO.US', 'MRK.US', 'PFE.US', 'manager_policy', 'AMGN.US', 'HUM.US', 'UNH.US', 'LLY.US', 'JNJ.US', 'SYK.US', 'BMY.US', 'ABT.US']>\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,911\tINFO algorithm_config.py:3667 -- Executing eagerly (framework='tf2'), with eager_tracing=True. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m Trainable.setup took 11.507 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,946\tINFO rollout_worker.py:1742 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['UNH.US', 'MRK.US', 'AMGN.US', 'PFE.US', 'TMO.US', 'JNJ.US', 'BMY.US', 'BDX.US', 'HUM.US', 'ABT.US', 'LLY.US', 'manager_policy', 'SYK.US', 'MDT.US']>\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,948\tDEBUG rollout_worker.py:645 -- Created rollout worker with env None (None), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['UNH.US', 'MRK.US', 'AMGN.US', 'PFE.US', 'TMO.US', 'JNJ.US', 'BMY.US', 'BDX.US', 'HUM.US', 'ABT.US', 'LLY.US', 'manager_policy', 'SYK.US', 'MDT.US']>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:44,107\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5227)\u001b[0m WARNING:tensorflow:5 out of the last 5 calls to <bound method with_lock.<locals>.wrapper of A3CTF2Policy_traced> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5227)\u001b[0m WARNING:tensorflow:5 out of the last 5 calls to <bound method with_lock.<locals>.wrapper of A3CTF2Policy_traced> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[2m\u001b[36m(pid=5222)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5227)\u001b[0m 2023-09-28 08:39:46,880\tINFO json_writer.py:107 -- Writing to new output file <_io.TextIOWrapper name='/Volumes/SSD980/ray/results/tunerun2/A2C/A2C_hrl_c3ec2_00001_1_entropy_coeff=0.0254,gamma=0.9723,lambda=0.9582,lr=0.0000,vf_loss_coeff=0.1742_2023-09-28_08-39-25/output-2023-09-28_08-39-46_worker-3_0.json' mode='w' encoding='UTF-8'>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5227)\u001b[0m 2023-09-28 08:39:46,881\tDEBUG json_writer.py:81 -- Wrote 100336 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/results/tunerun2/A2C/A2C_hrl_c3ec2_00001_1_entropy_coeff=0.0254,gamma=0.9723,lambda=0.9582,lr=0.0000,vf_loss_coeff=0.1742_2023-09-28_08-39-25/output-2023-09-28_08-39-46_worker-3_0.json' mode='w' encoding='UTF-8'> in 0.004518985748291016s\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,179\tDEBUG rollout_worker.py:1761 -- Creating policy for ABT.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,925\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(0.0, inf, (1,), float32)\u001b[32m [repeated 311x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,943\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Box(-inf, inf, (1,), float32)\u001b[32m [repeated 4463x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,257\tDEBUG catalog.py:793 -- Created preprocessor <ray.rllib.models.preprocessors.DictFlatteningPreprocessor object at 0x15c187910>: Dict('capital_allocation': Box(0.0, inf, (1,), float32), 'current_cash': Box(-inf, inf, (1,), float32), 'current_price': Box(-inf, inf, (1,), float32), 'current_stock_exposure': Box(-inf, inf, (1,), float32), 'day': Box(-inf, inf, (1,), float32), 'pnl': Box(-inf, inf, (1,), float32), 'shares_held': Box(-inf, inf, (1,), float32), 'tech_indicators': Box(-inf, inf, (14,), float32), 'total_costs': Box(-inf, inf, (1,), float32), 'total_trades': Box(-inf, inf, (1,), float32)) -> (23,)\u001b[32m [repeated 103x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,235\tDEBUG rollout_worker.py:1761 -- Creating policy for AMGN.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,237\tDEBUG rollout_worker.py:1761 -- Creating policy for BDX.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,240\tDEBUG rollout_worker.py:1761 -- Creating policy for BMY.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,242\tDEBUG rollout_worker.py:1761 -- Creating policy for HUM.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,245\tDEBUG rollout_worker.py:1761 -- Creating policy for JNJ.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,247\tDEBUG rollout_worker.py:1761 -- Creating policy for LLY.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,249\tDEBUG rollout_worker.py:1761 -- Creating policy for MDT.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,252\tDEBUG rollout_worker.py:1761 -- Creating policy for MRK.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,253\tDEBUG rollout_worker.py:1761 -- Creating policy for PFE.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,254\tDEBUG rollout_worker.py:1761 -- Creating policy for SYK.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,255\tDEBUG rollout_worker.py:1761 -- Creating policy for TMO.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,256\tDEBUG rollout_worker.py:1761 -- Creating policy for UNH.US\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,257\tDEBUG rollout_worker.py:1761 -- Creating policy for manager_policy\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,943\tDEBUG preprocessors.py:304 -- Creating sub-preprocessor for Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))\u001b[32m [repeated 299x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,263\tDEBUG catalog.py:793 -- Created preprocessor <ray.rllib.models.preprocessors.DictFlatteningPreprocessor object at 0x15c187d30>: Dict('ABT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'AMGN.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BDX.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'BMY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'HUM.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'JNJ.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'LLY.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MDT.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'MRK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'PFE.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'SYK.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'TMO.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32)), 'UNH.US': Dict('Sharpe_Ratio': Box(-inf, inf, (1,), float32), 'portfolio_value': Box(-inf, inf, (1,), float32), 'return_mean': Box(-inf, inf, (1,), float32), 'return_std': Box(-inf, inf, (1,), float32))) -> (52,)\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,819\tINFO eager_tf_policy_v2.py:80 -- Creating TF-eager policy running on CPU.\u001b[32m [repeated 111x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5222)\u001b[0m 2023-09-28 08:39:42,974\tINFO policy.py:1294 -- Policy (worker=1) running on CPU.\u001b[32m [repeated 83x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,305\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,306\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_modelv2.TFModelV2` has been deprecated. Use `ray.rllib.core.rl_module.RLModule` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,312\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.misc.normc_initializer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:46,952\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:46,961\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m \n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m { 'count': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'policy_batches': { 'ABT.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.017, max=0.002, mean=-0.006),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-4.549, max=-2.006, mean=-2.427),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.011, max=0.135, mean=0.099),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-2.252, max=2.0, mean=0.374),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-119.046, max=0.744, mean=-25.326),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-12516.689, max=7075415.0, mean=304317.688),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-8074.749, max=7075415.0, mean=301944.406),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-2.252, max=2.0, mean=0.356),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-70.369, max=2.799, mean=-2.861),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-70.369, max=2.799, mean=-4.272),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=3.0, max=3.0, mean=3.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-119.055, max=0.734, mean=-25.336),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.011, max=-0.008, mean=-0.01),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=-0.011, max=-0.008, mean=-0.01)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'AMGN.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.016, max=0.008, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'action_logp': np.ndarray((32,), dtype=float32, min=-5.268, max=-2.016, mean=-2.621),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'action_prob': np.ndarray((32,), dtype=float32, min=0.005, max=0.133, mean=0.088),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'actions': np.ndarray((32, 2), dtype=float32, min=-1.805, max=2.535, mean=0.498),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'advantages': np.ndarray((32,), dtype=float32, min=-16.783, max=32.969, mean=0.541),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'agent_index': np.ndarray((32,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'new_obs': np.ndarray((32, 23), dtype=float32, min=-1699.332, max=2968512.0, mean=138661.016),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'obs': np.ndarray((32, 23), dtype=float32, min=-1699.332, max=2968512.0, mean=135816.062),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'prev_actions': np.ndarray((32, 2), dtype=float32, min=-1.759, max=2.535, mean=0.495),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'prev_rewards': np.ndarray((32,), dtype=float32, min=-32.967, max=32.967, mean=1.236),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'rewards': np.ndarray((32,), dtype=float32, min=-32.967, max=32.967, mean=0.362),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'unroll_id': np.ndarray((32,), dtype=int64, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'value_targets': np.ndarray((32,), dtype=float32, min=-16.788, max=32.964, mean=0.538),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.006, max=-0.0, mean=-0.003),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                    'vf_preds': np.ndarray((32,), dtype=float32, min=-0.006, max=-0.0, mean=-0.003)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'BDX.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.011, max=0.013, mean=0.001),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-5.208, max=-2.019, mean=-2.442),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.005, max=0.133, mean=0.098),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-2.503, max=2.0, mean=0.456),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-16.767, max=93.009, mean=29.683),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=3.0, max=3.0, mean=3.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-967.66, max=1524314.375, mean=69080.148),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-967.66, max=1524314.375, mean=69080.672),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-2.503, max=2.0, mean=0.409),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-13.873, max=60.171, mean=1.937),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-13.873, max=60.171, mean=3.326),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=7.0, max=7.0, mean=7.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-16.76, max=93.011, mean=29.688),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=0.002, max=0.008, mean=0.005),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=0.002, max=0.008, mean=0.005)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'BMY.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.013, max=-0.001, mean=-0.007),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-4.304, max=-2.014, mean=-2.675),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.014, max=0.133, mean=0.084),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-2.141, max=2.0, mean=0.487),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-119.992, max=12.26, mean=-18.385),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=4.0, max=4.0, mean=4.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-9953.668, max=2857173.5, mean=127917.609),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-9953.668, max=2857173.5, mean=127796.117),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-2.107, max=2.0, mean=0.479),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-66.432, max=29.68, mean=-3.457),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-66.432, max=29.68, mean=-3.688),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=9.0, max=9.0, mean=9.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-119.998, max=12.254, mean=-18.389),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.007, max=-0.002, mean=-0.004),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=-0.007, max=-0.002, mean=-0.004)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'HUM.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.013, max=0.01, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-3.802, max=-2.009, mean=-2.555),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.022, max=0.134, mean=0.087),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-1.882, max=2.0, mean=0.292),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-27.472, max=79.34, mean=15.815),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-9967.238, max=345678.0, mean=21035.688),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-9967.238, max=345678.0, mean=20218.762),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-1.882, max=2.0, mean=0.296),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-101.242, max=36.792, mean=1.552),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-101.242, max=36.792, mean=1.066),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=11.0, max=11.0, mean=11.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-27.474, max=79.338, mean=15.814),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.004, max=0.006, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=-0.004, max=0.006, mean=-0.001)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'JNJ.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.007, max=0.006, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-4.698, max=-2.018, mean=-2.508),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.009, max=0.133, mean=0.097),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-2.329, max=2.0, mean=0.467),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-19.599, max=9.658, mean=-5.962),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=6.0, max=6.0, mean=6.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-1870.901, max=9122048.0, mean=395924.469),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-1870.901, max=9122048.0, mean=395740.344),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-2.329, max=2.0, mean=0.435),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-16.161, max=11.609, mean=-0.734),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-16.161, max=11.609, mean=-0.987),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=13.0, max=13.0, mean=13.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-19.592, max=9.666, mean=-5.955),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=0.006, max=0.007, mean=0.007),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=0.006, max=0.007, mean=0.007)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'LLY.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.011, max=0.003, mean=-0.004),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-4.219, max=-2.026, mean=-2.444),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.015, max=0.132, mean=0.097),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-2.096, max=2.0, mean=0.504),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-65.822, max=-0.0, mean=-14.998),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=7.0, max=7.0, mean=7.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-2258.431, max=2188128.0, mean=98849.945),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-2219.708, max=2188128.0, mean=97754.805),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-2.096, max=2.0, mean=0.449),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-28.513, max=29.715, mean=-1.255),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-28.513, max=29.715, mean=-1.379),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=15.0, max=15.0, mean=15.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-65.819, max=0.004, mean=-14.995),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=0.002, max=0.006, mean=0.003),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=0.002, max=0.006, mean=0.003)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'MDT.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.012, max=0.012, mean=0.001),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-4.825, max=-2.002, mean=-2.636),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.008, max=0.135, mean=0.087),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-1.864, max=2.364, mean=0.53),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-51.101, max=-1.999, mean=-23.288),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=8.0, max=8.0, mean=8.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-3837.633, max=3927296.0, mean=180036.516),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-3423.231, max=3927296.0, mean=176480.812),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-1.864, max=2.364, mean=0.406),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-21.105, max=13.535, mean=-3.036),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-21.105, max=13.535, mean=-3.427),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=17.0, max=17.0, mean=17.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-51.109, max=-2.008, mean=-23.296),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.01, max=-0.007, mean=-0.009),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=-0.01, max=-0.007, mean=-0.009)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'MRK.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.003, max=0.006, mean=0.002),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-4.802, max=-2.019, mean=-2.55),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.008, max=0.133, mean=0.092),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-2.351, max=2.194, mean=0.626),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-45.727, max=20.148, mean=-8.865),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=9.0, max=9.0, mean=9.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-4366.6, max=7334398.5, mean=315331.219),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-4366.6, max=7334398.5, mean=312400.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-1.695, max=2.194, mean=0.653),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-17.953, max=18.033, mean=-2.251),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-17.953, max=18.033, mean=-2.173),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=19.0, max=19.0, mean=19.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-45.731, max=20.143, mean=-8.871),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.007, max=-0.004, mean=-0.006),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=-0.007, max=-0.004, mean=-0.006)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'PFE.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.006, max=0.008, mean=0.001),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-7.428, max=-2.022, mean=-2.548),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.001, max=0.132, mean=0.098),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-2.411, max=3.308, mean=0.595),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-111.953, max=72.144, mean=3.957),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=10.0, max=10.0, mean=10.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-3463.674, max=13160256.0, mean=568116.625),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-3463.674, max=13160256.0, mean=564016.188),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-2.411, max=3.308, mean=0.509),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-77.584, max=79.599, mean=-0.389),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-77.584, max=79.599, mean=0.202),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=21.0, max=21.0, mean=21.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-111.953, max=72.144, mean=3.957),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=-0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'SYK.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.014, max=0.004, mean=-0.005),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-4.48, max=-2.0, mean=-2.55),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.011, max=0.135, mean=0.091),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-1.872, max=2.208, mean=0.567),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-19.047, max=26.385, mean=1.955),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=11.0, max=11.0, mean=11.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-3859.983, max=526560.0, mean=33543.512),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-3859.983, max=526560.0, mean=32500.904),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-1.872, max=2.208, mean=0.485),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-24.451, max=29.752, mean=-0.425),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-24.451, max=29.752, mean=-0.163),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=23.0, max=23.0, mean=23.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-19.038, max=26.394, mean=1.967),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=0.006, max=0.018, mean=0.011),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=0.006, max=0.018, mean=0.011)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'TMO.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.02, max=0.017, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-5.373, max=-2.01, mean=-2.56),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.005, max=0.134, mean=0.092),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-1.956, max=2.581, mean=0.495),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-1.534, max=163.978, mean=31.584),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=12.0, max=12.0, mean=12.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-320.936, max=338528.562, mean=13489.719),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-320.936, max=338528.562, mean=12550.16),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-1.834, max=2.581, mean=0.49),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-45.314, max=93.007, mean=5.5),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-45.314, max=93.007, mean=6.482),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=25.0, max=25.0, mean=25.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-1.534, max=163.979, mean=31.587),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.001, max=0.009, mean=0.004),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=-0.001, max=0.009, mean=0.003)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'UNH.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.01, max=0.008, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_logp': np.ndarray((32,), dtype=float32, min=-6.776, max=-2.009, mean=-2.632),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'action_prob': np.ndarray((32,), dtype=float32, min=0.001, max=0.134, mean=0.092),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'actions': np.ndarray((32, 2), dtype=float32, min=-1.966, max=3.077, mean=0.729),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'advantages': np.ndarray((32,), dtype=float32, min=-93.637, max=180.843, mean=77.741),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'agent_index': np.ndarray((32,), dtype=int64, min=13.0, max=13.0, mean=13.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'new_obs': np.ndarray((32, 23), dtype=float32, min=-2377.873, max=2697408.0, mean=91147.273),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'obs': np.ndarray((32, 23), dtype=float32, min=-2377.873, max=2584128.0, mean=82118.898),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_actions': np.ndarray((32, 2), dtype=float32, min=-1.966, max=3.077, mean=0.645),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'prev_rewards': np.ndarray((32,), dtype=float32, min=-91.788, max=271.558, mean=13.246),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'rewards': np.ndarray((32,), dtype=float32, min=-91.788, max=271.558, mean=12.264),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'unroll_id': np.ndarray((32,), dtype=int64, min=27.0, max=27.0, mean=27.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'value_targets': np.ndarray((32,), dtype=float32, min=-93.642, max=180.835, mean=77.735),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.012, max=0.001, mean=-0.007),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_preds': np.ndarray((32,), dtype=float32, min=-0.012, max=0.001, mean=-0.007)},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'manager_policy': { 'action_dist_inputs': np.ndarray((32, 26), dtype=float32, min=-0.015, max=0.023, mean=0.001),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'action_logp': np.ndarray((32,), dtype=float32, min=-24.776, max=-13.801, mean=-18.594),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'action_prob': np.ndarray((32,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'actions': np.ndarray((32, 13), dtype=float32, min=-3.005, max=3.226, mean=-0.03),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'advantages': np.ndarray((32,), dtype=float32, min=-1.516, max=5.283, mean=1.491),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'agent_index': np.ndarray((32,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'eps_id': np.ndarray((32,), dtype=int64, min=3.349620037481327e+17, max=8.774536141429148e+17, mean=6.876540154698665e+17),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'new_obs': np.ndarray((32, 52), dtype=float32, min=-12.87, max=506126.312, mean=38574.641),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'obs': np.ndarray((32, 52), dtype=float32, min=-12.87, max=480950.531, mean=38548.602),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'prev_actions': np.ndarray((32, 13), dtype=float32, min=-3.005, max=3.226, mean=-0.019),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'prev_rewards': np.ndarray((32,), dtype=float32, min=-0.46, max=2.465, mean=0.298),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'rewards': np.ndarray((32,), dtype=float32, min=-0.46, max=2.465, mean=0.325),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'unroll_id': np.ndarray((32,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'value_targets': np.ndarray((32,), dtype=float32, min=-1.508, max=5.287, mean=1.494),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.013, max=0.012, mean=0.004),\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                           'vf_preds': np.ndarray((32,), dtype=float32, min=-0.013, max=0.012, mean=0.003)}},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m \n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,297\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,297\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,297\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,299\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,299\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:43,300\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5223)\u001b[0m 2023-09-28 08:39:47,026\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,372\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.MultiActionDistribution` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfMultiDistribution` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,373\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.DiagGaussian` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfDiagGaussian` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,373\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.TFActionDistribution` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfDistribution` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,377\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.Categorical` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.Categorical` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:47,201\tWARNING deprecation.py:50 -- DeprecationWarning: `compute_gradients` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:47,303\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable fc_1/kernel:0\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:47,303\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable fc_1/bias:0\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:47,303\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable fc_2/kernel:0\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:47,303\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable fc_2/bias:0\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:47,303\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable fc_out/kernel:0\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:47,303\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable fc_out/bias:0\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:47,303\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable value_out/kernel:0\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:47,303\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable value_out/bias:0\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m WARNING:tensorflow:5 out of the last 5 calls to <bound method EagerTFPolicyV2._learn_on_batch_helper of A3CTF2Policy_traced> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m WARNING:tensorflow:5 out of the last 5 calls to <bound method EagerTFPolicyV2._learn_on_batch_helper of A3CTF2Policy_traced> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,945\tINFO util.py:118 -- Using connectors:\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,946\tINFO util.py:119 --     AgentConnectorPipeline\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m         ObsPreprocessorConnector\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m         StateBufferConnector\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m         ViewRequirementAgentConnector\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,946\tINFO util.py:120 --     ActionConnectorPipeline\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m         ConvertToNumpyConnector\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m         NormalizeActionsConnector\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m         ImmutableActionsConnector\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,948\tINFO json_writer.py:50 -- You are using JSONWriter. It is recommended to use DatasetWriter instead.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,820\tINFO policy.py:1294 -- Policy (worker=local) running on CPU.\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,946\tINFO rollout_worker.py:1743 -- Built preprocessor map: {'ABT.US': None, 'AMGN.US': None, 'BDX.US': None, 'BMY.US': None, 'HUM.US': None, 'JNJ.US': None, 'LLY.US': None, 'MDT.US': None, 'MRK.US': None, 'PFE.US': None, 'SYK.US': None, 'TMO.US': None, 'UNH.US': None, 'manager_policy': None}\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,948\tINFO rollout_worker.py:550 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:43,970\tINFO algorithm_config.py:3667 -- Executing eagerly (framework='tf2'), with eager_tracing=True. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m Trainable.setup took 11.566 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5222)\u001b[0m 2023-09-28 08:39:44,190\tINFO rollout_worker.py:690 -- Generating sample batch of size 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5222)\u001b[0m WARNING:tensorflow:6 out of the last 6 calls to <bound method with_lock.<locals>.wrapper of A3CTF2Policy_traced> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m 2023-09-28 08:39:55,213\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m { 'ABT.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 40.000004,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.39264,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': -1907.0433,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 33895.95},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': -1.013279e-05},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'AMGN.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'policy_entropy': 80.576904,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'policy_loss': -3.3338966,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                   'vf_loss': 1810.0549},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                'vf_explained_var': 0.00011783838},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'BDX.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 39.999996,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.29306,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': 2305.891,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 33707.188},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': -7.390976e-06},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'BMY.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.451,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': -1490.9479,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 20517.621},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': 1.7046928e-05},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'HUM.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.35127,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': 1357.3127,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 14293.232},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': -1.0728836e-06},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'JNJ.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 40.000004,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.731445,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': -486.1594,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 1287.006},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': -3.7789345e-05},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'LLY.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.53737,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': -1111.7954,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 8022.0},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': 3.8087368e-05},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'MDT.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.367096,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': -1903.5897,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 11451.156},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': -2.6106834e-05},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'MRK.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.5073,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': -684.8697,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 5459.323},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': -3.695488e-06},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'PFE.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.69364,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': 235.2103,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 21715.215},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': 7.1525574e-07},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'SYK.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.34216,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': 150.17735,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 1707.3002},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': -2.0623207e-05},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'TMO.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 39.999996,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.63844,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': 2360.1567,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 50625.99},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': -1.66893e-05},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'UNH.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_entropy': 80.48277,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'policy_loss': 6724.289,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'var_gnorm': 22.62743,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                  'vf_loss': 157513.16},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m               'vf_explained_var': -5.722046e-06},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m   'manager_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'grad_gnorm': 40.0,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'learner_stats': { 'cur_lr': 9.859579586191103e-05,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                          'entropy_coeff': 0.09815014153718948,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                          'policy_entropy': 590.8353,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                          'policy_loss': 891.9219,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                          'var_gnorm': 22.627476,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                                          'vf_loss': 96.15842},\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'num_agent_steps_trained': 32,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'num_grad_updates_lifetime': 1,\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m                       'vf_explained_var': -0.001449585}}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5222)\u001b[0m 2023-09-28 08:39:46,924\tINFO json_writer.py:107 -- Writing to new output file <_io.TextIOWrapper name='/Volumes/SSD980/ray/results/tunerun2/A2C/A2C_hrl_c3ec2_00000_0_entropy_coeff=0.0982,gamma=0.9788,lambda=0.9898,lr=0.0001,vf_loss_coeff=0.1134_2023-09-28_08-39-25/output-2023-09-28_08-39-46_worker-1_0.json' mode='w' encoding='UTF-8'>\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5222)\u001b[0m 2023-09-28 08:39:46,925\tDEBUG json_writer.py:81 -- Wrote 110942 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/results/tunerun2/A2C/A2C_hrl_c3ec2_00000_0_entropy_coeff=0.0982,gamma=0.9788,lambda=0.9898,lr=0.0001,vf_loss_coeff=0.1134_2023-09-28_08-39-25/output-2023-09-28_08-39-46_worker-1_0.json' mode='w' encoding='UTF-8'> in 0.007759809494018555s\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:47,037\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:47,045\tINFO rollout_worker.py:786 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(A2C pid=5198)\u001b[0m \u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m { 'count': 32,\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'policy_batches': { 'ABT.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.016, max=0.007, mean=-0.001),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'action_logp': np.ndarray((32,), dtype=float32, min=-24.319, max=-14.64, mean=-17.818),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'action_prob': np.ndarray((32,), dtype=float32, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'actions': np.ndarray((32, 13), dtype=float32, min=-3.404, max=2.721, mean=-0.082),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'advantages': np.ndarray((32,), dtype=float32, min=-2.656, max=0.775, mean=-0.607),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'agent_index': np.ndarray((32,), dtype=int64, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'eps_id': np.ndarray((32,), dtype=int64, min=1.3434254895257957e+17, max=6.912236311907135e+17, mean=3.3961916041261274e+17),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'new_obs': np.ndarray((32, 52), dtype=float32, min=-15.872, max=517745.906, mean=38344.918),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'obs': np.ndarray((32, 52), dtype=float32, min=-15.872, max=517745.906, mean=38354.082),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'prev_actions': np.ndarray((32, 13), dtype=float32, min=-3.404, max=2.721, mean=-0.079),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'prev_rewards': np.ndarray((32,), dtype=float32, min=-0.641, max=1.321, mean=-0.085),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'rewards': np.ndarray((32,), dtype=float32, min=-0.641, max=1.321, mean=-0.092),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           't': np.ndarray((32,), dtype=int64, min=0.0, max=10.0, mean=4.844),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'unroll_id': np.ndarray((32,), dtype=int64, min=1.0, max=1.0, mean=1.0),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'value_targets': np.ndarray((32,), dtype=float32, min=-2.654, max=0.773, mean=-0.606),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.009, max=0.011, mean=0.001),\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                           'vf_preds': np.ndarray((32,), dtype=float32, min=-0.009, max=0.011, mean=0.001)}},\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'AMGN.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.009, max=0.004, mean=-0.002),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'BDX.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.012, max=0.005, mean=-0.002),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'BMY.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.007, max=0.013, mean=0.002),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'HUM.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.015, max=0.014, mean=-0.001),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'JNJ.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.004, max=0.005, mean=-0.001),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'LLY.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.007, max=0.009, mean=0.001),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'MDT.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.013, max=0.006, mean=-0.002),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'MRK.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.007, max=0.02, mean=0.002),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'PFE.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.001, max=0.006, mean=0.002),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'SYK.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.011, max=0.018, mean=0.004),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'TMO.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.018, max=0.013, mean=0.001),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'UNH.US': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.005, max=0.016, mean=0.005),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'manager_policy': { 'action_dist_inputs': np.ndarray((32, 26), dtype=float32, min=-0.022, max=0.017, mean=-0.0),\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'type': 'MultiAgentBatch'}\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5222)\u001b[0m 2023-09-28 08:39:46,934\tINFO rollout_worker.py:732 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:47,274\tWARNING deprecation.py:50 -- DeprecationWarning: `compute_gradients` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:47,378\tINFO eager_tf_policy_v2.py:1150 -- Optimizing variable value_out/bias:0\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics  </th><th>counters                                                                                                                      </th><th>custom_metrics  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_sampled_throughput_per_sec</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained_throughput_per_sec</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                          </th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf  </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                </th><th>timers                                                                                                                                                   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>A2C_hrl_c3ec2_00000</td><td style=\"text-align: right;\">                   8064</td><td>{}                 </td><td>{&#x27;num_env_steps_sampled&#x27;: 576, &#x27;num_env_steps_trained&#x27;: 576, &#x27;num_agent_steps_sampled&#x27;: 8064, &#x27;num_agent_steps_trained&#x27;: 8064}</td><td>{}              </td><td style=\"text-align: right;\">               nan</td><td>{}             </td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                  nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                   0</td><td>{&#x27;learner&#x27;: {&#x27;manager_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: 901.1411, &#x27;policy_entropy&#x27;: 601.7747, &#x27;var_gnorm&#x27;: 22.628664, &#x27;vf_loss&#x27;: 50.134377}, &#x27;grad_gnorm&#x27;: 39.999992, &#x27;vf_explained_var&#x27;: 0.009977937, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;ABT.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: -1006.28406, &#x27;policy_entropy&#x27;: 80.95214, &#x27;var_gnorm&#x27;: 22.626245, &#x27;vf_loss&#x27;: 12896.663}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -3.671646e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;AMGN.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: -3292.4568, &#x27;policy_entropy&#x27;: 79.18648, &#x27;var_gnorm&#x27;: 22.626692, &#x27;vf_loss&#x27;: 145427.92}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -6.556511e-06, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;BDX.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: 664.559, &#x27;policy_entropy&#x27;: 76.49822, &#x27;var_gnorm&#x27;: 22.629839, &#x27;vf_loss&#x27;: 4443.395}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 3.516674e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;BMY.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: -648.1537, &#x27;policy_entropy&#x27;: 82.30185, &#x27;var_gnorm&#x27;: 22.626278, &#x27;vf_loss&#x27;: 25455.188}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -1.04904175e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;HUM.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: -915.4024, &#x27;policy_entropy&#x27;: 83.10939, &#x27;var_gnorm&#x27;: 22.627779, &#x27;vf_loss&#x27;: 12785.971}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -8.237362e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;JNJ.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: -187.85243, &#x27;policy_entropy&#x27;: 78.22313, &#x27;var_gnorm&#x27;: 22.6267, &#x27;vf_loss&#x27;: 8748.242}, &#x27;grad_gnorm&#x27;: 39.999996, &#x27;vf_explained_var&#x27;: 1.1920929e-07, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;LLY.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: 325.8291, &#x27;policy_entropy&#x27;: 81.954185, &#x27;var_gnorm&#x27;: 22.628183, &#x27;vf_loss&#x27;: 4307.9087}, &#x27;grad_gnorm&#x27;: 40.000004, &#x27;vf_explained_var&#x27;: -4.017353e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;MDT.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: -876.94666, &#x27;policy_entropy&#x27;: 79.1238, &#x27;var_gnorm&#x27;: 22.628193, &#x27;vf_loss&#x27;: 8408.438}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 3.8444996e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;MRK.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: 2009.5212, &#x27;policy_entropy&#x27;: 79.18473, &#x27;var_gnorm&#x27;: 22.62747, &#x27;vf_loss&#x27;: 70097.32}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 8.523464e-06, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;PFE.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: -778.70966, &#x27;policy_entropy&#x27;: 79.73829, &#x27;var_gnorm&#x27;: 22.626127, &#x27;vf_loss&#x27;: 4801.4683}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 4.118681e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;SYK.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: -1364.7584, &#x27;policy_entropy&#x27;: 79.540276, &#x27;var_gnorm&#x27;: 22.62874, &#x27;vf_loss&#x27;: 41140.688}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 0.00020438433, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;TMO.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: -1614.6804, &#x27;policy_entropy&#x27;: 79.96266, &#x27;var_gnorm&#x27;: 22.626652, &#x27;vf_loss&#x27;: 69313.375}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -2.6464462e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;UNH.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 9.859579586191103e-05, &#x27;entropy_coeff&#x27;: 0.09815014153718948, &#x27;policy_loss&#x27;: -1679.7865, &#x27;policy_entropy&#x27;: 82.97427, &#x27;var_gnorm&#x27;: 22.626808, &#x27;vf_loss&#x27;: 48220.156}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 2.7000904e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}}, &#x27;num_env_steps_sampled&#x27;: 576, &#x27;num_env_steps_trained&#x27;: 576, &#x27;num_agent_steps_sampled&#x27;: 8064, &#x27;num_agent_steps_trained&#x27;: 8064}                                        </td><td style=\"text-align: right;\">                     8064</td><td style=\"text-align: right;\">                     8064</td><td style=\"text-align: right;\">                    576</td><td style=\"text-align: right;\">                              544</td><td style=\"text-align: right;\">                                   53.4054</td><td style=\"text-align: right;\">                    576</td><td style=\"text-align: right;\">                              544</td><td style=\"text-align: right;\">                                   53.4054</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    3</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          544</td><td>{&#x27;cpu_util_percent&#x27;: 83.28571428571426, &#x27;ram_util_percent&#x27;: 84.42857142857143}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{}            </td><td>{&#x27;episode_reward_max&#x27;: nan, &#x27;episode_reward_min&#x27;: nan, &#x27;episode_reward_mean&#x27;: nan, &#x27;episode_len_mean&#x27;: nan, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 0, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [], &#x27;episode_lengths&#x27;: []}, &#x27;sampler_perf&#x27;: {}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 648.727, &#x27;sample_time_ms&#x27;: 570.327, &#x27;learn_time_ms&#x27;: 60.712, &#x27;learn_throughput&#x27;: 527.075, &#x27;synch_weights_time_ms&#x27;: 17.043}</td></tr>\n",
       "<tr><td>A2C_hrl_c3ec2_00001</td><td style=\"text-align: right;\">                   8064</td><td>{}                 </td><td>{&#x27;num_env_steps_sampled&#x27;: 576, &#x27;num_env_steps_trained&#x27;: 576, &#x27;num_agent_steps_sampled&#x27;: 8064, &#x27;num_agent_steps_trained&#x27;: 8064}</td><td>{}              </td><td style=\"text-align: right;\">               nan</td><td>{}             </td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                  nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                   0</td><td>{&#x27;learner&#x27;: {&#x27;manager_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: 556.09717, &#x27;policy_entropy&#x27;: 590.591, &#x27;var_gnorm&#x27;: 22.627518, &#x27;vf_loss&#x27;: 20.31974}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -0.0065151453, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;ABT.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: -865.8674, &#x27;policy_entropy&#x27;: 80.621956, &#x27;var_gnorm&#x27;: 22.627419, &#x27;vf_loss&#x27;: 11959.842}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 2.7775764e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;AMGN.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: -137.4093, &#x27;policy_entropy&#x27;: 80.20775, &#x27;var_gnorm&#x27;: 22.627441, &#x27;vf_loss&#x27;: 13420.486}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -1.5497208e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;BDX.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: -1318.0247, &#x27;policy_entropy&#x27;: 80.59806, &#x27;var_gnorm&#x27;: 22.627346, &#x27;vf_loss&#x27;: 96704.58}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -1.4305115e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;BMY.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: 1918.8545, &#x27;policy_entropy&#x27;: 80.74762, &#x27;var_gnorm&#x27;: 22.627441, &#x27;vf_loss&#x27;: 38827.23}, &#x27;grad_gnorm&#x27;: 40.000004, &#x27;vf_explained_var&#x27;: -5.9604645e-06, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;HUM.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: -385.5206, &#x27;policy_entropy&#x27;: 80.43183, &#x27;var_gnorm&#x27;: 22.627441, &#x27;vf_loss&#x27;: 15117.015}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 3.6120415e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;JNJ.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: -2905.2727, &#x27;policy_entropy&#x27;: 80.2654, &#x27;var_gnorm&#x27;: 22.62743, &#x27;vf_loss&#x27;: 87446.47}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -2.026558e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;LLY.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: -395.91446, &#x27;policy_entropy&#x27;: 80.69043, &#x27;var_gnorm&#x27;: 22.627455, &#x27;vf_loss&#x27;: 10427.22}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -2.7537346e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;MDT.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: 1021.4447, &#x27;policy_entropy&#x27;: 80.41681, &#x27;var_gnorm&#x27;: 22.627434, &#x27;vf_loss&#x27;: 16002.359}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -2.5987625e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;MRK.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: 125.383644, &#x27;policy_entropy&#x27;: 80.13869, &#x27;var_gnorm&#x27;: 22.627495, &#x27;vf_loss&#x27;: 417.1977}, &#x27;grad_gnorm&#x27;: 40.000008, &#x27;vf_explained_var&#x27;: -5.7935715e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;PFE.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: -1232.9484, &#x27;policy_entropy&#x27;: 80.664474, &#x27;var_gnorm&#x27;: 22.627474, &#x27;vf_loss&#x27;: 17973.56}, &#x27;grad_gnorm&#x27;: 40.000004, &#x27;vf_explained_var&#x27;: 3.2782555e-06, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;SYK.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: 2686.023, &#x27;policy_entropy&#x27;: 80.46045, &#x27;var_gnorm&#x27;: 22.627417, &#x27;vf_loss&#x27;: 207924.72}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 1.7046928e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;TMO.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: -3117.0676, &#x27;policy_entropy&#x27;: 80.712105, &#x27;var_gnorm&#x27;: 22.627441, &#x27;vf_loss&#x27;: 112642.0}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 1.8775463e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}, &#x27;UNH.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.0548947102506645e-05, &#x27;entropy_coeff&#x27;: 0.025421002879738808, &#x27;policy_loss&#x27;: -3112.5872, &#x27;policy_entropy&#x27;: 80.46647, &#x27;var_gnorm&#x27;: 22.627409, &#x27;vf_loss&#x27;: 77825.05}, &#x27;grad_gnorm&#x27;: 39.999996, &#x27;vf_explained_var&#x27;: 3.0696392e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 18, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 17}}, &#x27;num_env_steps_sampled&#x27;: 576, &#x27;num_env_steps_trained&#x27;: 576, &#x27;num_agent_steps_sampled&#x27;: 8064, &#x27;num_agent_steps_trained&#x27;: 8064}</td><td style=\"text-align: right;\">                     8064</td><td style=\"text-align: right;\">                     8064</td><td style=\"text-align: right;\">                    576</td><td style=\"text-align: right;\">                              544</td><td style=\"text-align: right;\">                                   52.2368</td><td style=\"text-align: right;\">                    576</td><td style=\"text-align: right;\">                              544</td><td style=\"text-align: right;\">                                   52.2368</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    3</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          544</td><td>{&#x27;cpu_util_percent&#x27;: 76.5, &#x27;ram_util_percent&#x27;: 84.32666666666665}             </td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{}            </td><td>{&#x27;episode_reward_max&#x27;: nan, &#x27;episode_reward_min&#x27;: nan, &#x27;episode_reward_mean&#x27;: nan, &#x27;episode_len_mean&#x27;: nan, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 0, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [], &#x27;episode_lengths&#x27;: []}, &#x27;sampler_perf&#x27;: {}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 661.05, &#x27;sample_time_ms&#x27;: 567.554, &#x27;learn_time_ms&#x27;: 75.07, &#x27;learn_throughput&#x27;: 426.27, &#x27;synch_weights_time_ms&#x27;: 17.809}   </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m WARNING:tensorflow:6 out of the last 6 calls to <bound method EagerTFPolicyV2._learn_on_batch_helper of A3CTF2Policy_traced> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m 2023-09-28 08:39:55,220\tDEBUG rollout_worker.py:825 -- Training out:\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m { 'ABT.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'diff_num_grad_updates_vs_sampler_policy': 0,\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'grad_gnorm': 39.999996,\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'learner_stats': { 'cur_lr': 1.0548947102506645e-05,\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                          'entropy_coeff': 0.025421002879738808,\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                          'policy_entropy': 590.42316,\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                          'policy_loss': -340.913,\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                          'var_gnorm': 22.627476,\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                                          'vf_loss': 20.893913},\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'num_agent_steps_trained': 32,\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'num_grad_updates_lifetime': 1,\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m                       'vf_explained_var': -9.775162e-05}}\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'AMGN.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'BDX.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'BMY.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'HUM.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'JNJ.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'LLY.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'MDT.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'MRK.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'PFE.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'SYK.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'TMO.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'UNH.US': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m   'manager_policy': { 'custom_metrics': {},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5224)\u001b[0m 2023-09-28 08:40:00,300\tDEBUG json_writer.py:81 -- Wrote 115558 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/results/tunerun2/A2C/A2C_hrl_c3ec2_00000_0_entropy_coeff=0.0982,gamma=0.9788,lambda=0.9898,lr=0.0001,vf_loss_coeff=0.1134_2023-09-28_08-39-25/output-2023-09-28_08-39-46_worker-2_0.json' mode='w' encoding='UTF-8'> in 0.003008127212524414s\u001b[32m [repeated 49x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=5199)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5227)\u001b[0m 2023-09-28 08:40:05,474\tDEBUG json_writer.py:81 -- Wrote 108353 bytes to <_io.TextIOWrapper name='/Volumes/SSD980/ray/results/tunerun2/A2C/A2C_hrl_c3ec2_00001_1_entropy_coeff=0.0254,gamma=0.9723,lambda=0.9582,lr=0.0000,vf_loss_coeff=0.1742_2023-09-28_08-39-25/output-2023-09-28_08-39-46_worker-3_0.json' mode='w' encoding='UTF-8'> in 0.0027458667755126953s\u001b[32m [repeated 50x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Mit vielen policies\n",
    "import sys\n",
    "# Redirect the standard error to the log file\n",
    "\n",
    "# Create a log file\n",
    "log_file = open(\"console_logs.txt\", \"w\")\n",
    "\n",
    "\n",
    "# Redirect the standard output to the log file\n",
    "sys.stdout = log_file\n",
    "\n",
    "\n",
    "env = HRL()\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  \n",
    "\n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "def create_policy_spec(worker_id):\n",
    "    # print(f\"Creating policy for {worker_id} with obs space {env.observation_space[worker_id]} and action space {env.action_space[worker_id]}\")\n",
    "    return PolicySpec(\n",
    "        observation_space=env.observation_space[worker_id],\n",
    "        action_space=env.action_space[worker_id],\n",
    "        config={}\n",
    "    )\n",
    "\n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    "\n",
    "policies = {\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    "\n",
    "for worker_id in env.workers:\n",
    "    policies[worker_id] = create_policy_spec(worker_id)\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id == 'manager':\n",
    "        # print(f\"!!!!!! policy mapping manager: {agent_id}\")\n",
    "        return \"manager_policy\"\n",
    "    elif agent_id in env.workers:\n",
    "        return agent_id\n",
    "    else:\n",
    "        print(\"defaul policy triggered\")\n",
    "        return \"default_policy\"\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "    \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "        \"rollout_fragment_length\": \"auto\",\n",
    "        \"framework\": \"tf2\",\n",
    "        \"lr\": tune.uniform(1e-5,1e-4),\n",
    "        \"gamma\": tune.uniform(0.95, 0.9999),\n",
    "        \"lambda\": tune.uniform(0.9,1.0),\n",
    "        \"entropy_coeff\": tune.uniform(0.01,0.1),\n",
    "        \"vf_loss_coeff\": tune.uniform(0.1,0.3),\n",
    "        \"num_workers\": 3, \n",
    "        #Change for Debugging\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"output\": \"logdir\",\n",
    "        \"monitor\": True,\n",
    "}\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"A2C\",\n",
    "    metric=\"episode_reward_mean\", \n",
    "    num_samples=5,\n",
    "    mode=\"max\",\n",
    "    config=param_space, \n",
    "    local_dir=\"/Volumes/SSD980/ray/results/tunerun2\",\n",
    "    search_alg=None,\n",
    "    scheduler=None,\n",
    "    progress_reporter=CLIReporter(max_progress_rows=5,max_report_frequency=120),\n",
    "    max_concurrent_trials=2,\n",
    "    #checkpoint_config not checked yet\n",
    "    checkpoint_config={\n",
    "        \"num_to_keep\": 1,\n",
    "        \"checkpoint_score_attribute\": \"episode_reward_mean\",\n",
    "        \"checkpoint_score_order\": \"max\",\n",
    "        \"checkpoint_frequency\": 10\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HRL()\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  \n",
    "\n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    "    \n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    " \n",
    "initial_params = [{\n",
    "    \"lr\": 0.001,\n",
    "    \"gamma\": 0.92,\n",
    "    \"lambda\": 0.95,\n",
    "    \"entropy_coeff\": 1e-3,\n",
    "    \"vf_loss_coeff\": 0.5,\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": 64,\n",
    "        \"fcnet_activation\":\"relu\",\n",
    "    },\n",
    "}]\n",
    "\n",
    "search_space = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"gamma\": tune.uniform(0.9, 0.99),\n",
    "    \"lambda\": tune.uniform(0.9, 1.0),\n",
    "    \"entropy_coeff\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"vf_loss_coeff\": tune.uniform(0.1, 1.0),\n",
    "    # \"model\": {\n",
    "    #     \"fcnet_hiddens\": tune.grid_search([[64, 64], [128, 128], [256, 256]]),\n",
    "    #     \"fcnet_activation\": tune.choice([\"relu\", \"tanh\"]),\n",
    "    # },\n",
    "}\n",
    "\n",
    "\n",
    "algo = HyperOptSearch(space=search_space,metric=\"episode_reward_mean\", mode=\"max\",)\n",
    "\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "\n",
    "        \"num_workers\": 1,  \n",
    "        \"num_cpus_per_trial\": 1,\n",
    "         \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"gamma\": tune.uniform(0.9, 0.99),\n",
    "    \"lambda\": tune.uniform(0.9, 1.0),\n",
    "    \"entropy_coeff\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"vf_loss_coeff\": tune.uniform(0.1, 1.0),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "          \"rollout_fragment_length\": \"auto\",\n",
    "          \"lr\": tune.uniform(1e-5,1e-4),\n",
    "          \"lambda\": tune.uniform(0.95, 1.0),\n",
    "          \"vf_loss_coeff\": tune.uniform(0.1, 1.0),\n",
    "          \"entropy_coeff\": tune.uniform(1e-4, 1e-1),\n",
    "        \"num_workers\": 1,  \n",
    "        \"num_envs_per_worker\": 1\n",
    "}\n",
    "\n",
    "analysis = tune.run(\"A2C\", \n",
    "                    metric=\"episode_reward_mean\", \n",
    "                    mode=\"max\",\n",
    "                    config=param_space,\n",
    "                    num_samples=10,\n",
    "                    stop={\"training_iteration\": 100},\n",
    "                    progress_reporter=CLIReporter(max_progress_rows=10,max_report_frequency=600),\n",
    "                    # local_dir=\"/Users/floriankockler/rayresults/overnight1\",\n",
    "                    storage_path=\"/Users/floriankockler/Documents/GitHub.nosync/raystorage\",\n",
    "                    checkpoint_config=CheckpointConfig(\n",
    "                        num_to_keep=2,\n",
    "                        checkpoint_score_attribute=\"episode_reward_mean\", \n",
    "                        checkpoint_score_order=\"max\"\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "\n",
    "best = analysis.best_trial\n",
    "print(pretty_print(best.last_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = HRL()\n",
    "\n",
    "n_iterations = 1\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    obs, reward, done, truncated, info= env.step(action)\n",
    "\n",
    "    # print(f\"Action: {action}, Reward: {reward}, Portfolio Value: {obs[0] + obs[1] * obs[2]}\")\n",
    "    \n",
    "    if done[\"__all__\"]:\n",
    "        print(\"Episode finished!\")\n",
    "        state = env.reset()\n",
    "    else:\n",
    "        state = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    "    \n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    "\n",
    "    \n",
    "algo = PPOConfig().environment(env=HRL).multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn = policy_mapping_fn,  \n",
    "    ).training(train_batch_size=4000).build()\n",
    "\n",
    "print(algo.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HRL()\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  \n",
    "\n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    "    \n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    " \n",
    "search_space = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),  # Learning rate\n",
    "    \"gamma\": tune.uniform(0.9, 0.99),  # Discount factor\n",
    "}\n",
    "\n",
    "tune_config = TuneConfig(\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    max_concurrent_trials=1,\n",
    "    num_samples=100,\n",
    "    search_alg=BayesOptSearch(\n",
    "          search_space,\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        random_search_steps=4\n",
    "    ),\n",
    "    scheduler=HyperBandScheduler()\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    name=\"MyExperiment\",\n",
    "    storage_path=\"/Users/floriankockler/rayresults/tuner_trial\",\n",
    "    verbose=2,\n",
    "    # checkpoint_config=air.CheckpointConfig(checkpoint_frequency=2),\n",
    "    callbacks=None,\n",
    "    stop=None,\n",
    "    failure_config=None,\n",
    "    sync_config=None,\n",
    "    checkpoint_config=None,\n",
    "    progress_reporter=CLIReporter(max_progress_rows=10),# Define the policy mapping function\n",
    "\n",
    ")\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "          \"rollout_fragment_length\": \"auto\",\n",
    "          \"lr\": tune.uniform(1e-5,1e-4),\n",
    "        \"num_workers\": 1,  \n",
    "        \"num_cpus_per_trial\": 1,\n",
    "}\n",
    "\n",
    "analysis = tune.run(\"A2C\", metric=\"episode_reward_mean\", mode=\"max\",config=param_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "algo = PPOConfig().environment(env=HRL).multi_agent(\n",
    "    policies={\n",
    "        \"policy_1\": ()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune import CLIReporter\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  # Assuming this is your environment\n",
    " \n",
    "env = HRL()\n",
    "\n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    "    \n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    " \n",
    "search_space = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),  # Learning rate\n",
    "    \"gamma\": tune.uniform(0.9, 0.99),  # Discount factor\n",
    "}\n",
    "\n",
    "tune_config = TuneConfig(\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    max_concurrent_trials=1,\n",
    "    num_samples=100,\n",
    "    search_alg=BayesOptSearch(\n",
    "          search_space,\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        random_search_steps=4\n",
    "    ),\n",
    "    scheduler=HyperBandScheduler()\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    name=\"MyExperiment\",\n",
    "    storage_path=\"/Users/floriankockler/rayresults/tuner_trial\",\n",
    "    verbose=2,\n",
    "    # checkpoint_config=air.CheckpointConfig(checkpoint_frequency=2),\n",
    "    callbacks=None,\n",
    "    stop=None,\n",
    "    failure_config=None,\n",
    "    sync_config=None,\n",
    "    checkpoint_config=None,\n",
    "    progress_reporter=CLIReporter(max_progress_rows=10),# Define the policy mapping function\n",
    "\n",
    ")\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "          \"rollout_fragment_length\": \"auto\",\n",
    "        \"num_workers\": 1,  \n",
    "        \"num_cpus_per_trial\": 1,\n",
    "}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"A2C\",\n",
    "    tune_config=tune_config,\n",
    "    param_space=param_space,\n",
    "\n",
    "    run_config=run_config,\n",
    ")\n",
    "results = tuner.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from ray import air, tune\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "import argparse\n",
    "from ray.tune import CLIReporter\n",
    "from env.multi_agent.hrl import HRL\n",
    "\n",
    "reporter = CLIReporter(max_progress_rows=10)\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  # Assuming this is your environment\n",
    " \n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "manager_config = {\n",
    "    \"df\": train_df,\n",
    "\n",
    "}\n",
    "hrl_config={\n",
    "        \"manager_config\": manager_config\n",
    "        }\n",
    "env = HRL(hrl_config)\n",
    " \n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    " \n",
    "\n",
    "\n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\"\n",
    ")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    "\n",
    "\n",
    "def explore(config):\n",
    "    # Ensure we collect enough timesteps to do sgd\n",
    "    if config[\"train_batch_size\"] < config[\"rollout_fragment_length\"] * 2:\n",
    "        config[\"train_batch_size\"] = config[\"rollout_fragment_length\"] * 2\n",
    "    return config\n",
    "\n",
    "hyperparam_mutations = {\n",
    "    \"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "    \"gamma\": lambda: random.uniform(0.9, 1.0),\n",
    "    \"entropy_coeff\": [0.01, 0.1, 1.0],\n",
    "    \"num_envs_per_worker\": [1, 2, 4, 8],\n",
    "    #\"rollout_fragment_length\": [50, 100, 200, 400],\n",
    "    \"train_batch_size\": lambda: random.randint(200, 1500),\n",
    "    \"sgd_minibatch_size\": tune.choice([50, 100, 200]),\n",
    "\n",
    "}\n",
    "\n",
    "pbt = PopulationBasedTraining(\n",
    "        time_attr=\"time_total_s\",\n",
    "        perturbation_interval=120,\n",
    "        resample_probability=0.25,\n",
    "        # Specifies the mutations of these hyperparams\n",
    "        hyperparam_mutations=hyperparam_mutations,\n",
    "        custom_explore_fn=explore,\n",
    "    )\n",
    "\n",
    "# Stop when we've reached 100 training iterations or reward=300\n",
    "stopping_criteria = {\"training_iteration\": 100}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"A2C\",\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        scheduler=pbt,\n",
    "        num_samples=1 if args.smoke_test else 10,\n",
    "    ),\n",
    "    param_space={\n",
    "        \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "        \"env_config\": hrl_config,\n",
    "        \"rollout_fragment_length\": \"auto\",\n",
    "        \"framework\": \"tf2\",\n",
    "        \"num_workers\": 1,  # 1 for training + 4 for sampling\n",
    "        \"num_cpus_per_trial\": 3,\n",
    "        # \"num_cpus\": 1,  # number of CPUs to use per trial --> 6 in total = max available\n",
    "        # \"num_gpus\": 0,  # number of GPUs to use per trial\n",
    "        # These params are tuned from a fixed starting value.\n",
    "        \"lr\": 1e-4,\n",
    "        # These params start off randomly drawn from a set.\n",
    "        \"sgd_minibatch_size\": tune.choice([50, 100, 200]),\n",
    "        \"train_batch_size\": tune.choice([200, 400, 600]),\n",
    "    },\n",
    "\n",
    "    run_config=air.RunConfig(stop=stopping_criteria, local_dir=\"/Users/floriankockler/rayresults/autobatch\", progress_reporter=reporter),\n",
    ")\n",
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
