{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray, random, os \n",
    "ray.shutdown()# Set the random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 21:33:20,745\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import ray, random, os, sys\n",
    "from ray.air.config import ScalingConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# ray.init(logging_level=\"ERROR\",ignore_reinit_error=True, _temp_dir='/Users/floriankockler/raytemp/')\n",
    "from ray import air, tune\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "import argparse\n",
    "from ray.tune import CLIReporter\n",
    "from worker_standlone import WorkerStandAlone\n",
    "from multi_agent import MultiAgent\n",
    "from manager import Manager\n",
    "from ray.train.rl import RLTrainer\n",
    "from ray.rllib.policy.policy import Policy, PolicySpec\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune import TuneConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "from ray.air.config import RunConfig, CheckpointConfig\n",
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.rllib.utils import check_env\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "from ray.rllib.evaluation.episode import Episode\n",
    "from ray.rllib.evaluation import RolloutWorker\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.logger import TBXLoggerCallback\n",
    "from typing import Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.utils import check_env\n",
    "check_env(MultiAgent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "multi_agent_env = MultiAgent()\n",
    "\n",
    "# Initialize environment\n",
    "obs, info = multi_agent_env.reset()\n",
    "\n",
    "# Loop for a maximum of 6310 steps or until done\n",
    "for step in range(5):\n",
    "    actions = {}\n",
    "    \n",
    "    # Collect sample actions for each agent\n",
    "    for agent_id, agent_obs in obs.items():\n",
    "        action_space = multi_agent_env.action_space[agent_id]\n",
    "        action = action_space.sample()\n",
    "        actions[agent_id] = action\n",
    "    \n",
    "    # Step the environment\n",
    "    obs, reward, done, _, info = multi_agent_env.step(actions)\n",
    "    \n",
    "    # Check if any agent is done and reset the environment if so\n",
    "    if done[\"__all__\"]:\n",
    "        obs, info = multi_agent_env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_agent_env = MultiAgent()\n",
    "\n",
    "\n",
    "for episode in range(0):\n",
    "    obs, info = multi_agent_env.reset()\n",
    "    done = {\"__all__\": False}\n",
    "    \n",
    "    print(f\"Episode {episode + 1}\")\n",
    "\n",
    "    while not done[\"__all__\"]:\n",
    "        actions = {}\n",
    "        \n",
    "        # Collect actions for each agent\n",
    "        for agent_id, agent_obs in obs.items():\n",
    "            action_space = multi_agent_env.action_space[agent_id]\n",
    "            action = action_space.sample()\n",
    "            actions[agent_id] = action\n",
    "\n",
    "        # Step the environment\n",
    "        obs, reward, done, _, info = multi_agent_env.step(actions)\n",
    "\n",
    "        # print(f\"Actions: {actions}\")\n",
    "        # print(f\"Observations: {obs}\")\n",
    "        # print(f\"Rewards: {reward}\")\n",
    "        # print(f\"Done flags: {done}\")\n",
    "        # print(f\"Info: {info}\")\n",
    "\n",
    "    print(\"Episode done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.evaluation import Episode, RolloutWorker\n",
    "from ray.rllib.policy import Policy\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.algorithms.pg.pg import PGConfig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyCallbacks5(DefaultCallbacks):\n",
    "\n",
    "    def setup(self, **info):\n",
    "        pass  # Add any setup logic here if needed\n",
    "    def get_state(self, **info):\n",
    "        pass  # Add any setup logic here if needed\n",
    "\n",
    "    def on_step_begin(self, **info):\n",
    "        # print(\"info: \",info)\n",
    "        # info['result'].setdefault('custom_metrics', {})\n",
    "        # info['result']['custom_metrics']['performance'] = 10\n",
    "        # print(\"Callback5 step begin\")\n",
    "        pass  # Add logic here if needed\n",
    "    def on_trial_save(self, **info):\n",
    "        pass  # Add logic here if needed\n",
    "    \n",
    "    def on_checkpoint(self, **info):\n",
    "        pass  # Add logic here if needed\n",
    "    \n",
    "    def on_step_end(self, **info):\n",
    "        print(\"info step end\",info)\n",
    "        # print(\"Callback5 step end\")\n",
    "        pass  # Add logic here if needed\n",
    "    def on_trial_start(self, **info):\n",
    "        print(\"Callback5 ontrial start\")\n",
    "        pass  # Add logic here if needed\n",
    "    def on_trial_result(self, **info):\n",
    "        print(\"Callback5 ontrial result\")\n",
    "        pass  # Add logic here if needed\n",
    "    \n",
    "    def on_experiment_end(self, **info):\n",
    "        pass  # Add logic here if needed\n",
    "    \n",
    "    def on_episode_start(\n",
    "        self,\n",
    "        *,\n",
    "        worker: RolloutWorker,\n",
    "        base_env: BaseEnv,\n",
    "        policies: Dict[str, Policy],\n",
    "        episode: Episode,\n",
    "        env_index: int,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # Make sure this episode has just been started (only initial obs\n",
    "        # logged so far).\n",
    "        print(\"Callback5 episode start\")\n",
    "        assert episode.length == 0, (\n",
    "            \"ERROR: `on_episode_start()` callback should be called right \"\n",
    "            \"after env reset!\"\n",
    "        )\n",
    "        # Create lists to store angles in\n",
    "        episode.user_data[\"pole_angles\"] = []\n",
    "        episode.hist_data[\"pole_angles\"] = []\n",
    "\n",
    "    def on_episode_step(\n",
    "        self,\n",
    "        *,\n",
    "        worker: RolloutWorker,\n",
    "        base_env: BaseEnv,\n",
    "        policies: Dict[str, Policy],\n",
    "        episode: Episode,\n",
    "        env_index: int,\n",
    "        **kwargs\n",
    "    ):\n",
    "        print(\"callback5 on episode step\")\n",
    "        # Make sure this episode is ongoing.\n",
    "        # assert episode.length > 0, (\n",
    "        #     \"ERROR: `on_episode_step()` callback should not be called right \"\n",
    "        #     \"after env reset!\"\n",
    "        # )\n",
    "        # pole_angle = abs(episode.last_observation_for()[2])\n",
    "        # raw_angle = abs(episode.last_raw_obs_for()[2])\n",
    "        # assert pole_angle == raw_angle\n",
    "        # episode.user_data[\"pole_angles\"].append(pole_angle)\n",
    "\n",
    "        # # Sometimes our pole is moving fast. We can look at the latest velocity\n",
    "        # # estimate from our environment and log high velocities.\n",
    "        # if np.abs(episode.last_info_for()[\"pole_angle_vel\"]) > 0.25:\n",
    "        #     print(\"This is a fast pole!\")\n",
    "\n",
    "    def on_episode_end(\n",
    "        self,\n",
    "        *,\n",
    "        worker: RolloutWorker,\n",
    "        base_env: BaseEnv,\n",
    "        policies: Dict[str, Policy],\n",
    "        episode: Episode,\n",
    "        env_index: int,\n",
    "        **kwargs\n",
    "    ):\n",
    "        print(\"callback5 on episode end\")\n",
    "        # Check if there are multiple episodes in a batch, i.e.\n",
    "        # \"batch_mode\": \"truncate_episodes\".\n",
    "        # if worker.config.batch_mode == \"truncate_episodes\":\n",
    "        #     # Make sure this episode is really done.\n",
    "        #     assert episode.batch_builder.policy_collectors[\"default_policy\"].batches[\n",
    "        #         -1\n",
    "        #     ][\"dones\"][-1], (\n",
    "        #         \"ERROR: `on_episode_end()` should only be called \"\n",
    "        #         \"after episode is done!\"\n",
    "        #     )\n",
    "        # pole_angle = np.mean(episode.user_data[\"pole_angles\"])\n",
    "        # episode.custom_metrics[\"pole_angle\"] = pole_angle\n",
    "        # episode.hist_data[\"pole_angles\"] = episode.user_data[\"pole_angles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 21:36:40,974\tINFO tune.py:657 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/tune/tune.py:258: UserWarning: Passing a `local_dir` is deprecated and will be removed in the future. Pass `storage_path` instead or set the `RAY_AIR_LOCAL_CACHE_DIR` environment variable instead.\n",
      "  warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=31376)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=31376)\u001b[0m 2023-10-17 21:36:48,784\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a2c/` has been deprecated. Use `rllib_contrib/a2c/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=31376)\u001b[0m 2023-10-17 21:36:48,784\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a3c/` has been deprecated. Use `rllib_contrib/a3c/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=31376)\u001b[0m 2023-10-17 21:36:48,784\tWARNING algorithm_config.py:656 -- Cannot create A2CConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=31394)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=31393)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=31376)\u001b[0m Trainable.setup took 12.039 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics                                                                                                                                              </th><th>counters                                                                                                                              </th><th>custom_metrics  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_sampled_throughput_per_sec</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained_throughput_per_sec</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                           </th><th>policy_reward_max                                                      </th><th>policy_reward_mean                                                     </th><th>policy_reward_min                                                      </th><th>sampler_perf                                                                                                                                                                                                  </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </th><th>timers                                                                                                                                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>A2C_MultiAgent_7ede5_00000</td><td style=\"text-align: right;\">                 180096</td><td>{&#x27;ObsPreprocessorConnector_ms&#x27;: 0.02167622248331706, &#x27;StateBufferConnector_ms&#x27;: 0.0023265679677327475, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.06289084752400716}</td><td>{&#x27;num_env_steps_sampled&#x27;: 45024, &#x27;num_env_steps_trained&#x27;: 45024, &#x27;num_agent_steps_sampled&#x27;: 180096, &#x27;num_agent_steps_trained&#x27;: 180096}</td><td>{}              </td><td style=\"text-align: right;\">              6305</td><td>{}             </td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   1</td><td>{&#x27;learner&#x27;: {&#x27;controller_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.7546763046993874e-05, &#x27;entropy_coeff&#x27;: 0.014056925661861897, &#x27;policy_loss&#x27;: -0.024278514, &#x27;policy_entropy&#x27;: 812.9315, &#x27;var_gnorm&#x27;: 23.198652, &#x27;vf_loss&#x27;: 2.4091836e-08, &#x27;model&#x27;: {}, &#x27;grad_gnorm&#x27;: 12.613525, &#x27;vf_explained_var&#x27;: -1.0}}, &#x27;ABT.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.7546763046993874e-05, &#x27;entropy_coeff&#x27;: 0.014056925661861897, &#x27;policy_loss&#x27;: -0.0006072004, &#x27;policy_entropy&#x27;: 301.7782, &#x27;var_gnorm&#x27;: 23.038738, &#x27;vf_loss&#x27;: 1.573667e-09, &#x27;model&#x27;: {}, &#x27;grad_gnorm&#x27;: 7.2092333, &#x27;vf_explained_var&#x27;: -1.0}}, &#x27;MRK.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.7546763046993874e-05, &#x27;entropy_coeff&#x27;: 0.014056925661861897, &#x27;policy_loss&#x27;: 0.0032201838, &#x27;policy_entropy&#x27;: 302.3126, &#x27;var_gnorm&#x27;: 22.996954, &#x27;vf_loss&#x27;: 6.022748e-09, &#x27;model&#x27;: {}, &#x27;grad_gnorm&#x27;: 7.189658, &#x27;vf_explained_var&#x27;: 0.5856692}}, &#x27;PFE.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 1.7546763046993874e-05, &#x27;entropy_coeff&#x27;: 0.014056925661861897, &#x27;policy_loss&#x27;: -0.0006262958, &#x27;policy_entropy&#x27;: 299.7364, &#x27;var_gnorm&#x27;: 23.00664, &#x27;vf_loss&#x27;: 2.137931e-10, &#x27;model&#x27;: {}, &#x27;grad_gnorm&#x27;: 7.2020454, &#x27;vf_explained_var&#x27;: -1.0}}}, &#x27;num_env_steps_sampled&#x27;: 45024, &#x27;num_env_steps_trained&#x27;: 45024, &#x27;num_agent_steps_sampled&#x27;: 180096, &#x27;num_agent_steps_trained&#x27;: 180096}</td><td style=\"text-align: right;\">                   180096</td><td style=\"text-align: right;\">                   180096</td><td style=\"text-align: right;\">                  45024</td><td style=\"text-align: right;\">                             5472</td><td style=\"text-align: right;\">                                   541.989</td><td style=\"text-align: right;\">                  45024</td><td style=\"text-align: right;\">                             5472</td><td style=\"text-align: right;\">                                   541.989</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    3</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                         5472</td><td>{&#x27;cpu_util_percent&#x27;: 58.850000000000016, &#x27;ram_util_percent&#x27;: 84.36428571428573}</td><td>{&#x27;controller_policy&#x27;: 0.0, &#x27;ABT.US&#x27;: 0.0, &#x27;MRK.US&#x27;: 0.0, &#x27;PFE.US&#x27;: 0.0}</td><td>{&#x27;controller_policy&#x27;: 0.0, &#x27;ABT.US&#x27;: 0.0, &#x27;MRK.US&#x27;: 0.0, &#x27;PFE.US&#x27;: 0.0}</td><td>{&#x27;controller_policy&#x27;: 0.0, &#x27;ABT.US&#x27;: 0.0, &#x27;MRK.US&#x27;: 0.0, &#x27;PFE.US&#x27;: 0.0}</td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.9542613985494288, &#x27;mean_inference_ms&#x27;: 1.845284655805015, &#x27;mean_action_processing_ms&#x27;: 0.5604967839826883, &#x27;mean_env_wait_ms&#x27;: 0.8373553436233001, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 0.0, &#x27;episode_reward_min&#x27;: 0.0, &#x27;episode_reward_mean&#x27;: 0.0, &#x27;episode_len_mean&#x27;: 6305.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 1, &#x27;policy_reward_min&#x27;: {&#x27;controller_policy&#x27;: 0.0, &#x27;ABT.US&#x27;: 0.0, &#x27;MRK.US&#x27;: 0.0, &#x27;PFE.US&#x27;: 0.0}, &#x27;policy_reward_max&#x27;: {&#x27;controller_policy&#x27;: 0.0, &#x27;ABT.US&#x27;: 0.0, &#x27;MRK.US&#x27;: 0.0, &#x27;PFE.US&#x27;: 0.0}, &#x27;policy_reward_mean&#x27;: {&#x27;controller_policy&#x27;: 0.0, &#x27;ABT.US&#x27;: 0.0, &#x27;MRK.US&#x27;: 0.0, &#x27;PFE.US&#x27;: 0.0}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], &#x27;episode_lengths&#x27;: [6305, 6305, 6305, 6305, 6305, 6305], &#x27;policy_controller_policy_reward&#x27;: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], &#x27;policy_ABT.US_reward&#x27;: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], &#x27;policy_MRK.US_reward&#x27;: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], &#x27;policy_PFE.US_reward&#x27;: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.9542613985494288, &#x27;mean_inference_ms&#x27;: 1.845284655805015, &#x27;mean_action_processing_ms&#x27;: 0.5604967839826883, &#x27;mean_env_wait_ms&#x27;: 0.8373553436233001, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.02167622248331706, &#x27;StateBufferConnector_ms&#x27;: 0.0023265679677327475, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.06289084752400716}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 88.328, &#x27;sample_time_ms&#x27;: 72.654, &#x27;learn_time_ms&#x27;: 8.528, &#x27;learn_throughput&#x27;: 3752.169, &#x27;synch_weights_time_ms&#x27;: 7.069}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if os.name == 'nt':\n",
    "    path_to_save = \"C:\\\\GitHub\\\\ray\\\\workertune\"\n",
    "else:\n",
    "    # path_to_save = \"/Volumes/ssdmac/ray/4\"\n",
    "    path_to_save = \"/Users/floriankockler/raytemp/3/\"\n",
    "\n",
    "log_file = open(\"console_logs_ma4.txt\", \"w\")\n",
    "sys.stdout = log_file\n",
    "\n",
    "from ray.rllib.env import BaseEnv\n",
    "\n",
    "from ray.tune.logger import LoggerCallback\n",
    "\n",
    "\n",
    "class TestLoggerCallback(LoggerCallback):\n",
    "    def on_trial_result(self, iteration, trials, trial, result, **info):\n",
    "        print(f\"TestLogger for trial {trial}: {result}\")\n",
    "\n",
    "class MyCustomCallback(TBXLoggerCallback):\n",
    "    def on_episode_end(self, *, worker, episode, **kwargs):\n",
    "        # portfolio_return = episode.last_info_for()['portfolio_return']\n",
    "        portfolio_return = episode.last_info_for()\n",
    "        print(\"LOGGING of INFO:\", portfolio_return)\n",
    "        episode.custom_metrics[\"portfolio_return\"] = portfolio_return\n",
    "\n",
    "class MyCustomCallback2(TBXLoggerCallback):\n",
    "    def on_episode_step(self, *, worker, episode, **kwargs):\n",
    "        agent_id = \"controller\"  # Replace with the actual agent ID\n",
    "        portfolio_return = episode.last_info_for(agent_id).get('portfolio_return', 0)\n",
    "        print(\"custom Logger:\",portfolio_return)\n",
    "        episode.custom_metrics[f\"{agent_id}_portfolio_return\"] = portfolio_return\n",
    "\n",
    "class MyCallback3(DefaultCallbacks):\n",
    "    def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                       policies: Dict[str, Policy], episode: Episode,\n",
    "                       **kwargs):      \n",
    "        print(\"INSIDE CALLBACK: \", episode._agent_to_last_info)\n",
    "\n",
    "\n",
    "env = MultiAgent()\n",
    "def env_creator(env_config):\n",
    "    return MultiAgent()  \n",
    "\n",
    "register_env(\"MultiAgent\", env_creator)\n",
    "\n",
    "asha_scheduler = ASHAScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    max_t=300,\n",
    "    grace_period=50,\n",
    "    reduction_factor=3,\n",
    "    brackets=1,\n",
    ")\n",
    "\n",
    "def create_policy_spec(agent_id):\n",
    "    print(f\"Creating policy for {agent_id}\")\n",
    "    return PolicySpec(\n",
    "        observation_space=env.observation_space[agent_id],\n",
    "        action_space=env.action_space[agent_id],\n",
    "        config={}\n",
    "    )\n",
    "\n",
    "controller_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['controller'],\n",
    "    action_space=env.action_space['controller'],\n",
    "    config={}\n",
    ")\n",
    "\n",
    "policies = {\n",
    "    \"controller_policy\": controller_policy_spec,\n",
    "}\n",
    "\n",
    "for agent_id in env.agents:\n",
    "    policies[agent_id] = create_policy_spec(agent_id)\n",
    "\n",
    "class MyCallback4(DefaultCallbacks):\n",
    "    def on_episode_end(self, *, worker, base_env, policies, episode, **kwargs):\n",
    "        print(\"INSIDE CALLBACK: \", episode._agent_to_last_info)\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode=None, agent=None, **kwargs):\n",
    "    if agent_id == 'controller':\n",
    "        # print(f\"!!!!!! policy mapping manager: {agent_id}\")\n",
    "        return \"controller_policy\"\n",
    "    elif agent_id in env.agents:\n",
    "        return agent_id\n",
    "    else:\n",
    "        print(\"defaul policy triggered\")\n",
    "        return \"default_policy\"\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"MultiAgent\",\n",
    "    \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "        \"rollout_fragment_length\": \"auto\",\n",
    "        \"framework\": \"tf\",\n",
    "        \"lr\": tune.uniform(1e-5,1e-4),\n",
    "        \"gamma\": tune.uniform(0.95, 0.9999),\n",
    "        \"lambda\": tune.uniform(0.9,1.0),\n",
    "        \"entropy_coeff\": tune.uniform(0.01,0.1),\n",
    "        \"vf_loss_coeff\": tune.uniform(0.1,0.3),\n",
    "        \"num_workers\": 3, \n",
    "        # Change for Debugging\n",
    "        \"log_level\": \"ERROR\",\n",
    "        \"output\": \"logdir\",\n",
    "        \"monitor\": True,\n",
    "}\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"A2C\", \n",
    "    metric=\"episode_reward_mean\",\n",
    "    # metric=\"portfolio_return\",\n",
    "    num_samples=10,\n",
    "    resume=False,\n",
    "    callbacks=[MyCallbacks5()],\n",
    "    mode=\"max\",\n",
    "    config=param_space, \n",
    "    local_dir=path_to_save,\n",
    "    search_alg=None,\n",
    "    scheduler=asha_scheduler,\n",
    "    progress_reporter=CLIReporter(max_progress_rows=10,max_report_frequency=300),\n",
    "    max_concurrent_trials=1,\n",
    "    #checkpoint_config not checked yet\n",
    "    checkpoint_config={\n",
    "        \"num_to_keep\": 1,\n",
    "        \"checkpoint_score_attribute\": \"episode_reward_mean\",\n",
    "        \"checkpoint_score_order\": \"max\",\n",
    "        \"checkpoint_frequency\": 10\n",
    "    }\n",
    "    )\n",
    "\n",
    "print(pretty_print(analysis.last_result))\n",
    "print(\"Best hyperparameters found were: \", pretty_print(analysis.best_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WorkerStandAlone()\n",
    "def env_creator(env_config):\n",
    "    return WorkerStandAlone()  \n",
    "\n",
    "register_env(\"worker\", env_creator)\n",
    "\n",
    "\n",
    "\n",
    "analysis = tune.run(\n",
    "        \"A2C\",\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        config={\n",
    "            \"env\": \"worker\",\n",
    "            \"env_config\": {\"initial_capital\": 1e6},\n",
    "            \"lr\": tune.uniform(1e-5, 1e-4),\n",
    "            \"train_batch_size\": tune.choice([10000, 20000, 40000]),\n",
    "}, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_iterations = 30000\n",
    "\n",
    "\n",
    "env = WorkerStandAlone(data=data)\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    action = env.action_space.sample()  # Generate random actions for the manager\n",
    "    worker_dones = {...}  # You need to provide the values for worker_dones\n",
    "    worker_truncateds = {...}  # You need to provide the values for worker_truncateds\n",
    "\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        print(\"Episode finished!\")\n",
    "        state = env.reset()\n",
    "    else:\n",
    "        state = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = HRL()\n",
    "\n",
    "n_iterations = 1\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    obs, reward, done, truncated, info= env.step(action)\n",
    "\n",
    "    # print(f\"Action: {action}, Reward: {reward}, Portfolio Value: {obs[0] + obs[1] * obs[2]}\")\n",
    "    \n",
    "    if done[\"__all__\"]:\n",
    "        print(\"Episode finished!\")\n",
    "        state = env.reset()\n",
    "    else:\n",
    "        state = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    "    \n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    "\n",
    "    \n",
    "algo = PPOConfig().environment(env=HRL).multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn = policy_mapping_fn,  \n",
    "    ).training(train_batch_size=4000).build()\n",
    "\n",
    "print(algo.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HRL()\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  \n",
    "\n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    "    \n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    " \n",
    "search_space = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),  # Learning rate\n",
    "    \"gamma\": tune.uniform(0.9, 0.99),  # Discount factor\n",
    "}\n",
    "\n",
    "tune_config = TuneConfig(\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    max_concurrent_trials=1,\n",
    "    num_samples=100,\n",
    "    search_alg=BayesOptSearch(\n",
    "          search_space,\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        random_search_steps=4\n",
    "    ),\n",
    "    scheduler=HyperBandScheduler()\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    name=\"MyExperiment\",\n",
    "    storage_path=\"/Users/floriankockler/rayresults/tuner_trial\",\n",
    "    verbose=2,\n",
    "    # checkpoint_config=air.CheckpointConfig(checkpoint_frequency=2),\n",
    "    callbacks=None,\n",
    "    stop=None,\n",
    "    failure_config=None,\n",
    "    sync_config=None,\n",
    "    checkpoint_config=None,\n",
    "    progress_reporter=CLIReporter(max_progress_rows=10),# Define the policy mapping function\n",
    "\n",
    ")\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "          \"rollout_fragment_length\": \"auto\",\n",
    "          \"lr\": tune.uniform(1e-5,1e-4),\n",
    "        \"num_workers\": 1,  \n",
    "        \"num_cpus_per_trial\": 1,\n",
    "}\n",
    "\n",
    "analysis = tune.run(\"A2C\", metric=\"episode_reward_mean\", mode=\"max\",config=param_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "algo = PPOConfig().environment(env=HRL).multi_agent(\n",
    "    policies={\n",
    "        \"policy_1\": ()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune import CLIReporter\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  # Assuming this is your environment\n",
    " \n",
    "env = HRL()\n",
    "\n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    "    \n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    " \n",
    "search_space = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),  # Learning rate\n",
    "    \"gamma\": tune.uniform(0.9, 0.99),  # Discount factor\n",
    "}\n",
    "\n",
    "tune_config = TuneConfig(\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    max_concurrent_trials=1,\n",
    "    num_samples=100,\n",
    "    search_alg=BayesOptSearch(\n",
    "          search_space,\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        random_search_steps=4\n",
    "    ),\n",
    "    scheduler=HyperBandScheduler()\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    name=\"MyExperiment\",\n",
    "    storage_path=\"/Users/floriankockler/rayresults/tuner_trial\",\n",
    "    verbose=2,\n",
    "    # checkpoint_config=air.CheckpointConfig(checkpoint_frequency=2),\n",
    "    callbacks=None,\n",
    "    stop=None,\n",
    "    failure_config=None,\n",
    "    sync_config=None,\n",
    "    checkpoint_config=None,\n",
    "    progress_reporter=CLIReporter(max_progress_rows=10),# Define the policy mapping function\n",
    "\n",
    ")\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "          \"rollout_fragment_length\": \"auto\",\n",
    "        \"num_workers\": 1,  \n",
    "        \"num_cpus_per_trial\": 1,\n",
    "}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"A2C\",\n",
    "    tune_config=tune_config,\n",
    "    param_space=param_space,\n",
    "\n",
    "    run_config=run_config,\n",
    ")\n",
    "results = tuner.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from ray import air, tune\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "import argparse\n",
    "from ray.tune import CLIReporter\n",
    "from env.multi_agent.hrl import HRL\n",
    "\n",
    "reporter = CLIReporter(max_progress_rows=10)\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  # Assuming this is your environment\n",
    " \n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "manager_config = {\n",
    "    \"df\": train_df,\n",
    "\n",
    "}\n",
    "hrl_config={\n",
    "        \"manager_config\": manager_config\n",
    "        }\n",
    "env = HRL(hrl_config)\n",
    " \n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    " \n",
    "\n",
    "\n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\"\n",
    ")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    "\n",
    "\n",
    "def explore(config):\n",
    "    # Ensure we collect enough timesteps to do sgd\n",
    "    if config[\"train_batch_size\"] < config[\"rollout_fragment_length\"] * 2:\n",
    "        config[\"train_batch_size\"] = config[\"rollout_fragment_length\"] * 2\n",
    "    return config\n",
    "\n",
    "hyperparam_mutations = {\n",
    "    \"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "    \"gamma\": lambda: random.uniform(0.9, 1.0),\n",
    "    \"entropy_coeff\": [0.01, 0.1, 1.0],\n",
    "    \"num_envs_per_worker\": [1, 2, 4, 8],\n",
    "    #\"rollout_fragment_length\": [50, 100, 200, 400],\n",
    "    \"train_batch_size\": lambda: random.randint(200, 1500),\n",
    "    \"sgd_minibatch_size\": tune.choice([50, 100, 200]),\n",
    "\n",
    "}\n",
    "\n",
    "pbt = PopulationBasedTraining(\n",
    "        time_attr=\"time_total_s\",\n",
    "        perturbation_interval=120,\n",
    "        resample_probability=0.25,\n",
    "        # Specifies the mutations of these hyperparams\n",
    "        hyperparam_mutations=hyperparam_mutations,\n",
    "        custom_explore_fn=explore,\n",
    "    )\n",
    "\n",
    "# Stop when we've reached 100 training iterations or reward=300\n",
    "stopping_criteria = {\"training_iteration\": 100}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"A2C\",\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        scheduler=pbt,\n",
    "        num_samples=1 if args.smoke_test else 10,\n",
    "    ),\n",
    "    param_space={\n",
    "        \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "        \"env_config\": hrl_config,\n",
    "        \"rollout_fragment_length\": \"auto\",\n",
    "        \"framework\": \"tf2\",\n",
    "        \"num_workers\": 1,  # 1 for training + 4 for sampling\n",
    "        \"num_cpus_per_trial\": 3,\n",
    "        # \"num_cpus\": 1,  # number of CPUs to use per trial --> 6 in total = max available\n",
    "        # \"num_gpus\": 0,  # number of GPUs to use per trial\n",
    "        # These params are tuned from a fixed starting value.\n",
    "        \"lr\": 1e-4,\n",
    "        # These params start off randomly drawn from a set.\n",
    "        \"sgd_minibatch_size\": tune.choice([50, 100, 200]),\n",
    "        \"train_batch_size\": tune.choice([200, 400, 600]),\n",
    "    },\n",
    "\n",
    "    run_config=air.RunConfig(stop=stopping_criteria, local_dir=\"/Users/floriankockler/rayresults/autobatch\", progress_reporter=reporter),\n",
    ")\n",
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
