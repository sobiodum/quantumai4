{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 15:08:32,811\tINFO worker.py:1612 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "2023-09-29 15:08:38,335\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import ray, random, os \n",
    "from ray.air.config import ScalingConfig\n",
    "import pandas as pd\n",
    "ray.init(_temp_dir='/Volumes/SSD980/ray')\n",
    "from ray import air, tune\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "import argparse\n",
    "from ray.tune import CLIReporter\n",
    "from worker_standlone import WorkerStandAlone\n",
    "from multi_agent import MultiAgent\n",
    "from manager import Manager\n",
    "from ray.train.rl import RLTrainer\n",
    "from ray.rllib.policy.policy import Policy, PolicySpec\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune import TuneConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "from ray.air.config import RunConfig, CheckpointConfig\n",
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.rllib.utils import check_env\n",
    "from ray.tune.logger import pretty_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.utils import check_env\n",
    "check_env(MultiAgent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 15:08:39,379\tINFO tune.py:657 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/ray/tune/tune.py:258: UserWarning: Passing a `local_dir` is deprecated and will be removed in the future. Pass `storage_path` instead or set the `RAY_AIR_LOCAL_CACHE_DIR` environment variable instead.\n",
      "  warnings.warn(\n",
      "2023-09-29 15:08:39,406\tWARNING deprecation.py:50 -- DeprecationWarning: `build_tf_policy` has been deprecated. This will raise an error in the future!\n",
      "2023-09-29 15:08:39,408\tWARNING deprecation.py:50 -- DeprecationWarning: `build_policy_class` has been deprecated. This will raise an error in the future!\n",
      "2023-09-29 15:08:39,438\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/floriankockler/anaconda3/envs/py310/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "2023-09-29 15:08:39,477\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-09-29 15:08:40 (running for 00:00:00.75)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 1800.000: None | Iter 600.000: None | Iter 200.000: None\n",
      "Logical resource usage: 0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /Volumes/SSD980/ray/multiagent2/A2C\n",
      "Number of trials: 2/10 (2 PENDING)\n",
      "+----------------------------+----------+-------+-----------------+----------+----------+-------------+-----------------+\n",
      "| Trial name                 | status   | loc   |   entropy_coeff |    gamma |   lambda |          lr |   vf_loss_coeff |\n",
      "|----------------------------+----------+-------+-----------------+----------+----------+-------------+-----------------|\n",
      "| A2C_MultiAgent_4e970_00000 | PENDING  |       |       0.0455513 | 0.992995 | 0.994113 | 8.77166e-05 |        0.194472 |\n",
      "| A2C_MultiAgent_4e970_00001 | PENDING  |       |       0.0403279 | 0.971596 | 0.905525 | 6.33856e-05 |        0.245074 |\n",
      "+----------------------------+----------+-------+-----------------+----------+----------+-------------+-----------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=47403)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=47403)\u001b[0m 2023-09-29 15:08:46,033\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a2c/` has been deprecated. Use `rllib_contrib/a2c/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=47403)\u001b[0m 2023-09-29 15:08:46,033\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a3c/` has been deprecated. Use `rllib_contrib/a3c/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(A2C pid=47403)\u001b[0m 2023-09-29 15:08:46,033\tWARNING algorithm_config.py:656 -- Cannot create A2CConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=47412)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=47402)\u001b[0m 2023-09-29 15:08:46,033\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a3c/` has been deprecated. Use `rllib_contrib/a3c/` instead. This will raise an error in the future!\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(A2C pid=47402)\u001b[0m 2023-09-29 15:08:46,033\tWARNING algorithm_config.py:656 -- Cannot create A2CConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(A2C pid=47403)\u001b[0m Trainable.setup took 13.968 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=47415)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47416)\u001b[0m WARNING:tensorflow:5 out of the last 5 calls to <bound method with_lock.<locals>.wrapper of A3CTF2Policy_traced> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47416)\u001b[0m WARNING:tensorflow:5 out of the last 5 calls to <bound method with_lock.<locals>.wrapper of A3CTF2Policy_traced> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[2m\u001b[36m(A2C pid=47403)\u001b[0m WARNING:tensorflow:5 out of the last 5 calls to <bound method EagerTFPolicyV2._learn_on_batch_helper of A3CTF2Policy_traced> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[2m\u001b[36m(A2C pid=47403)\u001b[0m WARNING:tensorflow:5 out of the last 5 calls to <bound method EagerTFPolicyV2._learn_on_batch_helper of A3CTF2Policy_traced> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[2m\u001b[36m(A2C pid=47402)\u001b[0m Trainable.setup took 13.911 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=47415)\u001b[0m WARNING:tensorflow:6 out of the last 6 calls to <bound method with_lock.<locals>.wrapper of A3CTF2Policy_traced> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\u001b[32m [repeated 22x across cluster]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics  </th><th>counters                                                                                                                              </th><th>custom_metrics  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_sampled_throughput_per_sec</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained_throughput_per_sec</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                </th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf  </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                </th><th>timers                                                                                                                                                     </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>A2C_MultiAgent_4e970_00000</td><td style=\"text-align: right;\">                 209216</td><td>{}                 </td><td>{&#x27;num_env_steps_sampled&#x27;: 14944, &#x27;num_env_steps_trained&#x27;: 14944, &#x27;num_agent_steps_sampled&#x27;: 209216, &#x27;num_agent_steps_trained&#x27;: 209216}</td><td>{}              </td><td style=\"text-align: right;\">               nan</td><td>{}             </td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                  nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                   0</td><td>{&#x27;learner&#x27;: {&#x27;controller_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 8.771660213824362e-05, &#x27;entropy_coeff&#x27;: 0.04555129259824753, &#x27;policy_loss&#x27;: -0.0, &#x27;policy_entropy&#x27;: 35.155594, &#x27;var_gnorm&#x27;: 22.627424, &#x27;vf_loss&#x27;: 0.0}, &#x27;grad_gnorm&#x27;: 0.0, &#x27;vf_explained_var&#x27;: nan, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 467, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 466}, &#x27;ABT.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 8.771660213824362e-05, &#x27;entropy_coeff&#x27;: 0.04555129259824753, &#x27;policy_loss&#x27;: -84087.75, &#x27;policy_entropy&#x27;: 65.92951, &#x27;var_gnorm&#x27;: 22.655075, &#x27;vf_loss&#x27;: 35313948.0}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 1.7821789e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 467, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 466}, &#x27;AMGN.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 8.771660213824362e-05, &#x27;entropy_coeff&#x27;: 0.04555129259824753, &#x27;policy_loss&#x27;: 57340.15, &#x27;policy_entropy&#x27;: 78.422806, &#x27;var_gnorm&#x27;: 22.64249, &#x27;vf_loss&#x27;: 48519876.0}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -8.237362e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 467, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 466}, &#x27;BDX.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 8.771660213824362e-05, &#x27;entropy_coeff&#x27;: 0.04555129259824753, &#x27;policy_loss&#x27;: -14233.217, &#x27;policy_entropy&#x27;: 80.95892, &#x27;var_gnorm&#x27;: 22.647688, &#x27;vf_loss&#x27;: 1779893.6}, &#x27;grad_gnorm&#x27;: 39.999996, &#x27;vf_explained_var&#x27;: -0.00021350384, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 467, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 466}, &#x27;BMY.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 8.771660213824362e-05, &#x27;entropy_coeff&#x27;: 0.04555129259824753, &#x27;policy_loss&#x27;: -23090.994, &#x27;policy_entropy&#x27;: 73.29454, &#x27;var_gnorm&#x27;: 22.650208, &#x27;vf_loss&#x27;: 2024227.8}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 3.5226345e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 467, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 466}, &#x27;HUM.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 8.771660213824362e-05, &#x27;entropy_coeff&#x27;: 0.04555129259824753, &#x27;policy_loss&#x27;: 6550.0605, &#x27;policy_entropy&#x27;: 71.750946, &#x27;var_gnorm&#x27;: 22.64523, &#x27;vf_loss&#x27;: 2168264.8}, &#x27;grad_gnorm&#x27;: 39.999996, &#x27;vf_explained_var&#x27;: -3.4451485e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 467, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 466}, &#x27;JNJ.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 8.771660213824362e-05, &#x27;entropy_coeff&#x27;: 0.04555129259824753, &#x27;policy_loss&#x27;: -8103.7407, &#x27;policy_entropy&#x27;: 70.337814, &#x27;var_gnorm&#x27;: 22.653593, &#x27;vf_loss&#x27;: 3210584.0}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 5.64456e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 467, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 466}, &#x27;LLY.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 8.771660213824362e-05, &#x27;entropy_coeff&#x27;: 0.04555129259824753, &#x27;policy_loss&#x27;: -71308.81, &#x27;policy_entropy&#x27;: 88.71971, &#x27;var_gnorm&#x27;: 22.648188, &#x27;vf_loss&#x27;: 15333857.0}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 1.7285347e-06, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 467, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 466}, &#x27;MDT.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 8.771660213824362e-05, &#x27;entropy_coeff&#x27;: 0.04555129259824753, &#x27;policy_loss&#x27;: -15768.619, &#x27;policy_entropy&#x27;: 65.841644, &#x27;var_gnorm&#x27;: 22.66214, &#x27;vf_loss&#x27;: 53071404.0}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 4.7683716e-07, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 467, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 466}, &#x27;MRK.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 8.771660213824362e-05, &#x27;entropy_coeff&#x27;: 0.04555129259824753, &#x27;policy_loss&#x27;: -21369.617, &#x27;policy_entropy&#x27;: 65.1162, &#x27;var_gnorm&#x27;: 22.648367, &#x27;vf_loss&#x27;: 3386151.5}, &#x27;grad_gnorm&#x27;: 40.000004, &#x27;vf_explained_var&#x27;: 0.0005902648, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 467, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 466}, &#x27;PFE.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 8.771660213824362e-05, &#x27;entropy_coeff&#x27;: 0.04555129259824753, &#x27;policy_loss&#x27;: -46699.54, &#x27;policy_entropy&#x27;: 66.4682, &#x27;var_gnorm&#x27;: 22.658619, &#x27;vf_loss&#x27;: 13855740.0}, &#x27;grad_gnorm&#x27;: 39.999996, &#x27;vf_explained_var&#x27;: -2.7418137e-06, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 467, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 466}, &#x27;SYK.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 8.771660213824362e-05, &#x27;entropy_coeff&#x27;: 0.04555129259824753, &#x27;policy_loss&#x27;: 33038.426, &#x27;policy_entropy&#x27;: 75.792816, &#x27;var_gnorm&#x27;: 22.667772, &#x27;vf_loss&#x27;: 34307956.0}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -1.3709068e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 467, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 466}, &#x27;TMO.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 8.771660213824362e-05, &#x27;entropy_coeff&#x27;: 0.04555129259824753, &#x27;policy_loss&#x27;: -33163.703, &#x27;policy_entropy&#x27;: 87.999084, &#x27;var_gnorm&#x27;: 22.633608, &#x27;vf_loss&#x27;: 4944347.0}, &#x27;grad_gnorm&#x27;: 40.000004, &#x27;vf_explained_var&#x27;: -2.9444695e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 467, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 466}, &#x27;UNH.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 8.771660213824362e-05, &#x27;entropy_coeff&#x27;: 0.04555129259824753, &#x27;policy_loss&#x27;: -82699.11, &#x27;policy_entropy&#x27;: 75.25102, &#x27;var_gnorm&#x27;: 22.635336, &#x27;vf_loss&#x27;: 142559120.0}, &#x27;grad_gnorm&#x27;: 40.000004, &#x27;vf_explained_var&#x27;: 1.8239021e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 467, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 466}}, &#x27;num_env_steps_sampled&#x27;: 14944, &#x27;num_env_steps_trained&#x27;: 14944, &#x27;num_agent_steps_sampled&#x27;: 209216, &#x27;num_agent_steps_trained&#x27;: 209216}            </td><td style=\"text-align: right;\">                   209216</td><td style=\"text-align: right;\">                   209216</td><td style=\"text-align: right;\">                  14944</td><td style=\"text-align: right;\">                              608</td><td style=\"text-align: right;\">                                   37.8188</td><td style=\"text-align: right;\">                  14944</td><td style=\"text-align: right;\">                              608</td><td style=\"text-align: right;\">                                   37.8188</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    3</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          608</td><td>{&#x27;cpu_util_percent&#x27;: 84.525, &#x27;ram_util_percent&#x27;: 84.91875}          </td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{}            </td><td>{&#x27;episode_reward_max&#x27;: nan, &#x27;episode_reward_min&#x27;: nan, &#x27;episode_reward_mean&#x27;: nan, &#x27;episode_len_mean&#x27;: nan, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 0, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [], &#x27;episode_lengths&#x27;: []}, &#x27;sampler_perf&#x27;: {}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 1175.078, &#x27;sample_time_ms&#x27;: 1053.357, &#x27;learn_time_ms&#x27;: 96.455, &#x27;learn_throughput&#x27;: 331.762, &#x27;synch_weights_time_ms&#x27;: 24.576}</td></tr>\n",
       "<tr><td>A2C_MultiAgent_4e970_00001</td><td style=\"text-align: right;\">                 208320</td><td>{}                 </td><td>{&#x27;num_env_steps_sampled&#x27;: 14880, &#x27;num_env_steps_trained&#x27;: 14880, &#x27;num_agent_steps_sampled&#x27;: 208320, &#x27;num_agent_steps_trained&#x27;: 208320}</td><td>{}              </td><td style=\"text-align: right;\">               nan</td><td>{}             </td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                  nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                   0</td><td>{&#x27;learner&#x27;: {&#x27;controller_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 6.338561797747388e-05, &#x27;entropy_coeff&#x27;: 0.04032785817980766, &#x27;policy_loss&#x27;: -0.0, &#x27;policy_entropy&#x27;: 35.155594, &#x27;var_gnorm&#x27;: 22.627424, &#x27;vf_loss&#x27;: 0.0}, &#x27;grad_gnorm&#x27;: 0.0, &#x27;vf_explained_var&#x27;: nan, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 465, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464}, &#x27;ABT.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 6.338561797747388e-05, &#x27;entropy_coeff&#x27;: 0.04032785817980766, &#x27;policy_loss&#x27;: -6996.175, &#x27;policy_entropy&#x27;: 67.18506, &#x27;var_gnorm&#x27;: 22.667051, &#x27;vf_loss&#x27;: 4748253.0}, &#x27;grad_gnorm&#x27;: 40.000004, &#x27;vf_explained_var&#x27;: 9.536743e-07, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 465, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464}, &#x27;AMGN.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 6.338561797747388e-05, &#x27;entropy_coeff&#x27;: 0.04032785817980766, &#x27;policy_loss&#x27;: -115507.84, &#x27;policy_entropy&#x27;: 75.974434, &#x27;var_gnorm&#x27;: 22.64296, &#x27;vf_loss&#x27;: 105987040.0}, &#x27;grad_gnorm&#x27;: 40.000008, &#x27;vf_explained_var&#x27;: 3.027916e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 465, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464}, &#x27;BDX.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 6.338561797747388e-05, &#x27;entropy_coeff&#x27;: 0.04032785817980766, &#x27;policy_loss&#x27;: 10075.539, &#x27;policy_entropy&#x27;: 82.44299, &#x27;var_gnorm&#x27;: 22.65627, &#x27;vf_loss&#x27;: 2173541.5}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -5.364418e-06, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 465, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464}, &#x27;BMY.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 6.338561797747388e-05, &#x27;entropy_coeff&#x27;: 0.04032785817980766, &#x27;policy_loss&#x27;: -3044.5532, &#x27;policy_entropy&#x27;: 89.71817, &#x27;var_gnorm&#x27;: 22.643326, &#x27;vf_loss&#x27;: 294646.94}, &#x27;grad_gnorm&#x27;: 40.000004, &#x27;vf_explained_var&#x27;: -4.9710274e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 465, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464}, &#x27;HUM.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 6.338561797747388e-05, &#x27;entropy_coeff&#x27;: 0.04032785817980766, &#x27;policy_loss&#x27;: -8927.56, &#x27;policy_entropy&#x27;: 76.31587, &#x27;var_gnorm&#x27;: 22.634209, &#x27;vf_loss&#x27;: 1423875.2}, &#x27;grad_gnorm&#x27;: 39.999996, &#x27;vf_explained_var&#x27;: -3.2305717e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 465, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464}, &#x27;JNJ.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 6.338561797747388e-05, &#x27;entropy_coeff&#x27;: 0.04032785817980766, &#x27;policy_loss&#x27;: -17657.773, &#x27;policy_entropy&#x27;: 77.02626, &#x27;var_gnorm&#x27;: 22.636713, &#x27;vf_loss&#x27;: 4498182.0}, &#x27;grad_gnorm&#x27;: 39.999996, &#x27;vf_explained_var&#x27;: -2.2768974e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 465, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464}, &#x27;LLY.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 6.338561797747388e-05, &#x27;entropy_coeff&#x27;: 0.04032785817980766, &#x27;policy_loss&#x27;: -14688.275, &#x27;policy_entropy&#x27;: 82.333145, &#x27;var_gnorm&#x27;: 22.635271, &#x27;vf_loss&#x27;: 3843661.8}, &#x27;grad_gnorm&#x27;: 40.000004, &#x27;vf_explained_var&#x27;: 4.529953e-06, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 465, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464}, &#x27;MDT.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 6.338561797747388e-05, &#x27;entropy_coeff&#x27;: 0.04032785817980766, &#x27;policy_loss&#x27;: 23747.527, &#x27;policy_entropy&#x27;: 65.900475, &#x27;var_gnorm&#x27;: 22.67044, &#x27;vf_loss&#x27;: 192922050.0}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 3.993511e-06, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 465, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464}, &#x27;MRK.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 6.338561797747388e-05, &#x27;entropy_coeff&#x27;: 0.04032785817980766, &#x27;policy_loss&#x27;: -34626.734, &#x27;policy_entropy&#x27;: 78.7318, &#x27;var_gnorm&#x27;: 22.647062, &#x27;vf_loss&#x27;: 10817610.0}, &#x27;grad_gnorm&#x27;: 39.999992, &#x27;vf_explained_var&#x27;: -3.3974648e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 465, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464}, &#x27;PFE.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 6.338561797747388e-05, &#x27;entropy_coeff&#x27;: 0.04032785817980766, &#x27;policy_loss&#x27;: -1291.4191, &#x27;policy_entropy&#x27;: 84.28026, &#x27;var_gnorm&#x27;: 22.654493, &#x27;vf_loss&#x27;: 2395295.0}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -3.695488e-06, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 465, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464}, &#x27;SYK.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 6.338561797747388e-05, &#x27;entropy_coeff&#x27;: 0.04032785817980766, &#x27;policy_loss&#x27;: -38904.156, &#x27;policy_entropy&#x27;: 79.96017, &#x27;var_gnorm&#x27;: 22.645924, &#x27;vf_loss&#x27;: 181790430.0}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: -2.7418137e-06, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 465, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464}, &#x27;TMO.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 6.338561797747388e-05, &#x27;entropy_coeff&#x27;: 0.04032785817980766, &#x27;policy_loss&#x27;: -9684.881, &#x27;policy_entropy&#x27;: 75.947845, &#x27;var_gnorm&#x27;: 22.650301, &#x27;vf_loss&#x27;: 10917432.0}, &#x27;grad_gnorm&#x27;: 40.0, &#x27;vf_explained_var&#x27;: 5.4240227e-06, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 465, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464}, &#x27;UNH.US&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 6.338561797747388e-05, &#x27;entropy_coeff&#x27;: 0.04032785817980766, &#x27;policy_loss&#x27;: -288743.03, &#x27;policy_entropy&#x27;: 74.87444, &#x27;var_gnorm&#x27;: 22.644964, &#x27;vf_loss&#x27;: 592247940.0}, &#x27;grad_gnorm&#x27;: 40.000004, &#x27;vf_explained_var&#x27;: 1.5497208e-05, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 32, &#x27;num_grad_updates_lifetime&#x27;: 465, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464}}, &#x27;num_env_steps_sampled&#x27;: 14880, &#x27;num_env_steps_trained&#x27;: 14880, &#x27;num_agent_steps_sampled&#x27;: 208320, &#x27;num_agent_steps_trained&#x27;: 208320}</td><td style=\"text-align: right;\">                   208320</td><td style=\"text-align: right;\">                   208320</td><td style=\"text-align: right;\">                  14880</td><td style=\"text-align: right;\">                              224</td><td style=\"text-align: right;\">                                   20.4801</td><td style=\"text-align: right;\">                  14880</td><td style=\"text-align: right;\">                              224</td><td style=\"text-align: right;\">                                   20.4801</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    3</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          224</td><td>{&#x27;cpu_util_percent&#x27;: 92.4125, &#x27;ram_util_percent&#x27;: 85.55000000000001}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{}            </td><td>{&#x27;episode_reward_max&#x27;: nan, &#x27;episode_reward_min&#x27;: nan, &#x27;episode_reward_mean&#x27;: nan, &#x27;episode_len_mean&#x27;: nan, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 0, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [], &#x27;episode_lengths&#x27;: []}, &#x27;sampler_perf&#x27;: {}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 1231.698, &#x27;sample_time_ms&#x27;: 1082.374, &#x27;learn_time_ms&#x27;: 125.25, &#x27;learn_throughput&#x27;: 255.49, &#x27;synch_weights_time_ms&#x27;: 22.807} </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-09-29 15:10:40 (running for 00:02:00.81)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 1800.000: None | Iter 600.000: None | Iter 200.000: None\n",
      "Logical resource usage: 8.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /Volumes/SSD980/ray/multiagent2/A2C\n",
      "Number of trials: 2/10 (2 RUNNING)\n",
      "+----------------------------+----------+-----------------+-----------------+----------+----------+-------------+-----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                 | status   | loc             |   entropy_coeff |    gamma |   lambda |          lr |   vf_loss_coeff |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|----------------------------+----------+-----------------+-----------------+----------+----------+-------------+-----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|\n",
      "| A2C_MultiAgent_4e970_00000 | RUNNING  | 127.0.0.1:47402 |       0.0455513 | 0.992995 | 0.994113 | 8.77166e-05 |        0.194472 |      9 |          94.9018 | 5344 |      nan |                  nan |                  nan |                nan |\n",
      "| A2C_MultiAgent_4e970_00001 | RUNNING  | 127.0.0.1:47403 |       0.0403279 | 0.971596 | 0.905525 | 6.33856e-05 |        0.245074 |      9 |          95.1322 | 5312 |      nan |                  nan |                  nan |                nan |\n",
      "+----------------------------+----------+-----------------+-----------------+----------+----------+-------------+-----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-09-29 15:12:40 (running for 00:04:00.81)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 1800.000: None | Iter 600.000: None | Iter 200.000: None\n",
      "Logical resource usage: 8.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /Volumes/SSD980/ray/multiagent2/A2C\n",
      "Number of trials: 2/10 (2 RUNNING)\n",
      "+----------------------------+----------+-----------------+-----------------+----------+----------+-------------+-----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                 | status   | loc             |   entropy_coeff |    gamma |   lambda |          lr |   vf_loss_coeff |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|----------------------------+----------+-----------------+-----------------+----------+----------+-------------+-----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| A2C_MultiAgent_4e970_00000 | RUNNING  | 127.0.0.1:47402 |       0.0455513 | 0.992995 | 0.994113 | 8.77166e-05 |        0.194472 |     20 |          208.443 | 12480 |      nan |                  nan |                  nan |                nan |\n",
      "| A2C_MultiAgent_4e970_00001 | RUNNING  | 127.0.0.1:47403 |       0.0403279 | 0.971596 | 0.905525 | 6.33856e-05 |        0.245074 |     20 |          207.901 | 12384 |      nan |                  nan |                  nan |                nan |\n",
      "+----------------------------+----------+-----------------+-----------------+----------+----------+-------------+-----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 15:13:46,149\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 6.142 s, which may be a performance bottleneck.\n",
      "2023-09-29 15:13:46,207\tWARNING util.py:315 -- The `process_trial_result` operation took 6.204 s, which may be a performance bottleneck.\n",
      "2023-09-29 15:13:46,208\tWARNING util.py:315 -- Processing trial results took 6.205 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-09-29 15:13:46,209\tWARNING util.py:315 -- The `process_trial_result` operation took 6.208 s, which may be a performance bottleneck.\n"
     ]
    }
   ],
   "source": [
    "if os.name == 'nt':\n",
    "    path_to_save = \"C:\\\\GitHub\\\\ray\\\\workertune\"\n",
    "else:\n",
    "    path_to_save = \"/Volumes/SSD980/ray/multiagent2\"\n",
    "\n",
    "\n",
    "env = MultiAgent()\n",
    "def env_creator(env_config):\n",
    "    return MultiAgent()  \n",
    "\n",
    "register_env(\"MultiAgent\", env_creator)\n",
    "\n",
    "asha_scheduler = ASHAScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    max_t=2000,\n",
    "    grace_period=200,\n",
    "    reduction_factor=3,\n",
    "    brackets=1,\n",
    ")\n",
    "\n",
    "def create_policy_spec(agent_id):\n",
    "    # print(f\"Creating policy for {worker_id} with obs space {env.observation_space[worker_id]} and action space {env.action_space[worker_id]}\")\n",
    "    return PolicySpec(\n",
    "        observation_space=env.observation_space[agent_id],\n",
    "        action_space=env.action_space[agent_id],\n",
    "        config={}\n",
    "    )\n",
    "\n",
    "controller_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['controller'],\n",
    "    action_space=env.action_space['controller'],\n",
    "    config={}\n",
    ")\n",
    "\n",
    "policies = {\n",
    "    \"controller_policy\": controller_policy_spec,\n",
    "}\n",
    "\n",
    "for agent_id in env.agents:\n",
    "    policies[agent_id] = create_policy_spec(agent_id)\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode=None, agent=None, **kwargs):\n",
    "    if agent_id == 'controller':\n",
    "        # print(f\"!!!!!! policy mapping manager: {agent_id}\")\n",
    "        return \"controller_policy\"\n",
    "    elif agent_id in env.agents:\n",
    "        return agent_id\n",
    "    else:\n",
    "        print(\"defaul policy triggered\")\n",
    "        return \"default_policy\"\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"MultiAgent\",\n",
    "    \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "        \"rollout_fragment_length\": \"auto\",\n",
    "        \"framework\": \"tf2\",\n",
    "        \"lr\": tune.uniform(1e-5,1e-4),\n",
    "        \"gamma\": tune.uniform(0.95, 0.9999),\n",
    "        \"lambda\": tune.uniform(0.9,1.0),\n",
    "        \"entropy_coeff\": tune.uniform(0.01,0.1),\n",
    "        \"vf_loss_coeff\": tune.uniform(0.1,0.3),\n",
    "        \"num_workers\": 3, \n",
    "        #Change for Debugging\n",
    "        \"log_level\": \"ERROR\",\n",
    "        \"output\": \"logdir\",\n",
    "        \"monitor\": True,\n",
    "}\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"A2C\", \n",
    "    metric=\"episode_reward_mean\", \n",
    "    num_samples=10,\n",
    "    resume=False,\n",
    "    mode=\"max\",\n",
    "    config=param_space, \n",
    "    local_dir=path_to_save,\n",
    "    search_alg=None,\n",
    "    scheduler=asha_scheduler,\n",
    "    progress_reporter=CLIReporter(max_progress_rows=10,max_report_frequency=120),\n",
    "    max_concurrent_trials=2,\n",
    "    #checkpoint_config not checked yet\n",
    "    checkpoint_config={\n",
    "        \"num_to_keep\": 3,\n",
    "        \"checkpoint_score_attribute\": \"episode_reward_mean\",\n",
    "        \"checkpoint_score_order\": \"max\",\n",
    "        \"checkpoint_frequency\": 10\n",
    "    }\n",
    "    )\n",
    "\n",
    "print(pretty_print(analysis.last_result))\n",
    "print(\"Best hyperparameters found were: \", pretty_print(analysis.best_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_agent_env = MultiAgent()\n",
    "\n",
    "\n",
    "for episode in range(1):\n",
    "    obs, info = multi_agent_env.reset()\n",
    "    done = {\"__all__\": False}\n",
    "    \n",
    "    print(f\"Episode {episode + 1}\")\n",
    "\n",
    "    while not done[\"__all__\"]:\n",
    "        actions = {}\n",
    "        \n",
    "        # Collect actions for each agent\n",
    "        for agent_id, agent_obs in obs.items():\n",
    "            action_space = multi_agent_env.action_space[agent_id]\n",
    "            action = action_space.sample()\n",
    "            actions[agent_id] = action\n",
    "\n",
    "        # Step the environment\n",
    "        obs, reward, done, _, info = multi_agent_env.step(actions)\n",
    "\n",
    "        # print(f\"Actions: {actions}\")\n",
    "        # print(f\"Observations: {obs}\")\n",
    "        # print(f\"Rewards: {reward}\")\n",
    "        # print(f\"Done flags: {done}\")\n",
    "        # print(f\"Info: {info}\")\n",
    "\n",
    "    print(\"Episode done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WorkerStandAlone()\n",
    "def env_creator(env_config):\n",
    "    return WorkerStandAlone()  \n",
    "\n",
    "register_env(\"worker\", env_creator)\n",
    "\n",
    "\n",
    "\n",
    "analysis = tune.run(\n",
    "        \"A2C\",\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        config={\n",
    "            \"env\": \"worker\",\n",
    "            \"env_config\": {\"initial_capital\": 1e6},\n",
    "            \"lr\": tune.uniform(1e-5, 1e-4),\n",
    "            \"train_batch_size\": tune.choice([10000, 20000, 40000]),\n",
    "}, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_iterations = 30000\n",
    "\n",
    "\n",
    "env = WorkerStandAlone(data=data)\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    action = env.action_space.sample()  # Generate random actions for the manager\n",
    "    worker_dones = {...}  # You need to provide the values for worker_dones\n",
    "    worker_truncateds = {...}  # You need to provide the values for worker_truncateds\n",
    "\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        print(\"Episode finished!\")\n",
    "        state = env.reset()\n",
    "    else:\n",
    "        state = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mit vielen policies\n",
    "import sys\n",
    "# Redirect the standard error to the log file\n",
    "\n",
    "# Create a log file\n",
    "log_file = open(\"console_logs.txt\", \"w\")\n",
    "\n",
    "\n",
    "# Redirect the standard output to the log file\n",
    "sys.stdout = log_file\n",
    "\n",
    "\n",
    "env = HRL()\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  \n",
    "\n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "def create_policy_spec(worker_id):\n",
    "    # print(f\"Creating policy for {worker_id} with obs space {env.observation_space[worker_id]} and action space {env.action_space[worker_id]}\")\n",
    "    return PolicySpec(\n",
    "        observation_space=env.observation_space[worker_id],\n",
    "        action_space=env.action_space[worker_id],\n",
    "        config={}\n",
    "    )\n",
    "\n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    "\n",
    "policies = {\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    "\n",
    "for worker_id in env.workers:\n",
    "    policies[worker_id] = create_policy_spec(worker_id)\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id == 'manager':\n",
    "        # print(f\"!!!!!! policy mapping manager: {agent_id}\")\n",
    "        return \"manager_policy\"\n",
    "    elif agent_id in env.workers:\n",
    "        return agent_id\n",
    "    else:\n",
    "        print(\"defaul policy triggered\")\n",
    "        return \"default_policy\"\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "    \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "        \"rollout_fragment_length\": \"auto\",\n",
    "        \"framework\": \"tf2\",\n",
    "        \"lr\": tune.uniform(1e-5,1e-4),\n",
    "        \"gamma\": tune.uniform(0.95, 0.9999),\n",
    "        \"lambda\": tune.uniform(0.9,1.0),\n",
    "        \"entropy_coeff\": tune.uniform(0.01,0.1),\n",
    "        \"vf_loss_coeff\": tune.uniform(0.1,0.3),\n",
    "        \"num_workers\": 3, \n",
    "        #Change for Debugging\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"output\": \"logdir\",\n",
    "        \"monitor\": True,\n",
    "}\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"A2C\",\n",
    "    metric=\"episode_reward_mean\", \n",
    "    num_samples=5,\n",
    "    mode=\"max\",\n",
    "    config=param_space, \n",
    "    local_dir=\"/Volumes/SSD980/ray/results/tunerun2\",\n",
    "    search_alg=None,\n",
    "    scheduler=None,\n",
    "    progress_reporter=CLIReporter(max_progress_rows=5,max_report_frequency=120),\n",
    "    max_concurrent_trials=2,\n",
    "    #checkpoint_config not checked yet\n",
    "    checkpoint_config={\n",
    "        \"num_to_keep\": 1,\n",
    "        \"checkpoint_score_attribute\": \"episode_reward_mean\",\n",
    "        \"checkpoint_score_order\": \"max\",\n",
    "        \"checkpoint_frequency\": 10\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HRL()\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  \n",
    "\n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    "    \n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    " \n",
    "initial_params = [{\n",
    "    \"lr\": 0.001,\n",
    "    \"gamma\": 0.92,\n",
    "    \"lambda\": 0.95,\n",
    "    \"entropy_coeff\": 1e-3,\n",
    "    \"vf_loss_coeff\": 0.5,\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": 64,\n",
    "        \"fcnet_activation\":\"relu\",\n",
    "    },\n",
    "}]\n",
    "\n",
    "search_space = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"gamma\": tune.uniform(0.9, 0.99),\n",
    "    \"lambda\": tune.uniform(0.9, 1.0),\n",
    "    \"entropy_coeff\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"vf_loss_coeff\": tune.uniform(0.1, 1.0),\n",
    "    # \"model\": {\n",
    "    #     \"fcnet_hiddens\": tune.grid_search([[64, 64], [128, 128], [256, 256]]),\n",
    "    #     \"fcnet_activation\": tune.choice([\"relu\", \"tanh\"]),\n",
    "    # },\n",
    "}\n",
    "\n",
    "\n",
    "algo = HyperOptSearch(space=search_space,metric=\"episode_reward_mean\", mode=\"max\",)\n",
    "\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "\n",
    "        \"num_workers\": 1,  \n",
    "        \"num_cpus_per_trial\": 1,\n",
    "         \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"gamma\": tune.uniform(0.9, 0.99),\n",
    "    \"lambda\": tune.uniform(0.9, 1.0),\n",
    "    \"entropy_coeff\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"vf_loss_coeff\": tune.uniform(0.1, 1.0),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "          \"rollout_fragment_length\": \"auto\",\n",
    "          \"lr\": tune.uniform(1e-5,1e-4),\n",
    "          \"lambda\": tune.uniform(0.95, 1.0),\n",
    "          \"vf_loss_coeff\": tune.uniform(0.1, 1.0),\n",
    "          \"entropy_coeff\": tune.uniform(1e-4, 1e-1),\n",
    "        \"num_workers\": 1,  \n",
    "        \"num_envs_per_worker\": 1\n",
    "}\n",
    "\n",
    "analysis = tune.run(\"A2C\", \n",
    "                    metric=\"episode_reward_mean\", \n",
    "                    mode=\"max\",\n",
    "                    config=param_space,\n",
    "                    num_samples=10,\n",
    "                    stop={\"training_iteration\": 100},\n",
    "                    progress_reporter=CLIReporter(max_progress_rows=10,max_report_frequency=600),\n",
    "                    # local_dir=\"/Users/floriankockler/rayresults/overnight1\",\n",
    "                    storage_path=\"/Users/floriankockler/Documents/GitHub.nosync/raystorage\",\n",
    "                    checkpoint_config=CheckpointConfig(\n",
    "                        num_to_keep=2,\n",
    "                        checkpoint_score_attribute=\"episode_reward_mean\", \n",
    "                        checkpoint_score_order=\"max\"\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "\n",
    "best = analysis.best_trial\n",
    "print(pretty_print(best.last_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = HRL()\n",
    "\n",
    "n_iterations = 1\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    obs, reward, done, truncated, info= env.step(action)\n",
    "\n",
    "    # print(f\"Action: {action}, Reward: {reward}, Portfolio Value: {obs[0] + obs[1] * obs[2]}\")\n",
    "    \n",
    "    if done[\"__all__\"]:\n",
    "        print(\"Episode finished!\")\n",
    "        state = env.reset()\n",
    "    else:\n",
    "        state = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    "    \n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    "\n",
    "    \n",
    "algo = PPOConfig().environment(env=HRL).multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn = policy_mapping_fn,  \n",
    "    ).training(train_batch_size=4000).build()\n",
    "\n",
    "print(algo.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HRL()\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  \n",
    "\n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    "    \n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    " \n",
    "search_space = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),  # Learning rate\n",
    "    \"gamma\": tune.uniform(0.9, 0.99),  # Discount factor\n",
    "}\n",
    "\n",
    "tune_config = TuneConfig(\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    max_concurrent_trials=1,\n",
    "    num_samples=100,\n",
    "    search_alg=BayesOptSearch(\n",
    "          search_space,\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        random_search_steps=4\n",
    "    ),\n",
    "    scheduler=HyperBandScheduler()\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    name=\"MyExperiment\",\n",
    "    storage_path=\"/Users/floriankockler/rayresults/tuner_trial\",\n",
    "    verbose=2,\n",
    "    # checkpoint_config=air.CheckpointConfig(checkpoint_frequency=2),\n",
    "    callbacks=None,\n",
    "    stop=None,\n",
    "    failure_config=None,\n",
    "    sync_config=None,\n",
    "    checkpoint_config=None,\n",
    "    progress_reporter=CLIReporter(max_progress_rows=10),# Define the policy mapping function\n",
    "\n",
    ")\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "          \"rollout_fragment_length\": \"auto\",\n",
    "          \"lr\": tune.uniform(1e-5,1e-4),\n",
    "        \"num_workers\": 1,  \n",
    "        \"num_cpus_per_trial\": 1,\n",
    "}\n",
    "\n",
    "analysis = tune.run(\"A2C\", metric=\"episode_reward_mean\", mode=\"max\",config=param_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "algo = PPOConfig().environment(env=HRL).multi_agent(\n",
    "    policies={\n",
    "        \"policy_1\": ()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune import CLIReporter\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  # Assuming this is your environment\n",
    " \n",
    "env = HRL()\n",
    "\n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    "    \n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    " \n",
    "search_space = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),  # Learning rate\n",
    "    \"gamma\": tune.uniform(0.9, 0.99),  # Discount factor\n",
    "}\n",
    "\n",
    "tune_config = TuneConfig(\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    max_concurrent_trials=1,\n",
    "    num_samples=100,\n",
    "    search_alg=BayesOptSearch(\n",
    "          search_space,\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        random_search_steps=4\n",
    "    ),\n",
    "    scheduler=HyperBandScheduler()\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    name=\"MyExperiment\",\n",
    "    storage_path=\"/Users/floriankockler/rayresults/tuner_trial\",\n",
    "    verbose=2,\n",
    "    # checkpoint_config=air.CheckpointConfig(checkpoint_frequency=2),\n",
    "    callbacks=None,\n",
    "    stop=None,\n",
    "    failure_config=None,\n",
    "    sync_config=None,\n",
    "    checkpoint_config=None,\n",
    "    progress_reporter=CLIReporter(max_progress_rows=10),# Define the policy mapping function\n",
    "\n",
    ")\n",
    "\n",
    "param_space = {\n",
    "     \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "          \"rollout_fragment_length\": \"auto\",\n",
    "        \"num_workers\": 1,  \n",
    "        \"num_cpus_per_trial\": 1,\n",
    "}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"A2C\",\n",
    "    tune_config=tune_config,\n",
    "    param_space=param_space,\n",
    "\n",
    "    run_config=run_config,\n",
    ")\n",
    "results = tuner.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from ray import air, tune\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "import argparse\n",
    "from ray.tune import CLIReporter\n",
    "from env.multi_agent.hrl import HRL\n",
    "\n",
    "reporter = CLIReporter(max_progress_rows=10)\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return HRL(env_config)  # Assuming this is your environment\n",
    " \n",
    "register_env(\"hrl\", env_creator)\n",
    "\n",
    "manager_config = {\n",
    "    \"df\": train_df,\n",
    "\n",
    "}\n",
    "hrl_config={\n",
    "        \"manager_config\": manager_config\n",
    "        }\n",
    "env = HRL(hrl_config)\n",
    " \n",
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    if agent_id in env.workers:\n",
    "        return \"worker_policy\"\n",
    "    else:\n",
    "        return \"manager_policy\"\n",
    " \n",
    "\n",
    "\n",
    "first_worker_tic = next(iter(env.workers))\n",
    "worker_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space[first_worker_tic],\n",
    "    action_space=env.action_space[first_worker_tic],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "manager_policy_spec = PolicySpec(\n",
    "    observation_space=env.observation_space['manager'],\n",
    "    action_space=env.action_space['manager'],\n",
    "    config={}\n",
    ")\n",
    " \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\"\n",
    ")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "policies = {\n",
    "    \"worker_policy\": worker_policy_spec,\n",
    "    \"manager_policy\": manager_policy_spec,\n",
    "}\n",
    "\n",
    "\n",
    "def explore(config):\n",
    "    # Ensure we collect enough timesteps to do sgd\n",
    "    if config[\"train_batch_size\"] < config[\"rollout_fragment_length\"] * 2:\n",
    "        config[\"train_batch_size\"] = config[\"rollout_fragment_length\"] * 2\n",
    "    return config\n",
    "\n",
    "hyperparam_mutations = {\n",
    "    \"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "    \"gamma\": lambda: random.uniform(0.9, 1.0),\n",
    "    \"entropy_coeff\": [0.01, 0.1, 1.0],\n",
    "    \"num_envs_per_worker\": [1, 2, 4, 8],\n",
    "    #\"rollout_fragment_length\": [50, 100, 200, 400],\n",
    "    \"train_batch_size\": lambda: random.randint(200, 1500),\n",
    "    \"sgd_minibatch_size\": tune.choice([50, 100, 200]),\n",
    "\n",
    "}\n",
    "\n",
    "pbt = PopulationBasedTraining(\n",
    "        time_attr=\"time_total_s\",\n",
    "        perturbation_interval=120,\n",
    "        resample_probability=0.25,\n",
    "        # Specifies the mutations of these hyperparams\n",
    "        hyperparam_mutations=hyperparam_mutations,\n",
    "        custom_explore_fn=explore,\n",
    "    )\n",
    "\n",
    "# Stop when we've reached 100 training iterations or reward=300\n",
    "stopping_criteria = {\"training_iteration\": 100}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"A2C\",\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        scheduler=pbt,\n",
    "        num_samples=1 if args.smoke_test else 10,\n",
    "    ),\n",
    "    param_space={\n",
    "        \"env\": \"hrl\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        },\n",
    "        \"env_config\": hrl_config,\n",
    "        \"rollout_fragment_length\": \"auto\",\n",
    "        \"framework\": \"tf2\",\n",
    "        \"num_workers\": 1,  # 1 for training + 4 for sampling\n",
    "        \"num_cpus_per_trial\": 3,\n",
    "        # \"num_cpus\": 1,  # number of CPUs to use per trial --> 6 in total = max available\n",
    "        # \"num_gpus\": 0,  # number of GPUs to use per trial\n",
    "        # These params are tuned from a fixed starting value.\n",
    "        \"lr\": 1e-4,\n",
    "        # These params start off randomly drawn from a set.\n",
    "        \"sgd_minibatch_size\": tune.choice([50, 100, 200]),\n",
    "        \"train_batch_size\": tune.choice([200, 400, 600]),\n",
    "    },\n",
    "\n",
    "    run_config=air.RunConfig(stop=stopping_criteria, local_dir=\"/Users/floriankockler/rayresults/autobatch\", progress_reporter=reporter),\n",
    ")\n",
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
